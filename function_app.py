"""Azure Functions entry point (Python programming model v2)"""

import os
import json
import logging
import base64
import azure.functions as func

logger = logging.getLogger(__name__)

# ========================================
# 1. FastAPI ASGI Wrapper (HTTP íŠ¸ë¦¬ê±°)
# ========================================
from app.main import app as fastapi_app

app = func.FunctionApp(http_auth_level=func.AuthLevel.ANONYMOUS)

@app.function_name(name="http_app_func")
@app.route(route="{*route}", methods=["GET", "POST", "PUT", "DELETE", "PATCH", "OPTIONS", "HEAD"])
async def http_app_func(req: func.HttpRequest) -> func.HttpResponse:
    """FastAPIë¥¼ Azure Functions HTTP íŠ¸ë¦¬ê±°ë¡œ ë˜í•‘"""
    return await func.AsgiMiddleware(fastapi_app).handle_async(req)


# ========================================
# 2. Queue Trigger (ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤)
# ========================================
_QUEUE_NAME = os.getenv("AZURE_STORAGE_QUEUE_NAME", "ai-audiobook-jobs")


@app.function_name(name="session_job_worker")
@app.queue_trigger(
    arg_name="msg",
    queue_name=_QUEUE_NAME,
    connection="AzureWebJobsStorage",
)
def session_job_worker(msg: func.QueueMessage) -> None:
    """íì—ì„œ ì„¸ì…˜ ì²˜ë¦¬ ì‘ì—…ì„ ê°€ì ¸ì™€ ì‹¤í–‰"""
    import base64
    
    raw = msg.get_body().decode("utf-8", errors="replace")
    logger.info("[Queue] Received message: %s", raw[:200])

    try:
        # Base64 ë””ì½”ë”© ì‹œë„ (enqueue_session_jobì—ì„œ Base64 ì¸ì½”ë”©ë¨)
        try:
            decoded = base64.b64decode(raw).decode('utf-8')
            payload = json.loads(decoded)
        except Exception:
            # Base64ê°€ ì•„ë‹ˆë©´ ê·¸ëƒ¥ JSON íŒŒì‹±
            payload = json.loads(raw)
    except Exception:
        logger.exception("[Queue] Invalid JSON message")
        return

    kind = payload.get("kind", "generate")
    
    # ê¸°ì¡´ ë°©ì‹ (í˜¸í™˜ì„± ìœ ì§€)
    if kind == "generate":
        _handle_full_pipeline(payload)
        return
    
    # ìƒˆë¡œìš´ ë‹¨ê³„ë³„ ë°©ì‹
    if kind == "pipeline_step":
        _handle_pipeline_step(payload)
        return
    
    logger.warning("[Queue] Unknown kind=%s. skipping", kind)


def _run_service(service, session_id: str, channel_id: str, options: dict) -> None:
    """ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë™ê¸° ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‹¤í–‰"""
    import asyncio
    import inspect

    fn = service.process_audiobook_generation

    if inspect.iscoroutinefunction(fn):
        # ë¹„ë™ê¸° í•¨ìˆ˜ì¸ ê²½ìš° asyncio.runìœ¼ë¡œ ì‹¤í–‰
        try:
            asyncio.run(fn(session_id=session_id, channel_id=channel_id, options=options))
        except Exception as e:
            logger.error(f"[Queue] asyncio.run failed: {e}")
            raise
    else:
        # ë™ê¸° í•¨ìˆ˜ì¸ ê²½ìš° ë°”ë¡œ ì‹¤í–‰
        fn(session_id=session_id, channel_id=channel_id, options=options)


def _handle_full_pipeline(payload: dict) -> None:
    """ê¸°ì¡´ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (í˜¸í™˜ì„± ìœ ì§€)"""
    session_id = payload.get("session_id")
    channel_id = payload.get("channel_id")
    options = payload.get("options") or {}

    if not session_id or not channel_id:
        logger.warning("[Queue] Missing session_id/channel_id")
        return

    logger.info(f"[Queue] Full pipeline: session_id={session_id}")

    try:
        from app.dependencies.repos import (
            get_db,
            get_channel_repo,
            get_session_repo,
            get_session_input_repo,
        )
        from app.services.storage_service import get_storage
        from app.services.session_service import SessionService

        storage = get_storage()
        backend = os.getenv("REPO_BACKEND", "memory").lower().strip()

        if backend == "postgres":
            db_gen = get_db()
            db = next(db_gen)
            try:
                channel_repo = get_channel_repo(db=db)
                session_repo = get_session_repo(db=db)
                session_input_repo = get_session_input_repo(db=db)

                service = SessionService(
                    channel_repo=channel_repo,
                    session_repo=session_repo,
                    session_input_repo=session_input_repo,
                    storage=storage,
                )
                _run_service(service, session_id, channel_id, options)
            finally:
                try:
                    next(db_gen)
                except StopIteration:
                    pass
        else:
            channel_repo = get_channel_repo(db=None)
            session_repo = get_session_repo(db=None)
            session_input_repo = get_session_input_repo(db=None)

            service = SessionService(
                channel_repo=channel_repo,
                session_repo=session_repo,
                session_input_repo=session_input_repo,
                storage=storage,
            )
            _run_service(service, session_id, channel_id, options)

        logger.info(f"[Queue] âœ… Full pipeline completed: {session_id}")

    except Exception as e:
        logger.exception(f"[Queue] âŒ Full pipeline failed: {session_id}")

# ======================================
# ë‹¨ê³„ë³„ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# ======================================

def _handle_pipeline_step(payload: dict) -> None:
    """ë‹¨ê³„ë³„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    session_id = payload.get("session_id")
    channel_id = payload.get("channel_id")
    step = payload.get("step")
    options = payload.get("options") or {}

    if not session_id or not channel_id or not step:
        logger.warning("[Queue] Missing required fields in pipeline_step")
        return

    logger.info(f"[Queue] Pipeline step: {step} for session={session_id}")

    # DB ì—°ê²°
    from app.dependencies.repos import get_db, get_channel_repo, get_session_repo, get_session_input_repo
    from app.services.storage_service import get_storage

    storage = get_storage()
    backend = os.getenv("REPO_BACKEND", "memory").lower().strip()

    if backend == "postgres":
        db_gen = get_db()
        db = next(db_gen)
        try:
            channel_repo = get_channel_repo(db=db)
            session_repo = get_session_repo(db=db)
            session_input_repo = get_session_input_repo(db=db)

            _execute_pipeline_step(
                step=step,
                session_id=session_id,
                channel_id=channel_id,
                options=options,
                storage=storage,
                session_repo=session_repo,
                session_input_repo=session_input_repo,
            )
        finally:
            try:
                next(db_gen)
            except StopIteration:
                pass
    else:
        channel_repo = get_channel_repo(db=None)
        session_repo = get_session_repo(db=None)
        session_input_repo = get_session_input_repo(db=None)

        _execute_pipeline_step(
            step=step,
            session_id=session_id,
            channel_id=channel_id,
            options=options,
            storage=storage,
            session_repo=session_repo,
            session_input_repo=session_input_repo,
        )


def _execute_pipeline_step(
    step: str,
    session_id: str,
    channel_id: str,
    options: dict,
    storage,
    session_repo,
    session_input_repo,
) -> None:
    """ì‹¤ì œ ë‹¨ê³„ë³„ ì‹¤í–‰ ë¡œì§"""
    try:
        # ===== resume(ë©±ë“±) ê°€ë“œ =====
        # ì´ë¯¸ ì‚°ì¶œë¬¼ì´ ìˆìœ¼ë©´ í•´ë‹¹ stepì€ ë‹¤ì‹œ ì‹¤í–‰í•˜ì§€ ì•Šê³  ë‹¤ìŒ stepì„ enqueue
        session = session_repo.get_session(session_id)
        storage_prefix = (session or {}).get("storage_prefix", "")

        def _json_exists(key: str) -> bool:
            try:
                storage.download_json(key)
                return True
            except Exception:
                return False

        from app.services.queue_service import enqueue_pipeline_step

        # extract_ocr ì¬ê°œ: ocr_results.json ìˆìœ¼ë©´ extract_finalizeë¡œ
        if step == "extract_ocr":
            ocr_key = f"{storage_prefix}pipeline/ocr_results.json"
            if _json_exists(ocr_key):
                logger.info("[Resume] ocr_results exists. Skipping extract_ocr -> enqueue extract_finalize")
                enqueue_pipeline_step(session_id=session_id, channel_id=channel_id, step="extract_finalize", options=options)
                return

        # extract_finalize ì¬ê°œ: extracted_data.json ìˆìœ¼ë©´ scriptë¡œ
        if step == "extract_finalize":
            extracted_key = f"{storage_prefix}pipeline/extracted_data.json"
            if _json_exists(extracted_key):
                logger.info("[Resume] extracted_data exists. Skipping extract_finalize -> enqueue script")
                enqueue_pipeline_step(session_id=session_id, channel_id=channel_id, step="script", options=options)
                return

        # script ì¬ê°œ: script.json ìˆìœ¼ë©´ audioë¡œ
        if step == "script":
            script_key = f"{storage_prefix}pipeline/script.json"
            if _json_exists(script_key):
                logger.info("[Resume] script exists. Skipping script -> enqueue audio")
                enqueue_pipeline_step(session_id=session_id, channel_id=channel_id, step="audio", options=options)
                return

        # audio ì¬ê°œ: audio_metadata.json ìˆìœ¼ë©´ finalizeë¡œ
        if step == "audio":
            audio_meta_key = f"{storage_prefix}pipeline/audio_metadata.json"
            if _json_exists(audio_meta_key):
                logger.info("[Resume] audio_metadata exists. Skipping audio -> enqueue finalize")
                enqueue_pipeline_step(session_id=session_id, channel_id=channel_id, step="finalize", options=options)
                return

        # finalize ì¬ê°œ: ì´ë¯¸ completedë©´ ìŠ¤í‚µ(ì¤‘ë³µ finalize ë°©ì§€)
        if step == "finalize":
            sess = session_repo.get_session(session_id)
            if (sess or {}).get("status") == "completed" or (sess or {}).get("current_step") == "completed":
                logger.info("[Resume] session already completed. Skipping finalize")
                return

        # ===== ê¸°ì¡´ ë¡œì§ =====
        if step == "extract_ocr":
            _run_extract_ocr_step(session_id, channel_id, options, storage, session_repo, session_input_repo)
        elif step == "extract_finalize":
            _run_extract_finalize_step(session_id, channel_id, options, storage, session_repo)
        elif step == "script":
            _run_script_step(session_id, channel_id, options, storage, session_repo)
        elif step == "audio":
            _run_audio_step(session_id, channel_id, options, storage, session_repo)
        elif step == "finalize":
            _run_finalize_step(session_id, channel_id, options, storage, session_repo)
        else:
            logger.error(f"[Queue] Unknown step: {step}")

    except Exception as e:
        logger.exception(f"[Queue] âŒ Step {step} failed for session={session_id}")
        
        # ì—ëŸ¬ ìƒíƒœ ì—…ë°ì´íŠ¸
        try:
            session_repo.update_session_fields(
                session_id,
                status="failed",
                current_step=f"{step}_error",
                error_message=str(e)[:500],
            )
        except Exception:
            logger.exception("Failed to update error status")


# ========================================
# ê° ë‹¨ê³„ë³„ ì‹¤í–‰ í•¨ìˆ˜
# ========================================

def _run_extract_ocr_step(session_id, channel_id, options, storage, session_repo, session_input_repo):
    """
    âœ… Extract Phase 1: OCR ìˆ˜í–‰
    - ì…ë ¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    - extract_texts_node() ì‹¤í–‰ (OCR í¬í•¨)
    - OCR ê²°ê³¼ë§Œ Blobì— ì €ì¥
    """
    import tempfile
    from app.utils.session_helpers import session_exists
    
    logger.info(f"[ExtractOCR] Starting for session={session_id}")
    
    # ì„¸ì…˜ ì‚­ì œ ì²´í¬
    if not session_exists(session_repo, session_id):
        logger.info(f"[ExtractOCR] Session {session_id} deleted - skipping")
        return
    
    session = session_repo.get_session(session_id)
    storage_prefix = session.get("storage_prefix", "")
    
    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
    session_repo.update_session_fields(
        session_id,
        current_step="extract_texts",
    )
    
    # ì…ë ¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    inputs = session_input_repo.list_inputs(session_id)
    if not inputs:
        raise Exception("ì…ë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    main_sources = []
    aux_sources = []
    temp_files = []
    
    try:
        for inp in inputs:
            if inp.get("is_link"):
                source_path = inp["link_url"]
            else:
                input_key = inp["input_key"]
                file_data = storage.download(input_key)
                
                file_ext = os.path.splitext(input_key)[1] or ".tmp"
                temp_fd, temp_path = tempfile.mkstemp(suffix=file_ext, prefix=f"input_{inp['input_id']}_")
                
                with os.fdopen(temp_fd, 'wb') as f:
                    f.write(file_data)
                
                temp_files.append(temp_path)
                source_path = temp_path
            
            if inp["role"] == "main":
                main_sources.append(source_path)
            else:
                aux_sources.append(source_path)
        
        # ì²´í¬í¬ì¸íŠ¸ ì½œë°± í•¨ìˆ˜ ì •ì˜
        def checkpoint_callback(key: str, data: dict):
            """ì¤‘ê°„ ì €ì¥ ì½œë°± - Blob Storageì— ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
            try:
                storage.upload_json(key, data)
                logger.info(f"[ExtractOCR] ğŸ’¾ Checkpoint saved: {key}")
            except Exception as e:
                logger.warning(f"[ExtractOCR] âš ï¸ Checkpoint save failed: {e}")

        # LangGraph ë…¸ë“œ ì§ì ‘ í˜¸ì¶œ (OCR ìˆ˜í–‰)
        from app.langgraph_pipeline.podcast.graph import extract_texts_node
        
        state = {
            "main_sources": main_sources,
            "aux_sources": aux_sources,
            "source_data": {},
            "main_texts": [],
            "aux_texts": [],
            "combined_text": "",
            "errors": [],
            "usage": {},
            "session_id": session_id,
            "storage_prefix": storage_prefix,
            "checkpoint_callback": checkpoint_callback,
        }
        
        # âœ… OCR ìˆ˜í–‰
        state = extract_texts_node(state)
        
        if not session_exists(session_repo, session_id):
            logger.info(f"[ExtractOCR] Session {session_id} deleted during execution")
            return
        
        if state.get("errors"):
            raise Exception(f"Extract OCR failed: {state['errors']}")
        
        # âœ… OCR ê²°ê³¼ ì €ì¥ (combine ì „)
        ocr_results_key = f"{storage_prefix}pipeline/ocr_results.json"
        
        ocr_data = {
            "source_data": state["source_data"],
            "main_texts": state["main_texts"],
            "aux_texts": state["aux_texts"],
            "usage": state.get("usage", {}),
        }
        
        storage.upload_json(ocr_results_key, ocr_data)
        
        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
        progress_key = f"{storage_prefix}pipeline/progress.json"
        progress_data = {
            "completed_steps": ["extract_ocr"],
            "current_step": "extract_finalize",
            "intermediate_keys": {
                "ocr_results": ocr_results_key,
            }
        }
        storage.upload_json(progress_key, progress_data)
        
        # DB ì—…ë°ì´íŠ¸
        session_repo.update_session_fields(
            session_id,
            current_step="extract_ocr_complete",
        )
        
        logger.info(f"[ExtractOCR] âœ… Completed for session={session_id}")
        
        # âœ… ë‹¤ìŒ ë‹¨ê³„ íì‰ (extract_finalize)
        from app.services.queue_service import enqueue_pipeline_step
        enqueue_pipeline_step(
            session_id=session_id,
            channel_id=channel_id,
            step="extract_finalize",
            options=options,
        )
        
    finally:
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        for temp_file in temp_files:
            try:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
            except Exception:
                pass


def _run_extract_finalize_step(session_id, channel_id, options, storage, session_repo):
    """
    âœ… Extract Phase 2: í…ìŠ¤íŠ¸ ë³‘í•© + ë©”íƒ€ë°ì´í„° ìƒì„±
    - OCR ê²°ê³¼ ë¡œë“œ
    - combine_texts_node() ì‹¤í–‰
    - ìµœì¢… extracted_data.json ì €ì¥
    """
    from app.utils.session_helpers import session_exists
    
    logger.info(f"[ExtractFinalize] Starting for session={session_id}")
    
    if not session_exists(session_repo, session_id):
        logger.info(f"[ExtractFinalize] Session {session_id} deleted - skipping")
        return
    
    session = session_repo.get_session(session_id)
    storage_prefix = session.get("storage_prefix", "")
    
    session_repo.update_session_fields(session_id, current_step="combine_texts")
    
    # âœ… ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ
    ocr_results_key = f"{storage_prefix}pipeline/ocr_results.json"
    ocr_data = storage.download_json(ocr_results_key)
    
    # LangGraph ë…¸ë“œ ì§ì ‘ í˜¸ì¶œ
    from app.langgraph_pipeline.podcast.graph import combine_texts_node
    
    state = {
        "source_data": ocr_data["source_data"],
        "main_texts": ocr_data["main_texts"],
        "aux_texts": ocr_data["aux_texts"],
        "combined_text": "",
        "errors": [],
        "usage": ocr_data.get("usage", {}),
    }
    
    # âœ… í…ìŠ¤íŠ¸ ë³‘í•© (ë¹ ë¥¸ ì‘ì—…)
    state = combine_texts_node(state)
    
    if not session_exists(session_repo, session_id):
        logger.info(f"[ExtractFinalize] Session {session_id} deleted during execution")
        return
    
    # âœ… ìµœì¢… ê²°ê³¼ ì €ì¥
    extracted_key = f"{storage_prefix}pipeline/extracted_data.json"
    
    extracted_data = {
        "combined_text": state["combined_text"],
        "source_data": state["source_data"],
        "main_texts": state["main_texts"],
        "aux_texts": state["aux_texts"],
        "usage": state.get("usage", {}),
    }
    
    storage.upload_json(extracted_key, extracted_data)
    
    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
    progress_key = f"{storage_prefix}pipeline/progress.json"
    progress_data = storage.download_json(progress_key)
    progress_data["completed_steps"].append("extract_finalize")
    progress_data["current_step"] = "script"
    progress_data["intermediate_keys"]["extracted_data"] = extracted_key
    storage.upload_json(progress_key, progress_data)
    
    # DB ì—…ë°ì´íŠ¸
    session_repo.update_session_fields(
        session_id,
        current_step="extract_complete",
    )
    
    logger.info(f"[ExtractFinalize] âœ… Completed for session={session_id}")
    
    # âœ… ë‹¤ìŒ ë‹¨ê³„ íì‰ (script)
    from app.services.queue_service import enqueue_pipeline_step
    enqueue_pipeline_step(
        session_id=session_id,
        channel_id=channel_id,
        step="script",
        options=options,
    )


def _run_script_step(session_id, channel_id, options, storage, session_repo):
    """2ë‹¨ê³„: ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
    from app.utils.session_helpers import session_exists
    
    logger.info(f"[Script] Starting for session={session_id}")
    
    if not session_exists(session_repo, session_id):
        logger.info(f"[Script] Session {session_id} deleted - skipping")
        return
    
    session = session_repo.get_session(session_id)
    storage_prefix = session.get("storage_prefix", "")
    
    session_repo.update_session_fields(session_id, current_step="generate_script")
    
    # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ
    extracted_key = f"{storage_prefix}pipeline/extracted_data.json"
    extracted_data = storage.download_json(extracted_key)
    
    # LangGraph ë…¸ë“œ ì§ì ‘ í˜¸ì¶œ
    from app.langgraph_pipeline.podcast.graph import generate_script_node
    
    # âœ… Vertex AI ì„¤ì •
    project_id = os.getenv("VERTEX_AI_PROJECT_ID")
    region = os.getenv("VERTEX_AI_REGION")
    sa_file = os.getenv("VERTEX_AI_SERVICE_ACCOUNT_FILE")
    
    state = {
        "combined_text": extracted_data["combined_text"],
        "source_data": extracted_data["source_data"],
        "project_id": project_id,
        "region": region,
        "sa_file": sa_file,
        "host_name": options.get("host1", "Fenrir"),
        "guest_name": options.get("host2", ""),
        "style": options.get("style", "explain"),
        "duration": options.get("duration", 5),
        "difficulty": options.get("difficulty", "intermediate"),
        "user_prompt": options.get("user_prompt", ""),
        "usage": extracted_data.get("usage", {}),
        "errors": [],
    }
    
    state = generate_script_node(state)
    
    if not session_exists(session_repo, session_id):
        logger.info(f"[Script] Session {session_id} deleted during execution")
        return
    
    if state.get("errors"):
        raise Exception(f"Script generation failed: {state['errors']}")
    
    # ìŠ¤í¬ë¦½íŠ¸ ì €ì¥
    script_key = f"{storage_prefix}pipeline/script.json"
    script_data = {
        "title": state["title"],
        "script": state["script"],
        "usage": state.get("usage", {}),
    }
    storage.upload_json(script_key, script_data)
    
    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
    progress_key = f"{storage_prefix}pipeline/progress.json"
    progress_data = storage.download_json(progress_key)
    progress_data["completed_steps"].append("script")
    progress_data["current_step"] = "audio"
    progress_data["intermediate_keys"]["script"] = script_key
    storage.upload_json(progress_key, progress_data)
    
    session_repo.update_session_fields(
        session_id,
        current_step="script_complete",
        title=state["title"],
    )
    
    logger.info(f"[Script] âœ… Completed for session={session_id}")
    
    # ë‹¤ìŒ ë‹¨ê³„ íì‰
    from app.services.queue_service import enqueue_pipeline_step
    enqueue_pipeline_step(
        session_id=session_id,
        channel_id=channel_id,
        step="audio",
        options=options,
    )


def _run_audio_step(session_id, channel_id, options, storage, session_repo):
    """3ë‹¨ê³„: ì˜¤ë””ì˜¤ ìƒì„±"""
    from app.utils.session_helpers import session_exists
    
    logger.info(f"[Audio] Starting for session={session_id}")
    
    if not session_exists(session_repo, session_id):
        logger.info(f"[Audio] Session {session_id} deleted - skipping")
        return
    
    session = session_repo.get_session(session_id)
    storage_prefix = session.get("storage_prefix", "")
    
    session_repo.update_session_fields(session_id, current_step="generate_audio")
    
    # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ
    script_key = f"{storage_prefix}pipeline/script.json"
    script_data = storage.download_json(script_key)
    
    # LangGraph ë…¸ë“œ ì§ì ‘ í˜¸ì¶œ
    from app.langgraph_pipeline.podcast.graph import generate_audio_node
    
    state = {
        "script": script_data["script"],
        "host_name": options.get("host1", "Fenrir"),
        "guest_name": options.get("host2", ""),
        "usage": script_data.get("usage", {}),
        "errors": [],
    }
    
    state = generate_audio_node(state)
    
    if not session_exists(session_repo, session_id):
        logger.info(f"[Audio] Session {session_id} deleted during execution")
        return
    
    if state.get("errors"):
        raise Exception(f"Audio generation failed: {state['errors']}")
    
    # ì˜¤ë””ì˜¤ íŒŒì¼ë“¤ì„ Blobì— ì—…ë¡œë“œ
    audio_parts_dir = f"{storage_prefix}pipeline/audio_parts/"
    wav_files = state["wav_files"]
    
    uploaded_audio_keys = []
    for i, wav_path in enumerate(wav_files):
        with open(wav_path, 'rb') as f:
            audio_data = f.read()
        
        audio_key = f"{audio_parts_dir}part_{i}.wav"
        storage.upload_bytes(audio_key, audio_data, content_type="audio/wav")
        uploaded_audio_keys.append(audio_key)
        
        # ë¡œì»¬ ì„ì‹œ íŒŒì¼ ì‚­ì œ
        try:
            os.remove(wav_path)
        except Exception:
            pass
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    audio_metadata_key = f"{storage_prefix}pipeline/audio_metadata.json"
    audio_data = {
        "audio_metadata": state["audio_metadata"],
        "audio_parts_keys": uploaded_audio_keys,
        "usage": state.get("usage", {}),
    }
    storage.upload_json(audio_metadata_key, audio_data)
    
    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
    progress_key = f"{storage_prefix}pipeline/progress.json"
    progress_data = storage.download_json(progress_key)
    progress_data["completed_steps"].append("audio")
    progress_data["current_step"] = "finalize"
    progress_data["intermediate_keys"]["audio_metadata"] = audio_metadata_key
    storage.upload_json(progress_key, progress_data)
    
    session_repo.update_session_fields(session_id, current_step="audio_complete")
    
    logger.info(f"[Audio] âœ… Completed for session={session_id}")
    
    # ë‹¤ìŒ ë‹¨ê³„ íì‰
    from app.services.queue_service import enqueue_pipeline_step
    enqueue_pipeline_step(
        session_id=session_id,
        channel_id=channel_id,
        step="finalize",
        options=options,
    )


def _run_finalize_step(session_id, channel_id, options, storage, session_repo):
    """4ë‹¨ê³„: ìµœì¢… ë³‘í•© + íŠ¸ëœìŠ¤í¬ë¦½íŠ¸"""
    import tempfile
    from app.utils.session_helpers import session_exists
    
    logger.info(f"[Finalize] Starting for session={session_id}")
    
    if not session_exists(session_repo, session_id):
        logger.info(f"[Finalize] Session {session_id} deleted - skipping")
        return
    
    session = session_repo.get_session(session_id)
    storage_prefix = session.get("storage_prefix", "")
    
    session_repo.update_session_fields(session_id, current_step="merge_audio")
    
    # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ
    script_key = f"{storage_prefix}pipeline/script.json"
    audio_metadata_key = f"{storage_prefix}pipeline/audio_metadata.json"
    
    script_data = storage.download_json(script_key)
    audio_data = storage.download_json(audio_metadata_key)
    
    # ì˜¤ë””ì˜¤ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    audio_parts_keys = audio_data["audio_parts_keys"]
    temp_wav_files = []
    
    try:
        for audio_key in audio_parts_keys:
            audio_bytes = storage.download(audio_key)
            
            temp_fd, temp_path = tempfile.mkstemp(suffix=".wav")
            with os.fdopen(temp_fd, 'wb') as f:
                f.write(audio_bytes)
            
            temp_wav_files.append(temp_path)
        
        # LangGraph ë…¸ë“œ ì§ì ‘ í˜¸ì¶œ
        from app.langgraph_pipeline.podcast.graph import merge_audio_node, generate_transcript_node
        
        state = {
            "wav_files": temp_wav_files,
            "audio_metadata": audio_data["audio_metadata"],
            "script": script_data["script"],
            "title": script_data["title"],
            "usage": audio_data.get("usage", {}),
            "errors": [],
        }
        
        # ë³‘í•©
        state = merge_audio_node(state)
        
        if not session_exists(session_repo, session_id):
            logger.info(f"[Finalize] Session {session_id} deleted during merge")
            return
        
        if state.get("errors"):
            raise Exception(f"Audio merge failed: {state['errors']}")
        
        session_repo.update_session_fields(session_id, current_step="merge_complete")
        
        # íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ìƒì„±
        state = generate_transcript_node(state)
        
        if not session_exists(session_repo, session_id):
            logger.info(f"[Finalize] Session {session_id} deleted during transcript")
            return
        
        # ìµœì¢… íŒŒì¼ ì—…ë¡œë“œ
        final_audio_path = state["final_podcast_path"]
        transcript_path = state["transcript_path"]
        
        output_dir = f"{storage_prefix}output_files/"
        
        # ì˜¤ë””ì˜¤ ì—…ë¡œë“œ
        with open(final_audio_path, 'rb') as f:
            audio_bytes = f.read()
        audio_key = f"{output_dir}audio/{os.path.basename(final_audio_path)}"
        storage.upload_bytes(audio_key, audio_bytes, content_type="audio/mpeg")
        
        # ìŠ¤í¬ë¦½íŠ¸ ì—…ë¡œë“œ
        with open(transcript_path, 'rb') as f:
            script_bytes = f.read()
        script_out_key = f"{output_dir}script/{os.path.basename(transcript_path)}"
        storage.upload_bytes(script_out_key, script_bytes, content_type="text/plain")
        
        # ì˜¤ë””ì˜¤ ê¸¸ì´ ê³„ì‚°
        from pydub import AudioSegment
        audio = AudioSegment.from_file(final_audio_path)
        total_duration_sec = int(len(audio) / 1000)
        
        # ìŠ¤í¬ë¦½íŠ¸ í…ìŠ¤íŠ¸ ì½ê¸°
        with open(transcript_path, 'r', encoding='utf-8') as f:
            script_text = f.read()
        
        # ìµœì¢… ì—…ë°ì´íŠ¸
        if not session_exists(session_repo, session_id):
            logger.info(f"[Finalize] Session {session_id} deleted before final update")
            # ì—…ë¡œë“œí•œ íŒŒì¼ ì‚­ì œ
            try:
                storage.delete(audio_key)
                storage.delete(script_out_key)
            except:
                pass
            return
        
        session_repo.update_session_fields(
            session_id,
            status="completed",
            current_step="completed",
            audio_key=audio_key,
            script_key=script_out_key,
            script_text=script_text,
            total_duration_sec=total_duration_sec,
        )
        
        logger.info(f"[Finalize] âœ… Completed for session={session_id}")
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        try:
            os.remove(final_audio_path)
            os.remove(transcript_path)
        except Exception:
            pass
        
    finally:
        # WAV íŒŒì¼ ì •ë¦¬
        for temp_file in temp_wav_files:
            try:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
            except Exception:
                pass