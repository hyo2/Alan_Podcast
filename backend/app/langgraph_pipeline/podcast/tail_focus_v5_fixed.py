"""
Tail Focus V5 - ê¸€ì ìˆ˜ + ë¬¸ì¥ ê°œìˆ˜ ë™ì‹œ ì œí•œ ìµœì¢… ë²„ì „ (ì¤‘ë³µ ë°©ì§€ ê°œì„ !)
150ê°œ+ ë°œí™” ëŒ€ì‘: 2500ì ë˜ëŠ” 50ê°œ ì¤‘ ë¨¼ì € ë„ë‹¬ ì‹œ ë°°ì¹˜ ë¶„í• 

ê°œì„ ì‚¬í•­:
- MAX_BATCH_SIZE = 50 (ë¬¸ì¥ ê°œìˆ˜ ì œí•œ)
- MAX_BATCH_CHARS = 2500 (ê¸€ì ìˆ˜ ì œí•œ, 3000 â†’ 2500 ì•ˆì •ì„± í–¥ìƒ)
- ë‘˜ ì¤‘ ë¨¼ì € ë„ë‹¬í•˜ëŠ” ì¡°ê±´ìœ¼ë¡œ ë°°ì¹˜ ë¶„í• 
- ë¬¸ì¥ ì™„ì „ì„± 100% ë³´ì¥ (ì ˆëŒ€ ì¤‘ê°„ì— ì•ˆ ìë¦„!)
- ì„ ìƒë‹˜ ê¸´ ë°œí™” ì•ˆì „í•˜ê²Œ ì²˜ë¦¬

âœ… ì¤‘ë³µ ë°©ì§€ ê°œì„  (v5.1):
- Tail ê¸¸ì´: 3ë‹¨ì–´ â†’ 5-7ë‹¨ì–´ (ë™ì  ì¡°ì •)
- Search Window: -2~+5ì´ˆ â†’ -1~+2ì´ˆ (ë²”ìœ„ ì¶•ì†Œ)
- ì¤‘ë³µ ë¬¸êµ¬ í•„í„°ë§: ë™ì¼ STT ë¬¸êµ¬ ì¬ì‚¬ìš© ë°©ì§€
- ì‹œê°„ ìš°ì„  ì •ì±…: ê°€ì¤‘ì¹˜ 30% â†’ 50% (ì‹œê°„ìƒ ê°€ê¹Œìš´ í›„ë³´ ìš°ì„ )
- ì‹œê°„ ë²”ìœ„ í•„í„°ë§: ì˜ˆìƒ ì‹œê°„ Â±3ì´ˆ ì´ë‚´ë§Œ ê³ ë ¤
"""

import os
import logging
import wave
import json
import requests
import base64
import time
import difflib
import re
import uuid
import numpy as np
from pathlib import Path
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
from collections import deque
from dotenv import load_dotenv
from google.oauth2 import service_account
from google.auth.transport.requests import Request
from google.cloud import speech

# âœ… ë¹„ìš© ê³„ì‚° ìœ í‹¸ë¦¬í‹°
try:
    from app.langgraph_pipeline.podcast.pricing import calculate_tts_cost, calculate_stt_cost, format_cost
except ImportError:
    # ë…ë¦½ ì‹¤í–‰ ì‹œì—ëŠ” ë¹„ìš© ê³„ì‚° ìŠ¤í‚µ
    def calculate_tts_cost(chars): return 0.0
    def calculate_stt_cost(secs): return 0.0
    def format_cost(usd, include_krw=True): return f"${usd:.4f}"

load_dotenv()

logger = logging.getLogger(__name__)

@dataclass
class Dialogue:
    speaker: str
    text: str


class TailFocusV5Generator:
    """Tail Focus V5 Generator (ê¸€ì ìˆ˜ + ë¬¸ì¥ ê°œìˆ˜ ë™ì‹œ ì œí•œ)"""
    
    # âœ… ë°°ì¹˜ ì œí•œ (ë‘˜ ì¤‘ ë¨¼ì € ë„ë‹¬í•˜ë©´ ë¶„í• !)
    MAX_BATCH_SIZE = 50      # ìµœëŒ€ ë¬¸ì¥ ê°œìˆ˜
    MAX_BATCH_CHARS = 2500   # ìµœëŒ€ ê¸€ì ìˆ˜ (3000 â†’ 2500, ì•ˆì •ì„± í–¥ìƒ)
    
    def __init__(
        self,
        credentials_file: str = "./vertex-ai-service-account.json",
        output_dir: str = "podcast_tail_v5",
        host_voice: str = "Kore",
        guest_voice: str = "Leda",
        tts_model_name: str = "gemini-2.5-flash-preview-tts",
        tts_region: str = "us-central1",
        separator_text: str = "\n\n\n\n\n",
        tail_thresholds: List[float] = None,
        top_n_candidates: int = 10,
        silence_threshold: int = 500,
        silence_min_duration: float = 0.05,
        boundary_search_window: float = 1.0,
        default_margin: float = 0.2
    ):
        self.credentials_file = credentials_file
        self.output_dir = output_dir
        self.host_voice = host_voice
        self.guest_voice = guest_voice
        self.tts_model_name = tts_model_name
        self.tts_region = tts_region
        self.separator_text = separator_text
        self.tail_thresholds = tail_thresholds or [0.70, 0.60, 0.50]
        self.top_n_candidates = top_n_candidates
        self.silence_threshold = silence_threshold
        self.silence_min_duration = silence_min_duration
        self.boundary_search_window = boundary_search_window
        self.default_margin = default_margin
        
        # âœ… ì„¸ì…˜ ê³ ìœ  ID (íŒŒì¼ëª… ì¶©ëŒ ë°©ì§€!)
        self.session_id = uuid.uuid4().hex[:8]
        
        # ì¬ì‹œë„ ì„¤ì •
        self.retry_delays = [2.0, 4.0, 8.0]
        
        # ì„±ëŠ¥ ì¸¡ì • ë³€ìˆ˜
        self.tts_time = 0.0
        self.stt_time = 0.0
        self.segment_time = 0.0
        self.merge_time = 0.0
        self.api_calls = 0
        self.error_429_count = 0
        self.retry_count = 0
        
        self.output_path = Path(output_dir).resolve()
        self.output_path.mkdir(parents=True, exist_ok=True)
        
        # ë³µí•©ì–´ ì‚¬ì „ (ìµœì†Œí™”)
        self.compound_mapping = {
            'AI': 'ì—ì´ì•„ì´', 'API': 'ì—ì´í”¼ì•„ì´', 'URL': 'ìœ ì•Œì—˜',
            'COVID': 'ì½”ë¹„ë“œ', 'RNA': 'ì•Œì—”ì—ì´', 'DNA': 'ë””ì—”ì—ì´'
        }
        
        # ì˜ë¬¸ì ë§¤í•‘
        self.char_mapping = {
            'A': 'ì—ì´', 'B': 'ë¹„', 'C': 'ì”¨', 'D': 'ë””', 'E': 'ì´',
            'F': 'ì—í”„', 'G': 'ì§€', 'H': 'ì—ì´ì¹˜', 'I': 'ì•„ì´', 'J': 'ì œì´',
            'K': 'ì¼€ì´', 'L': 'ì—˜', 'M': 'ì— ', 'N': 'ì—”', 'O': 'ì˜¤',
            'P': 'í”¼', 'Q': 'í', 'R': 'ì•Œ', 'S': 'ì—ìŠ¤', 'T': 'í‹°',
            'U': 'ìœ ', 'V': 'ë¸Œì´', 'W': 'ë”ë¸”ìœ ', 'X': 'ì—‘ìŠ¤', 'Y': 'ì™€ì´', 'Z': 'ì œíŠ¸'
        }
        
        self._setup_auth()
    
    def _setup_auth(self):
        """ì¸ì¦ ì„¤ì •"""
        with open(self.credentials_file, 'r') as f:
            creds_data = json.load(f)
            self.project_id = creds_data.get("project_id")
        
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = str(
            Path(self.credentials_file).resolve()
        )
        
        self.creds = service_account.Credentials.from_service_account_file(
            self.credentials_file,
            scopes=['https://www.googleapis.com/auth/cloud-platform']
        )
        self.creds.refresh(Request())
        self.speech_client = speech.SpeechClient(credentials=self.creds)
    
    def _get_vertex_headers(self):
        """Vertex AI í—¤ë”"""
        if self.creds.expired:
            self.creds.refresh(Request())
        return {
            "Authorization": f"Bearer {self.creds.token}",
            "Content-Type": "application/json; charset=utf-8"
        }
    
    def _get_retry_delay(self, attempt: int) -> float:
        """ì¬ì‹œë„ ì§€ì—° ì‹œê°„ ê³„ì‚°"""
        if attempt < len(self.retry_delays):
            return self.retry_delays[attempt]
        else:
            return self.retry_delays[-1]
    
    def _normalize_text(self, text: str) -> str:
        """ì˜ì–´ë¥¼ í•œê¸€ ë°œìŒìœ¼ë¡œ ë³€í™˜"""
        for eng, kor in self.compound_mapping.items():
            text = text.replace(eng, kor)
            text = text.replace(eng.lower(), kor)
        
        result = []
        for char in text.upper():
            if char in self.char_mapping:
                result.append(self.char_mapping[char])
            else:
                result.append(char)
        text = "".join(result)
        
        return re.sub(r'[^ê°€-í£]', '', text)
    
    # =========================================================================
    # ë°°ì¹˜ ë¶„í•  (ê¸€ì ìˆ˜ + ë¬¸ì¥ ê°œìˆ˜!)
    # =========================================================================
    
    def _split_into_batches(self, texts: List[str]) -> List[List[str]]:
        """
        ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë°°ì¹˜ ë¶„í•  (ì ˆëŒ€ ë¬¸ì¥ ì¤‘ê°„ ì•ˆ ìë¦„!)
        
        ì¡°ê±´:
        - MAX_BATCH_SIZE (50ê°œ) ë„ë‹¬ â†’ ë¶„í• 
        - MAX_BATCH_CHARS (2500ì) ì´ˆê³¼ ì˜ˆìƒ â†’ ë¶„í•  (3000 â†’ 2500 ê°œì„ )
        """
        batches = []
        current_batch = []
        current_chars = 0
        
        for text in texts:
            text_len = len(text)
            
            # ì¡°ê±´ 1: ë¬¸ì¥ ê°œìˆ˜ ë„ë‹¬
            # ì¡°ê±´ 2: ê¸€ì ìˆ˜ ì´ˆê³¼ ì˜ˆìƒ
            if (len(current_batch) >= self.MAX_BATCH_SIZE or 
                (current_batch and current_chars + text_len > self.MAX_BATCH_CHARS)):
                
                # í˜„ì¬ ë°°ì¹˜ ì™„ë£Œ (ì´ ë¬¸ì¥ ì œì™¸!)
                batches.append(current_batch)
                current_batch = []
                current_chars = 0
            
            # ì´ ë¬¸ì¥ì„ í˜„ì¬ ë°°ì¹˜ì— ì¶”ê°€ (ì™„ì „í•œ ë¬¸ì¥!)
            current_batch.append(text)
            current_chars += text_len
        
        # ë§ˆì§€ë§‰ ë°°ì¹˜
        if current_batch:
            batches.append(current_batch)
        
        return batches
    
    # =========================================================================
    # TTS (ë°°ì¹˜ ë¶„í• !)
    # =========================================================================
    
    def _merge_wav_files(self, wav_files: List[str], output_path: str):
        """ì—¬ëŸ¬ WAV íŒŒì¼ì„ í•˜ë‚˜ë¡œ ë³‘í•©"""
        logger.info(f"  ğŸ”— {len(wav_files)}ê°œ ë°°ì¹˜ WAV ë³‘í•© ì¤‘...")
        
        # ì²« ë²ˆì§¸ íŒŒì¼ì—ì„œ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
        with wave.open(wav_files[0], 'rb') as w:
            params = w.getparams()
        
        # ëª¨ë“  ì˜¤ë””ì˜¤ ë°ì´í„° ê²°í•©
        combined_data = bytearray()
        for wav_file in wav_files:
            with wave.open(wav_file, 'rb') as w:
                combined_data.extend(w.readframes(w.getnframes()))
        
        # ìµœì¢… íŒŒì¼ ì €ì¥
        with wave.open(output_path, 'wb') as w:
            w.setparams(params)
            w.writeframes(combined_data)
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        for wav_file in wav_files:
            if os.path.exists(wav_file):
                os.remove(wav_file)
        
        logger.info(f"  âœ… ë°°ì¹˜ ë³‘í•© ì™„ë£Œ")
    
    def _generate_single_batch(
        self, 
        texts: List[str], 
        voice: str, 
        output_path: str
    ):
        """ë‹¨ì¼ ë°°ì¹˜ TTS ìƒì„± (ë¬´í•œ ì¬ì‹œë„)"""
        full_text = self.separator_text.join(texts)
        
        url = (
            f"https://{self.tts_region}-aiplatform.googleapis.com"
            f"/v1beta1/projects/{self.project_id}"
            f"/locations/{self.tts_region}"
            f"/publishers/google/models/{self.tts_model_name}:generateContent"
        )
        
        prompt = f"Read naturally in Korean. Please PAUSE clearly between sentences.\nText:\n{full_text}"
        
        data = {
            "contents": [{"role": "user", "parts": [{"text": prompt}]}],
            "generation_config": {
                "response_modalities": ["AUDIO"],
                "speech_config": {"voice_config": {"prebuilt_voice_config": {"voice_name": voice}}}
            }
        }
        
        # ë¬´í•œ ì¬ì‹œë„
        attempt = 0
        while True:
            self.api_calls += 1
            
            try:
                res = requests.post(
                    url, 
                    headers=self._get_vertex_headers(), 
                    json=data,
                    timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                )
                
                if res.status_code == 200:
                    audio_data = base64.b64decode(
                        res.json()["candidates"][0]["content"]["parts"][0]["inlineData"]["data"]
                    )
                    with wave.open(output_path, 'wb') as f:
                        f.setnchannels(1)
                        f.setsampwidth(2)
                        f.setframerate(24000)
                        f.writeframes(audio_data)
                    return
                
                elif res.status_code == 429:
                    self.error_429_count += 1
                    self.retry_count += 1
                    delay = self._get_retry_delay(attempt)
                    logger.warning(f"      âš ï¸  429 ì—ëŸ¬ â†’ {delay:.1f}ì´ˆ í›„ ì¬ì‹œë„ ({attempt+1}íšŒ)")
                    time.sleep(delay)
                    attempt += 1
                else:
                    raise Exception(f"TTS Error: {res.status_code} - {res.text}")
                    
            except Exception as e:
                logger.error(f"      âŒ ì˜ˆì™¸ ë°œìƒ: {e}")
                self.retry_count += 1
                delay = self._get_retry_delay(attempt)
                time.sleep(delay)
                attempt += 1
    
    def _generate_batch_audio(self, texts: List[str], voice: str, output_path: str):
        """ë°°ì¹˜ TTS ìƒì„± (ê¸€ì ìˆ˜ + ë¬¸ì¥ ê°œìˆ˜ ë™ì‹œ ì œí•œ)"""
        # gusetê°€ 0ì´ë©´ ì¢…ë£Œ
        if not texts:
            return
        
        # âœ… ê¸°ì¡´ ì˜¤ë””ì˜¤ ì¬ì‚¬ìš© ë¡œì§ ì œê±° (í”„ë¡œë•ì…˜ ì•ˆì •ì„±)
        
        total_texts = len(texts)
        total_chars = sum(len(t) for t in texts)
        avg_chars = total_chars / total_texts if total_texts > 0 else 0
        
        logger.info(f"  ğŸ”Š TTS ìƒì„± ì¤‘...")
        logger.info(f"     ë¬¸ì¥ ìˆ˜: {total_texts}ê°œ")
        logger.info(f"     ì´ ê¸€ììˆ˜: {total_chars}ì (í‰ê· : {avg_chars:.0f}ì/ë¬¸ì¥)")
        
        # âœ… ë°°ì¹˜ ë¶„í•  (ê¸€ì ìˆ˜ + ë¬¸ì¥ ê°œìˆ˜ ë™ì‹œ ì²´í¬!)
        batches = self._split_into_batches(texts)
        
        if len(batches) == 1:
            # ë‹¨ì¼ ë°°ì¹˜
            logger.info(f"     ì „ëµ: ë‹¨ì¼ ë°°ì¹˜ ({len(batches[0])}ê°œ, {sum(len(t) for t in batches[0])}ì)")
            self._generate_single_batch(batches[0], voice, output_path)
            logger.info(f"  âœ… TTS ì™„ë£Œ")
        else:
            # ë°°ì¹˜ ë¶„í• 
            logger.info(f"     ì „ëµ: {len(batches)}ê°œ ë°°ì¹˜ë¡œ ë¶„í• ")
            for i, batch in enumerate(batches):
                batch_chars = sum(len(t) for t in batch)
                logger.info(f"       ë°°ì¹˜ {i+1}: {len(batch)}ê°œ ë¬¸ì¥, {batch_chars}ì")
            
            temp_wavs = []
            for batch_idx, batch_texts in enumerate(batches):
                # âœ… ê³ ìœ í•œ ì„ì‹œ íŒŒì¼ëª… (session_id í¬í•¨)
                temp_wav = str(self.output_path / f"temp_batch_{batch_idx}_{self.session_id}_{voice}.wav")
                
                batch_chars = sum(len(t) for t in batch_texts)
                logger.info(f"     ë°°ì¹˜ {batch_idx+1}/{len(batches)}: {len(batch_texts)}ê°œ ë¬¸ì¥, {batch_chars}ì ìƒì„± ì¤‘...")
                
                self._generate_single_batch(batch_texts, voice, temp_wav)
                temp_wavs.append(temp_wav)
                
                # ë°°ì¹˜ ê°„ ì§§ì€ ëŒ€ê¸°
                if batch_idx < len(batches) - 1:
                    time.sleep(1.0)
            
            # ë°°ì¹˜ ë³‘í•©
            self._merge_wav_files(temp_wavs, output_path)
            logger.info(f"  âœ… TTS ì™„ë£Œ ({len(batches)}ê°œ ë°°ì¹˜)")
    
    def _transcribe_audio(self, wav_path: str) -> List[Dict]:
        """STT ë³€í™˜"""
        with wave.open(wav_path, "rb") as wav:
            rate = wav.getframerate()
            content = wav.readframes(wav.getnframes())
        
        chunk_len = 50 * rate * 2
        all_words = []
        
        logger.info(f"  ğŸ§ STT ë³€í™˜ ì¤‘... ({os.path.basename(wav_path)})")
        
        for i, start_byte in enumerate(range(0, len(content), chunk_len)):
            chunk = content[start_byte:start_byte + chunk_len]
            if len(chunk) < 100:
                continue
            
            try:
                resp = self.speech_client.recognize(
                    config=speech.RecognitionConfig(
                        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
                        sample_rate_hertz=rate,
                        language_code="ko-KR",
                        enable_word_time_offsets=True
                    ),
                    audio=speech.RecognitionAudio(content=chunk)
                )
                
                time_offset = start_byte / (rate * 2)
                
                for res in resp.results:
                    for w in res.alternatives[0].words:
                        all_words.append({
                            "word": w.word,
                            "start": round(w.start_time.total_seconds() + time_offset, 3),
                            "end": round(w.end_time.total_seconds() + time_offset, 3)
                        })
            except Exception as e:
                logger.error(f"    âš ï¸  STT ì²­í¬ {i} ì‹¤íŒ¨: {e}")
        
        logger.info(f"  âœ… STT ì™„ë£Œ ({len(all_words)}ê°œ ë‹¨ì–´)")
        return all_words
    
    # =========================================================================
    # ì •ë°€ ê²½ê³„ ê°ì§€
    # =========================================================================
    
    def _find_precise_boundary(self, wav_path: str, tail_end_time: float) -> float:
        """ê¼¬ë¦¬ ì´í›„ ì •ë°€í•œ ë¬µìŒ ê²½ê³„ ì°¾ê¸°"""
        with wave.open(wav_path, 'rb') as w:
            rate = w.getframerate()
            w.setpos(0)
            audio_data = np.frombuffer(
                w.readframes(w.getnframes()),
                dtype=np.int16
            )
        
        start_sample = int(tail_end_time * rate)
        end_sample = int((tail_end_time + self.boundary_search_window) * rate)
        
        if end_sample > len(audio_data):
            end_sample = len(audio_data)
        
        search_segment = audio_data[start_sample:end_sample]
        window_size = int(0.01 * rate)
        
        silence_start = None
        silence_duration = 0
        
        for i in range(0, len(search_segment) - window_size, window_size // 2):
            window = search_segment[i:i+window_size]
            energy = np.abs(window).mean()
            
            if energy < self.silence_threshold:
                if silence_start is None:
                    silence_start = i
                silence_duration += (window_size / 2) / rate
                
                if silence_duration >= self.silence_min_duration:
                    precise_end = tail_end_time + (silence_start / rate)
                    return round(precise_end, 3)
            else:
                silence_start = None
                silence_duration = 0
        
        return round(tail_end_time + self.default_margin, 3)
    
    # =========================================================================
    # í›„ë³´êµ° ê¼¬ë¦¬ ì°¾ê¸°
    # =========================================================================
    
    def _find_tail_with_candidates(
        self,
        all_words: List[Dict],
        text: str,
        search_start_idx: int,
        expected_start_time: float
    ) -> Tuple[bool, float, str, float, int]:
        """í›„ë³´êµ° ë°©ì‹ìœ¼ë¡œ ê¼¬ë¦¬ ì°¾ê¸° (ì¤‘ë³µ ë°©ì§€ ê°œì„ !)"""
        
        # âœ… ê°œì„ : Tail ê¸¸ì´ ì¦ê°€ (3ë‹¨ì–´ â†’ 5-7ë‹¨ì–´)
        # - ì§§ì€ íŒ¨í„´("2ë‹¨ê³„") ì¤‘ë³µ ë§¤ì¹­ ë°©ì§€
        # - í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¼ ë™ì  ì¡°ì •
        words = text.strip().split()
        tail_len = min(7, max(5, len(words) // 3))  # ìµœì†Œ 5, ìµœëŒ€ 7ë‹¨ì–´
        tail_words = words[-tail_len:]
        
        tail_raw = "".join(tail_words)
        target_tail = self._normalize_text(tail_raw)
        
        candidates = []
        
        # âœ… ê°œì„ : Search Window ì¶•ì†Œ (-2~+5ì´ˆ â†’ -1~+2ì´ˆ)
        # - ë‹¤ìŒ ë°œí™”ê¹Œì§€ ê²€ìƒ‰ ë²”ìœ„ í™•ì¥ ë°©ì§€
        estimated_duration = len(text) * 0.20
        search_window_start = max(expected_start_time - 1.0, 0)  # 2ì´ˆ â†’ 1ì´ˆ
        search_window_end = expected_start_time + estimated_duration + 2.0  # 5ì´ˆ â†’ 2ì´ˆ
        
        # âœ… ê°œì„ : ì¤‘ë³µ ë¬¸êµ¬ í•„í„°ë§
        # - ë™ì¼í•œ STT ë¬¸êµ¬ ì¬ì‚¬ìš© ë°©ì§€
        seen_phrases = set()
        
        for j in range(len(all_words)):
            if all_words[j]['start'] < search_window_start:
                continue
            if all_words[j]['start'] > search_window_end:
                break
            
            for window_size in [2, 3, 4, 5, 6, 7, 8, 9, 10]:  # ìœˆë„ìš° í¬ê¸° í™•ì¥ (tail ê¸¸ì´ ì¦ê°€ ëŒ€ì‘)
                if j + window_size > len(all_words):
                    continue
                
                stt_phrase_raw = "".join([
                    w['word'] for w in all_words[j:j+window_size]
                ])
                stt_phrase_norm = self._normalize_text(stt_phrase_raw)
                
                # âœ… ì¤‘ë³µ ë°©ì§€: ì´ë¯¸ ì‚¬ìš©í•œ ë¬¸êµ¬ëŠ” ìŠ¤í‚µ
                if stt_phrase_norm in seen_phrases:
                    continue
                
                score = difflib.SequenceMatcher(
                    None, target_tail, stt_phrase_norm
                ).ratio()
                
                if score > 0.50:
                    time_diff = abs(all_words[j]['start'] - expected_start_time)
                    
                    # âœ… ì‹œê°„ ë²”ìœ„ í•„í„°ë§: ë„ˆë¬´ ë¨¼ í›„ë³´ëŠ” ì œì™¸
                    if time_diff > estimated_duration + 3.0:  # ì˜ˆìƒ ì‹œê°„ Â± 3ì´ˆ ì´ë‚´ë§Œ
                        continue
                    
                    seen_phrases.add(stt_phrase_norm)  # ì‚¬ìš© ê¸°ë¡
                    
                    candidates.append({
                        "score": score,
                        "end_time": all_words[j+window_size-1]['end'],
                        "phrase": stt_phrase_raw,
                        "idx": j + window_size,
                        "time_diff": time_diff
                    })
        
        if not candidates:
            return False, 0.0, "", 0.0, search_start_idx
        
        # âœ… ê°œì„ : ì‹œê°„ ìš°ì„  ì •ì±… ê°•í™”
        # - ì‹œê°„ ê°€ì¤‘ì¹˜: 30% â†’ 50% (ì‹œê°„ìƒ ê°€ê¹Œìš´ í›„ë³´ ìš°ì„ )
        for c in candidates:
            time_score = 1.0 / (1.0 + c['time_diff'])
            c['combined_score'] = c['score'] * 0.5 + time_score * 0.5  # 50:50
        
        candidates.sort(key=lambda x: -x['combined_score'])
        
        for threshold in self.tail_thresholds:
            for c in candidates:
                if c['score'] >= threshold:
                    return True, c['end_time'], c['phrase'], c['score'], c['idx']
        
        best = candidates[0]
        return True, best['end_time'], best['phrase'], best['score'], best['idx']
    
    # =========================================================================
    # ë¬¸ì¥ ë¶„í• 
    # =========================================================================
    
    def _find_segments_robust(
        self,
        wav_path: str,
        all_words: List[Dict],
        texts: List[str]
    ) -> List[Dict]:
        """ê°•í™”ëœ ë¬¸ì¥ ë¶„í•  (ì„¸ê·¸ë¨¼íŠ¸ ê°œìˆ˜ ë³´ì¥!)"""
        logger.info(f"\n  ğŸ§© ê°•í™”ëœ ë¬¸ì¥ ë¶„í•  (í›„ë³´êµ° ë°©ì‹)...")
        
        with wave.open(wav_path, 'rb') as w:
            total_duration = w.getnframes() / w.getframerate()
        
        logger.info(f"     ì˜¤ë””ì˜¤ ì´ ê¸¸ì´: {total_duration:.1f}ì´ˆ")
        
        segments = []
        stt_search_idx = 0
        current_start = 0.0
        
        for i, text in enumerate(texts):
            is_last = (i == len(texts) - 1)
            
            if is_last:
                final_end = total_duration
            else:
                success, found_end, best_phrase, score, next_idx = self._find_tail_with_candidates(
                    all_words, text, stt_search_idx, current_start
                )
                
                if success:
                    precise_end = self._find_precise_boundary(wav_path, found_end)
                    stt_search_idx = next_idx
                    final_end = precise_end
                else:
                    if segments:
                        avg_duration = sum([s['end'] - s['start'] for s in segments]) / len(segments)
                        final_end = round(current_start + avg_duration, 3)
                    else:
                        final_end = round(current_start + len(text) * 0.15, 3)
                
                if final_end > total_duration:
                    final_end = total_duration
            
            segments.append({
                "start": round(current_start, 3),
                "end": final_end
            })
            
            current_start = final_end
        
        # âœ… ì„¸ê·¸ë¨¼íŠ¸ ê°œìˆ˜ ê²€ì¦ ë° ë³´ì¥!
        if len(segments) != len(texts):
            logger.warning(f"  âš ï¸  ì„¸ê·¸ë¨¼íŠ¸ ê°œìˆ˜ ë¶ˆì¼ì¹˜ ê°ì§€!")
            logger.info(f"     í…ìŠ¤íŠ¸: {len(texts)}ê°œ, ì„¸ê·¸ë¨¼íŠ¸: {len(segments)}ê°œ")
            
            # ë¶€ì¡±í•˜ë©´ ì¶”ê°€
            MIN_DURATION = 0.5  # ìµœì†Œ 0.5ì´ˆ ë³´ì¥
            
            while len(segments) < len(texts):
                last_end = segments[-1]['end'] if segments else 0.0
                
                # âœ… ì˜¤ë””ì˜¤ ëì— ë„ë‹¬í–ˆëŠ”ì§€ ì²´í¬
                if last_end >= total_duration - 0.01:  # 0.01ì´ˆ ì—¬ìœ 
                    logger.warning(f"     âš ï¸  ì˜¤ë””ì˜¤ ë ë„ë‹¬, ë” ì´ìƒ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ê°€ ë¶ˆê°€")
                    logger.warning(f"        í…ìŠ¤íŠ¸ {len(texts)}ê°œ ì¤‘ {len(segments)}ê°œë§Œ ë§¤ì¹­ë¨")
                    logger.warning(f"        â†’ ìŠ¤í¬ë¦½íŠ¸ ê¸¸ì´ì— ë¹„í•´ TTS ì˜¤ë””ì˜¤ê°€ ì§§ìŠµë‹ˆë‹¤")
                    break
                
                # í‰ê·  duration ê³„ì‚°
                if segments:
                    avg_dur = sum([s['end'] - s['start'] for s in segments]) / len(segments)
                else:
                    avg_dur = 5.0
                
                # âœ… ìµœì†Œ duration ë³´ì¥
                new_end = min(last_end + avg_dur, total_duration)
                actual_duration = new_end - last_end
                
                if actual_duration < MIN_DURATION:
                    logger.warning(f"     âš ï¸  ì„¸ê·¸ë¨¼íŠ¸ ì¶”ê°€ ë¶ˆê°€ (ë‚¨ì€ ì‹œê°„ ë¶€ì¡±: {actual_duration:.2f}ì´ˆ < {MIN_DURATION}ì´ˆ)")
                    logger.warning(f"        í…ìŠ¤íŠ¸ {len(texts)}ê°œ ì¤‘ {len(segments)}ê°œë§Œ ë§¤ì¹­ë¨")
                    break
                
                new_seg = {
                    'start': last_end,
                    'end': new_end
                }
                segments.append(new_seg)
                logger.info(f"     ì„¸ê·¸ë¨¼íŠ¸ ì¶”ê°€: {len(segments)}ë²ˆì§¸ ({new_seg['start']:.1f}ì´ˆ~{new_seg['end']:.1f}ì´ˆ, duration={actual_duration:.2f}ì´ˆ)")
            
            # ë„ˆë¬´ ë§ìœ¼ë©´ ì œê±°
            while len(segments) > len(texts):
                removed = segments.pop()
                logger.info(f"     ì„¸ê·¸ë¨¼íŠ¸ ì œê±°: {len(segments)+1}ë²ˆì§¸")
        
        logger.info(f"  âœ… ìµœì¢… ì„¸ê·¸ë¨¼íŠ¸: {len(segments)}ê°œ (í…ìŠ¤íŠ¸: {len(texts)}ê°œ)")
        
        # ============================================================
        # âœ… ì„¸ê·¸ë¨¼íŠ¸ ê²€ì¦ (ë¹„ì •ìƒ duration ê°ì§€)
        # ============================================================
        MAX_SEGMENT_DURATION = 60.0  # 60ì´ˆ ì´ˆê³¼ ì‹œ ê²½ê³ 

        for i, seg in enumerate(segments):
            duration = seg['end'] - seg['start']
            
            # ë¹„ì •ìƒì ìœ¼ë¡œ ê¸´ ì„¸ê·¸ë¨¼íŠ¸ ê°ì§€
            if duration > MAX_SEGMENT_DURATION:
                logger.error(f"âŒ ë¹„ì •ìƒ ì„¸ê·¸ë¨¼íŠ¸ ê°ì§€!")
                logger.error(f"   ì„¸ê·¸ë¨¼íŠ¸ {i+1}: {duration:.1f}ì´ˆ (ìµœëŒ€: {MAX_SEGMENT_DURATION}ì´ˆ)")
                logger.error(f"   í…ìŠ¤íŠ¸: {texts[i][:100] if i < len(texts) else 'N/A'}...")
                
                # ì˜µì…˜ 1: ê²½ê³ ë§Œ (í˜„ì¬)
                logger.warning(f"âš ï¸  ë¹„ì •ìƒ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤ (ìˆ˜ë™ í™•ì¸ í•„ìš”)")
                
                # ì˜µì…˜ 2: ì—ëŸ¬ ë°œìƒ (ê¶Œì¥)
                # raise ValueError(
                #     f"ë¹„ì •ìƒì ìœ¼ë¡œ ê¸´ ì„¸ê·¸ë¨¼íŠ¸ ê°ì§€: {duration:.1f}ì´ˆ > {MAX_SEGMENT_DURATION}ì´ˆ. "
                #     f"ìŠ¤í¬ë¦½íŠ¸ì— ì¤‘ë³µ ë˜ëŠ” ë¶ˆì™„ì „í•œ ë°œí™”ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                # )
            
            # ìŒìˆ˜ durationë„ ì²´í¬
            if duration < 0:
                logger.error(f"âŒ ìŒìˆ˜ ì„¸ê·¸ë¨¼íŠ¸ ê°ì§€!")
                logger.error(f"   ì„¸ê·¸ë¨¼íŠ¸ {i+1}: {duration:.1f}ì´ˆ")
                raise ValueError(f"ìŒìˆ˜ duration ê°ì§€: {duration:.1f}ì´ˆ")

        logger.info(f"âœ… ì„¸ê·¸ë¨¼íŠ¸ ê²€ì¦ ì™„ë£Œ: {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸ (ìµœëŒ€: {max([s['end']-s['start'] for s in segments]):.1f}ì´ˆ)")

        return segments
    
    # =========================================================================
    # ë³‘í•©
    # =========================================================================
    
    def _merge_segments_safe(
        self,
        dialogues: List[Dialogue],
        host_wav: str,
        guest_wav: str,
        host_segs: List[Dict],
        guest_segs: List[Dict],
        output_path: str
    ):
        """ì•ˆì „í•œ ì„¸ê·¸ë¨¼íŠ¸ ë³‘í•©"""
        logger.info(f"\n  âœ‚ï¸  ëŒ€ë³¸ ìˆœì„œëŒ€ë¡œ ì¡°ë¦½ ì¤‘...")
        
        with wave.open(host_wav, 'rb') as w:
            host_duration = w.getnframes() / w.getframerate()
        with wave.open(guest_wav, 'rb') as w:
            guest_duration = w.getnframes() / w.getframerate()
        
        logger.info(f"    Host ê¸¸ì´: {host_duration:.1f}ì´ˆ / Guest ê¸¸ì´: {guest_duration:.1f}ì´ˆ")
        
        def extract_audio(path, start, end, max_duration):
            with wave.open(path, 'rb') as w:
                params = w.getparams()
                rate = w.getframerate()
                
                start_sample = int(start * rate)
                end_sample = int(end * rate)
                
                if start_sample < 0:
                    start_sample = 0
                if end_sample > w.getnframes():
                    end_sample = w.getnframes()
                if start_sample >= end_sample:
                    return b"", params
                
                w.setpos(start_sample)
                return w.readframes(end_sample - start_sample), params
        
        host_queue = deque(host_segs)
        guest_queue = deque(guest_segs)
        final_audio = bytearray()
        params = None
        
        logger.info(f"    ì§„í–‰ì: {len(host_queue)}ê°œ / ê²ŒìŠ¤íŠ¸: {len(guest_queue)}ê°œ")
        
        for i, line in enumerate(dialogues):
            if line.speaker == "host":
                if host_queue:
                    seg = host_queue.popleft()
                    data, params = extract_audio(host_wav, seg['start'], seg['end'], host_duration)
                    final_audio.extend(data)
            elif line.speaker == "guest":
                if guest_queue:
                    seg = guest_queue.popleft()
                    data, params = extract_audio(guest_wav, seg['start'], seg['end'], guest_duration)
                    final_audio.extend(data)
        
        with wave.open(output_path, 'wb') as f:
            f.setparams(params)
            f.writeframes(final_audio)
        
        logger.info(f"  âœ… ë³‘í•© ì™„ë£Œ: {output_path}")
    
    # =========================================================================
    # Main Pipeline
    # =========================================================================
    
    def generate(self, dialogues: List[Dialogue]):
        """ë©”ì¸ íŒŒì´í”„ë¼ì¸"""
        logger.info("\n" + "="*60)
        logger.info("ğŸš€ Tail Focus V5 Generator ì‹œì‘")
        logger.info("   (ê¸€ì ìˆ˜ + ë¬¸ì¥ ê°œìˆ˜ ë™ì‹œ ì œí•œ)")
        logger.info(f"   Session ID: {self.session_id}")
        logger.info("="*60 + "\n")
        
        host_texts = [d.text for d in dialogues if d.speaker == "host"]
        guest_texts = [d.text for d in dialogues if d.speaker == "guest"]
        
        logger.info(f"ğŸ“Š ëŒ€í™” ë¶„ì„:")
        logger.info(f"   ì§„í–‰ì: {len(host_texts)}ê°œ")
        logger.info(f"   ê²ŒìŠ¤íŠ¸: {len(guest_texts)}ê°œ")
        logger.info(f"   ë°°ì¹˜ ì œí•œ: {self.MAX_BATCH_SIZE}ê°œ ë˜ëŠ” {self.MAX_BATCH_CHARS}ì\n")
        
        # Stage 1: TTS
        logger.info("="*60)
        logger.info("ğŸ“ Stage 1: ë°°ì¹˜ TTS (ê¸€ì ìˆ˜ + ë¬¸ì¥ ê°œìˆ˜ ì œí•œ)")
        logger.info("="*60)
        
        tts_start = time.time()
        
        # âœ… ê³ ìœ í•œ íŒŒì¼ëª… (session_id í¬í•¨)
        host_wav = str(self.output_path / f"host_{self.session_id}.wav")
        guest_wav = str(self.output_path / f"guest_{self.session_id}.wav")
        
        self._generate_batch_audio(host_texts, self.host_voice, host_wav)
        # âœ… guest ë°œí™”ê°€ ì—†ìœ¼ë©´ guest wavë¥¼ ë§Œë“¤ì§€ ì•ŠìŒ
        if guest_texts:
            self._generate_batch_audio(guest_texts, self.guest_voice, guest_wav)
        else:
            guest_wav = None
        
        
        # âœ… TTS ë¬¸ì ìˆ˜ ê³„ì‚°
        host_chars = sum(len(text) for text in host_texts)
        guest_chars = sum(len(text) for text in guest_texts) if guest_texts else 0
        self.total_tts_chars = host_chars + guest_chars
        
        self.tts_time = time.time() - tts_start
        
        # Stage 2: STT
        logger.info("\n" + "="*60)
        logger.info("ğŸ“ Stage 2: STT ë³€í™˜")
        logger.info("="*60)
        
        stt_start = time.time()
        
        host_words = self._transcribe_audio(host_wav)
        # âœ… guestê°€ ì—†ìœ¼ë©´ STT ìŠ¤í‚µ
        if guest_wav:
            guest_words = self._transcribe_audio(guest_wav)
        else:
            guest_words = []
        
        self.stt_time = time.time() - stt_start
        
        # Stage 3: ë¶„í• 
        logger.info("\n" + "="*60)
        logger.info("ğŸ“ Stage 3: ê°•í™”ëœ ë¶„í•  (í›„ë³´êµ° ë°©ì‹)")
        logger.info("="*60)
        
        segment_start = time.time()
        
        host_segs = self._find_segments_robust(host_wav, host_words, host_texts)
        # âœ… guestê°€ ì—†ìœ¼ë©´ ë¶„í•  ìŠ¤í‚µ
        if guest_wav:
            guest_segs = self._find_segments_robust(guest_wav, guest_words, guest_texts)
        else:
            guest_segs = []
        
        self.segment_time = time.time() - segment_start
        
        # Stage 4: ë³‘í•©
        logger.info("\n" + "="*60)
        logger.info("ğŸ“ Stage 4: ì•ˆì „í•œ ë³‘í•©")
        logger.info("="*60)
        
        merge_start = time.time()
        
        # âœ… ê³ ìœ í•œ ìµœì¢… íŒŒì¼ëª… (session_id í¬í•¨)
        final_wav = str(self.output_path / f"podcast_final_{self.session_id}.wav")
        # âœ… guestê°€ ì—†ìœ¼ë©´ hostë§Œìœ¼ë¡œ ë³‘í•©(=ì‚¬ì‹¤ìƒ host ë³µì‚¬)
        if guest_wav:
            self._merge_segments_safe(
                dialogues, host_wav, guest_wav,
                host_segs, guest_segs, final_wav
            )
        else:
            # host_wavë¥¼ ìµœì¢… íŒŒì¼ë¡œ ë³µì‚¬ (wave ë³µì‚¬ë¡œ ì•ˆì „í•˜ê²Œ)
            with wave.open(host_wav, 'rb') as src:
                params = src.getparams()
                frames = src.readframes(src.getnframes())
            with wave.open(final_wav, 'wb') as dst:
                dst.setparams(params)
                dst.writeframes(frames)
        
        self.merge_time = time.time() - merge_start
        
        logger.info("\n" + "="*60)
        logger.info("ğŸ‰ ì™„ë£Œ! ìµœì¢… WAV íŒŒì¼:")
        logger.info(f"   ğŸ“ {final_wav}")
        logger.info("="*60 + "\n")
        
        print("ğŸ“Š ì„±ëŠ¥ ì¸¡ì •:")
        print(f"   TTS: {self.tts_time:.2f}ì´ˆ")
        print(f"   STT: {self.stt_time:.2f}ì´ˆ")
        print(f"   ë¶„í• : {self.segment_time:.2f}ì´ˆ")
        print(f"   ë³‘í•©: {self.merge_time:.2f}ì´ˆ")
        print(f"   ì´: {self.tts_time + self.stt_time + self.segment_time + self.merge_time:.2f}ì´ˆ")
        print(f"   API í˜¸ì¶œ: {self.api_calls}ë²ˆ")
        print(f"   ğŸ’° TTS ë¬¸ì: {self.total_tts_chars:,}ì")
        
        # âœ… ë¹„ìš© ê³„ì‚°
        tts_cost = calculate_tts_cost(self.total_tts_chars)
        stt_cost = calculate_stt_cost(self.stt_time)
        print(f"   ğŸ’µ TTS ë¹„ìš©: {format_cost(tts_cost)}")
        print(f"   ğŸ’µ STT ë¹„ìš©: {format_cost(stt_cost)}")
        # STT ì‹œê°„ì€ ì´ë¯¸ ìœ„ì— ì¶œë ¥ë¨
        print(f"   429 ì—ëŸ¬: {self.error_429_count}ë²ˆ")
        print(f"   ì¬ì‹œë„: {self.retry_count}ë²ˆ")
        print("="*60 + "\n")
        
        # âœ… ìµœì¢… WAV ê²½ë¡œ + ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ ë°˜í™˜ (ì •í™•í•œ íƒ€ì„ìŠ¤íƒ¬í”„ìš©!)
        return final_wav, host_segs, guest_segs


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸: ì„ ìƒë‹˜ ê¸´ ë°œí™” + í•™ìƒ ì§§ì€ ì‘ë‹µ
    dialogues = []
    for i in range(50):
        # ì„ ìƒë‹˜: ê¸´ ì„¤ëª… (100ì)
        dialogues.append(Dialogue(
            "host", 
            f"ì„ ìƒë‹˜ ë°œí™” {i+1}ë²ˆì…ë‹ˆë‹¤. ì˜¤ëŠ˜ì€ ì¤‘ìš”í•œ ê°œë…ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì´ ë‚´ìš©ì€ ì—¬ëŸ¬ë¶„ì˜ í•™ìŠµì— ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤."
        ))
        # í•™ìƒ: ì§§ì€ ì‘ë‹µ (20ì)
        dialogues.append(Dialogue(
            "guest", 
            f"ë„¤, ì´í•´í–ˆì–´ìš”!"
        ))
    
    generator = TailFocusV5Generator(
        tail_thresholds=[0.70, 0.60, 0.50],
        top_n_candidates=10
    )
    generator.generate(dialogues)