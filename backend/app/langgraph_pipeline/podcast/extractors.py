import os
import re
import logging
import requests
from io import BytesIO
from bs4 import BeautifulSoup
from docx import Document
import pdfplumber
from typing import List, Optional

logger = logging.getLogger(__name__)

# ì˜µì…˜: ë‹¤ìš´ë¡œë“œ íŒŒì¼ì„ ë¡œì»¬ì— ì €ì¥í• ì§€ ì—¬ë¶€
SAVE_DOWNLOADED_FILES = False  # Trueë¡œ ì„¤ì •í•˜ë©´ ë””ë²„ê¹… ê°€ëŠ¥

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ì¶œë ¥ í´ë” - ì ˆëŒ€ ê²½ë¡œë¡œ ìˆ˜ì •
OUTPUT_SAVE_DIR = os.path.abspath("outputs/podcasts")
os.makedirs(OUTPUT_SAVE_DIR, exist_ok=True)

# print(f"[EXTRACTORS] Output directory: {OUTPUT_SAVE_DIR}")

class TextExtractor:
    """ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""

    # -------------------------
    # ê³µí†µ - URL ë‹¤ìš´ë¡œë“œ
    # -------------------------
    @staticmethod
    def download_file(url: str, suffix: str) -> BytesIO:
        """
        URLì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë©”ëª¨ë¦¬(BytesIO)ë¡œ ë°˜í™˜
        í•„ìš”ì‹œ ë¡œì»¬ ì €ì¥ë„ ê°€ëŠ¥(SAVE_DOWNLOADED_FILES ì‚¬ìš©)
        """
        try:
            logger.info(f"[DOWNLOAD START] URL: {url[:100]}...")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            resp = requests.get(url, timeout=30, headers=headers, stream=True)
            resp.raise_for_status()
            
            # ì „ì²´ ë‚´ìš©ì„ ë©”ëª¨ë¦¬ì— ì½ê¸°
            file_bytes = resp.content
            logger.info(f"[DOWNLOAD SUCCESS] Size: {len(file_bytes):,} bytes")

            if SAVE_DOWNLOADED_FILES:
                # URLì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ (query parameter ì œê±°)
                filename = url.split("?")[0].split("/")[-1]
                
                # íŒŒì¼ëª…ì´ ë„ˆë¬´ ê¸¸ê±°ë‚˜ ì´ìƒí•˜ë©´ ê¸°ë³¸ ì´ë¦„ ì‚¬ìš©
                if not filename or len(filename) > 100 or not any(c.isalnum() for c in filename):
                    import time
                    filename = f"download_{int(time.time())}{suffix}"
                
                save_path = os.path.join(OUTPUT_SAVE_DIR, filename)
                
                try:
                    with open(save_path, "wb") as f:
                        f.write(file_bytes)
                    logger.info(f"[FILE SAVED] {save_path}")
                    print(f"âœ… ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì €ì¥ë¨: {save_path}")
                except Exception as save_error:
                    logger.warning(f"[SAVE FAILED] {save_error}")

            return BytesIO(file_bytes)
            
        except requests.exceptions.Timeout:
            logger.error("[DOWNLOAD ERROR] Timeout (30s)")
            raise Exception(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ íƒ€ì„ì•„ì›ƒ: {url[:100]}")
        except requests.exceptions.RequestException as e:
            logger.error(f"[DOWNLOAD ERROR] {e}")
            raise Exception(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

    # -------------------------
    # ì›¹ í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    # -------------------------
    @staticmethod
    def extract_from_web(url: str) -> str:
        try:
            logger.info(f"[WEB EXTRACT] {url[:100]}...")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            resp = requests.get(url, timeout=10, headers=headers)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, 'html.parser')

            for tag in soup(['script', 'style']):
                tag.decompose()

            text = soup.get_text(separator="\n", strip=True)
            cleaned = re.sub(r"\n{3,}", "\n\n", text)
            logger.info(f"[WEB SUCCESS] Extracted {len(cleaned)} characters")
            return cleaned
        except Exception as e:
            logger.error(f"[WEB ERROR] {e}")
            return ""

    # -------------------------
    # DOCX ì¶”ì¶œ
    # -------------------------
    @staticmethod
    def extract_from_docx(source: str) -> str:
        """
        sourceê°€ URLì´ë©´ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë©”ëª¨ë¦¬ì—ì„œ ì²˜ë¦¬
        sourceê°€ ë¡œì»¬ ê²½ë¡œë©´ ê·¸ëŒ€ë¡œ ì²˜ë¦¬
        """
        try:
            logger.info(f"[DOCX EXTRACT START] Source: {source[:100]}...")
            
            if source.startswith("http"):
                file_obj = TextExtractor.download_file(source, suffix=".docx")
                doc = Document(file_obj)
            else:
                if not os.path.exists(source):
                    raise FileNotFoundError(f"DOCX íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {source}")
                doc = Document(source)

            text = "\n".join([p.text for p in doc.paragraphs if p.text.strip()])
            logger.info(f"[DOCX SUCCESS] Extracted {len(text)} characters")
            
            # ë¯¸ë¦¬ë³´ê¸° ì¶œë ¥
            if text:
                preview = text[:200].replace("\n", " ")
                print(f"ğŸ“„ DOCX Preview: {preview}...")
            
            return text

        except Exception as e:
            logger.error(f"[DOCX ERROR] {e}", exc_info=True)
            raise Exception(f"DOCX ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")

    # -------------------------
    # PDF ì¶”ì¶œ
    # -------------------------
    @staticmethod
    def extract_from_pdf(source: str) -> str:
        """
        sourceê°€ URL â†’ ë©”ëª¨ë¦¬ì—ì„œ pdfplumber ì²˜ë¦¬
        sourceê°€ ë¡œì»¬ ê²½ë¡œ â†’ ê¸°ì¡´ ì²˜ë¦¬
        """
        pdf_stream = None
        try:
            logger.info(f"[PDF EXTRACT START] Source: {source[:100]}...")
            
            if source.startswith("http"):
                file_obj = TextExtractor.download_file(source, suffix=".pdf")
                pdf_stream = file_obj
            else:
                if not os.path.exists(source):
                    raise FileNotFoundError(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {source}")
                pdf_stream = open(source, "rb")

            text = ""
            with pdfplumber.open(pdf_stream) as pdf:
                total_pages = len(pdf.pages)
                logger.info(f"[PDF INFO] Total pages: {total_pages}")
                
                for idx, page in enumerate(pdf.pages):
                    page_text = page.extract_text() or ""
                    text += page_text + "\n"
                    
                    # ì²« í˜ì´ì§€ ë¯¸ë¦¬ë³´ê¸°
                    if idx == 0 and page_text:
                        preview = page_text[:200].replace("\n", " ")
                        logger.info(f"[PDF PREVIEW] First page: {preview}...")
                        print(f"ğŸ“• PDF Preview (Page 1): {preview}...")

            # ìœ ë‹ˆì½”ë“œ ì œì–´ ë¬¸ìë§Œ ì œê±°
            text = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f-\x9f]', '', text)
            
            logger.info(f"[PDF SUCCESS] Extracted {len(text)} characters from {total_pages} pages")
            
            if not text or len(text.strip()) < 10:
                logger.warning("[PDF WARNING] Extracted text is too short or empty")
                print("âš ï¸  ê²½ê³ : PDFì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ê±°ë‚˜ ë§¤ìš° ì§§ìŠµë‹ˆë‹¤.")
            
            return text.strip()

        except Exception as e:
            logger.error(f"[PDF ERROR] {e}", exc_info=True)
            raise Exception(f"PDF ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")

        finally:
            if pdf_stream and not source.startswith("http"):
                try:
                    pdf_stream.close()
                except:
                    pass

    # -------------------------
    # ìë™ íƒ€ì… íŒë³„
    # -------------------------
    @classmethod
    def extract(cls, source: str) -> str:
        s = source.strip()
        
        logger.info(f"[EXTRACT] Detecting type for: {s[:100]}...")

        # 1) ì›¹ ë§í¬
        if s.startswith("http://") or s.startswith("https://"):
            # URLì—ì„œ í™•ì¥ì ì¶”ì¶œ (query parameter ì œê±°)
            url_path = s.split("?")[0].lower()
            
            if url_path.endswith(".pdf"):
                logger.info("[TYPE] PDF URL detected")
                return cls.extract_from_pdf(s)
            elif url_path.endswith(".docx"):
                logger.info("[TYPE] DOCX URL detected")
                return cls.extract_from_docx(s)
            else:
                logger.info("[TYPE] Web page detected")
                return cls.extract_from_web(s)

        # 2) ë¡œì»¬ íŒŒì¼
        if s.lower().endswith(".pdf"):
            logger.info("[TYPE] Local PDF file")
            return cls.extract_from_pdf(s)

        if s.lower().endswith(".docx"):
            logger.info("[TYPE] Local DOCX file")
            return cls.extract_from_docx(s)

        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì†ŒìŠ¤ íƒ€ì…: {source}")


# -------------------------------------
# ì—¬ëŸ¬ ì†ŒìŠ¤ ì¼ê´„ ì²˜ë¦¬
# -------------------------------------
def extract_all_sources(sources: List[str]) -> tuple[List[str], List[str]]:
    extracted = []
    errors = []

    print(f"\n{'='*80}")
    print(f"ğŸ“š í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘: {len(sources)}ê°œ ì†ŒìŠ¤")
    print(f"{'='*80}\n")

    for i, src in enumerate(sources):
        name = os.path.basename(src) if not src.startswith("http") else src[:50]
        logger.info(f"[EXTRACT] {i+1}/{len(sources)} â†’ {name}")
        print(f"\n[{i+1}/{len(sources)}] ì²˜ë¦¬ ì¤‘: {name}...")

        try:
            text = TextExtractor.extract(src)

            if text and len(text.strip()) > 0:
                extracted.append(text)
                print(f"âœ… ì„±ê³µ: {len(text):,} ê¸€ì ì¶”ì¶œë¨")
                logger.info(f"[SUCCESS] Extracted {len(text)} characters")
            else:
                error_msg = f"{name}: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ (ë¹ˆ ê²°ê³¼)"
                errors.append(error_msg)
                print(f"âŒ ì‹¤íŒ¨: {error_msg}")
                logger.warning(error_msg)
                
        except Exception as e:
            error_msg = f"{name}: ì²˜ë¦¬ ì˜¤ë¥˜ â†’ {str(e)}"
            errors.append(error_msg)
            print(f"âŒ ì‹¤íŒ¨: {error_msg}")
            logger.error(error_msg, exc_info=True)

    print(f"\n{'='*80}")
    print(f"ğŸ“Š ì¶”ì¶œ ì™„ë£Œ: ì„±ê³µ {len(extracted)}ê°œ / ì‹¤íŒ¨ {len(errors)}ê°œ")
    print(f"{'='*80}\n")
    
    logger.info(f"[EXTRACT COMPLETE] Success: {len(extracted)}, Errors: {len(errors)}")
    return extracted, errors