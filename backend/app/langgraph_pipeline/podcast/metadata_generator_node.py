"""
Metadata Generator Node (V2 - pdfplumber ì „í™˜)
===============================================

ë³€ê²½ì‚¬í•­:
- PyMuPDF ì™„ì „ ì œê±°
- pdfplumber + OCR (pdf2image + PaddleOCR)ë¡œ í†µí•©
- improved_hybrid_filter.py V3ì™€ ì™„ì „ í˜¸í™˜

ì…ë ¥:
- primary_file: ì£¼ê°•ì˜ìë£Œ (1ê°œ, í•„ìˆ˜)
- supplementary_files: ë³´ì¡°ìë£Œ (0~3ê°œ, ì„ íƒ)

ì¶œë ¥:
- metadata.json (ì´ë¯¸ì§€ ì„¤ëª… í¬í•¨)

í†µí•©:
- DocumentConverterNode: PDF ë³€í™˜ + TXT/URL ì²˜ë¦¬
- ImprovedHybridFilterPipeline: ì´ë¯¸ì§€ í•„í„°ë§
- TextExtractor: í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
- ImageDescriptionGenerator: ì´ë¯¸ì§€ ìƒì„¸ ì„¤ëª…

"""

import os
import json
import tempfile
from pathlib import Path
from typing import List, Optional, Dict, Any
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

# OCR ë¡œê·¸ ì–µì œ
os.environ['FLAGS_log_level'] = '3'
os.environ['PPOCR_SHOW_LOG'] = 'False'

# pdfplumber (í•„ìˆ˜)
try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    print("âŒ pdfplumberê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("   pip install pdfplumber")
    PDFPLUMBER_AVAILABLE = False

# OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„ íƒ)
try:
    from paddleocr import PaddleOCR
    from pdf2image import convert_from_path
    import numpy as np
    from PIL import Image
    from io import BytesIO
    
    OCR_AVAILABLE = True
    ocr_engine = PaddleOCR(lang='korean', use_textline_orientation=True)
    print("âœ… OCR ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ (PaddleOCR)")
except ImportError:
    OCR_AVAILABLE = False
    ocr_engine = None
    print("âš ï¸  OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜ (ì„ íƒ ì‚¬í•­)")
except Exception as e:
    print(f"âš ï¸  PaddleOCR ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    OCR_AVAILABLE = False
    ocr_engine = None

# ê¸°ì¡´ ë…¸ë“œ ì„í¬íŠ¸
from .document_converter_node import DocumentConverterNode, DocumentType
from .improved_hybrid_filter import (
    ImprovedHybridFilterPipeline,
    UniversalImageExtractor,
    ImageMetadata,
    model
)

from vertexai.generative_models import Part


class TextExtractor:
    """
    PDFì—ì„œ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ + ë§ˆì»¤ ì‚½ì…
    V2: pdfplumber + OCR í†µí•©
    """
    
    def __init__(self):
        """TextExtractor ì´ˆê¸°í™”"""
        if not PDFPLUMBER_AVAILABLE:
            raise ImportError("pdfplumberê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install pdfplumber")
        
        self.ocr_enabled = OCR_AVAILABLE
        self.min_text_length = 100  # OCR íŠ¸ë¦¬ê±° ê¸°ì¤€
    
    def _perform_ocr_on_page(self, pdf_path: str, page_num: int) -> str:
        """
        í˜ì´ì§€ì— OCR ìˆ˜í–‰ (pdf2image + PaddleOCR)
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            page_num: í˜ì´ì§€ ë²ˆí˜¸ (0-indexed)
        
        Returns:
            OCRë¡œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸
        """
        if not self.ocr_enabled or ocr_engine is None:
            return ""
        
        try:
            # PDF í˜ì´ì§€ â†’ ì´ë¯¸ì§€ ë³€í™˜
            images = convert_from_path(
                pdf_path,
                first_page=page_num + 1,  # 1-indexed
                last_page=page_num + 1,
                dpi=150
            )
            
            if not images:
                return ""
            
            img = images[0]
            img_array = np.array(img)
            
            # OCR ì‹¤í–‰
            result = ocr_engine.ocr(img_array, cls=True)
            
            if result and result[0]:
                lines = []
                for line in result[0]:
                    if line and len(line) >= 2:
                        text = line[1][0]
                        lines.append(text)
                return "\n".join(lines)
            
            return ""
        
        except Exception as e:
            print(f"      âš ï¸  OCR ì‹¤íŒ¨: {e}")
            return ""
    
    def extract_with_markers(
        self, 
        pdf_path: str, 
        prefix: str = "MAIN"
    ) -> Dict[str, Any]:
        """
        PDFì—ì„œ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ + ë§ˆì»¤ ì‚½ì…
        pdfplumber ì‚¬ìš©, í…ìŠ¤íŠ¸ ë¶€ì¡± ì‹œ OCR ìë™ ìˆ˜í–‰
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            prefix: í˜ì´ì§€ ë§ˆì»¤ ì ‘ë‘ì‚¬ (MAIN, SUPP1, SUPP2, SUPP3)
        
        Returns:
            {
                "full_text": "[MAIN-PAGE 1: ì œëª©]\në‚´ìš©...",
                "total_pages": 21
            }
        """
        pages_text = []
        total_pages = 0
        ocr_count = 0
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                total_pages = len(pdf.pages)
                
                print(f"   ğŸ“„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘... (OCR {'í™œì„±í™”' if self.ocr_enabled else 'ë¹„í™œì„±í™”'})")
                
                for page_num, page in enumerate(pdf.pages):
                    # pdfplumberë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    text = page.extract_text() or ""
                    text_length = len(text.strip())
                    
                    # í…ìŠ¤íŠ¸ ë¶€ì¡± â†’ OCR ìˆ˜í–‰
                    if text_length < self.min_text_length and self.ocr_enabled:
                        print(f"      â†’ í˜ì´ì§€ {page_num + 1}: í…ìŠ¤íŠ¸ ë¶€ì¡± ({text_length}ì) â†’ OCR ìˆ˜í–‰")
                        ocr_text = self._perform_ocr_on_page(pdf_path, page_num)
                        
                        if ocr_text:
                            text = ocr_text
                            ocr_count += 1
                            print(f"         âœ… OCR ì™„ë£Œ ({len(ocr_text)}ì ì¶”ì¶œ)")
                        else:
                            print(f"         âš ï¸  OCR ì‹¤íŒ¨, ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©")
                    
                    # í˜ì´ì§€ ì œëª© ì¶”ì¶œ
                    lines = text.split('\n')
                    title = lines[0][:50] if lines and lines[0].strip() else f"Page {page_num + 1}"
                    
                    # ë§ˆì»¤ ì‚½ì…
                    pages_text.append(f"[{prefix}-PAGE {page_num + 1}: {title}]")
                    pages_text.append(text)
                    pages_text.append("")
                
                if ocr_count > 0:
                    print(f"   âœ… OCR ì²˜ë¦¬ ì™„ë£Œ: {ocr_count}ê°œ í˜ì´ì§€")
        
        except Exception as e:
            print(f"   âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {"full_text": "", "total_pages": 0}
        
        return {
            "full_text": "\n".join(pages_text),
            "total_pages": total_pages
        }


class ImageDescriptionGenerator:
    """í†µê³¼ëœ ì´ë¯¸ì§€ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª… ìƒì„± (2-4ë¬¸ì¥)"""
    
    def generate_description(
        self, 
        image_bytes: bytes, 
        adjacent_text: str,
        keywords: List[str],
        max_retries=3
    ) -> str:
        """
        Vision APIë¡œ ì´ë¯¸ì§€ ìƒì„¸ ì„¤ëª… ìƒì„±
        ì¬ì‹œë„ ë¡œì§ í¬í•¨ (429 Rate Limit ëŒ€ì‘)
        """
        import time
        
        for attempt in range(max_retries):
            try:
                mime_type = self._get_mime_type(image_bytes)
                image_part = Part.from_data(data=image_bytes, mime_type=mime_type)
                
                keyword_context = ', '.join(keywords[:10]) if keywords else "ì¼ë°˜ í•™ìŠµ ë‚´ìš©"
                
                prompt = f"""
ì´ ì´ë¯¸ì§€ë¥¼ 2-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.

ê°•ì˜ ì£¼ì œ: {keyword_context}
ì£¼ë³€ í…ìŠ¤íŠ¸: "{adjacent_text}"

ì„¤ëª…ì— í¬í•¨í•  ë‚´ìš©:
1. ì´ë¯¸ì§€ê°€ ë‚˜íƒ€ë‚´ëŠ” ì£¼ì œ/ê°œë… (1ë¬¸ì¥)
2. ì£¼ìš” êµ¬ì„± ìš”ì†Œ 2-3ê°œ (1-2ë¬¸ì¥)
3. í•µì‹¬ ì •ë³´ë‚˜ íŒ¨í„´ (1ë¬¸ì¥)

ì œì™¸í•  ë‚´ìš©:
- ì„¸ë¶€ ìš”ì†Œ ì „ì²´ ë‚˜ì—´
- ë¶ˆí•„ìš”í•œ ì¶”ì¸¡ì´ë‚˜ í•´ì„

ì¶œë ¥: ëª…í™•í•˜ê³  ê°„ê²°í•œ 2-4ë¬¸ì¥ë§Œ.
"""
                response = model.generate_content([image_part, prompt])
                description = response.text.strip()
                return description
                
            except Exception as e:
                error_msg = str(e)
                
                if "429" in error_msg or "Resource exhausted" in error_msg:
                    if attempt < max_retries - 1:
                        wait_time = (attempt + 1) * 3
                        print(f"      âš ï¸  Rate Limit, {wait_time}ì´ˆ ëŒ€ê¸° ì¤‘...", end='', flush=True)
                        time.sleep(wait_time)
                        print(" ì¬ì‹œë„")
                        continue
                    else:
                        return "ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: API rate limit exceeded"
                else:
                    return f"ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {error_msg}"
        
        return "ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: Failed after all retries"
    
    def _get_mime_type(self, image_bytes: bytes) -> str:
        """ì´ë¯¸ì§€ ë°”ì´ë„ˆë¦¬ì—ì„œ MIME íƒ€ì… ê°ì§€"""
        if image_bytes.startswith(b'\xff\xd8'):
            return "image/jpeg"
        elif image_bytes.startswith(b'\x89PNG\r\n\x1a\n'):
            return "image/png"
        elif image_bytes.startswith(b'GIF87a') or image_bytes.startswith(b'GIF89a'):
            return "image/gif"
        elif image_bytes.startswith(b'RIFF') and image_bytes[8:12] == b'WEBP':
            return "image/webp"
        return "image/png"


class MetadataGenerator:
    """
    ë©”íƒ€ë°ì´í„° ìƒì„± ë…¸ë“œ
    
    ì£¼ê°•ì˜ìë£Œ + ë³´ì¡°ìë£Œ â†’ metadata.json
    """
    
    def __init__(self):
        self.converter = None
        self.text_extractor = TextExtractor()
        self.image_filter = ImprovedHybridFilterPipeline(auto_extract_keywords=True)
        self.image_describer = ImageDescriptionGenerator()
    
    def _extract_page_title(self, slide_title: str, adjacent_text: str) -> str:
        """ì˜ë¯¸ìˆëŠ” í˜ì´ì§€ ì œëª© ì¶”ì¶œ"""
        if slide_title and slide_title.strip() and slide_title.lower() != "no title":
            return slide_title.strip()[:50]
        
        if adjacent_text:
            lines = adjacent_text.strip().split('\n')
            for line in lines:
                line = line.strip()
                if len(line) > 3 and not line.startswith('â˜'):
                    return line[:50]
        
        return "í˜ì´ì§€ ì œëª© ì—†ìŒ"
    
    def generate(
        self,
        primary_file: str,
        supplementary_files: Optional[List[str]] = None,
        output_path: str = "output/metadata.json"
    ) -> str:
        """ë©”íƒ€ë°ì´í„° ìƒì„±"""
        print(f"\n{'='*120}")
        print(f"ğŸ¯ ë©”íƒ€ë°ì´í„° ìƒì„± ì‹œì‘")
        print(f"{'='*120}")
        print(f"ì£¼ê°•ì˜ìë£Œ: {primary_file}")
        if supplementary_files:
            print(f"ë³´ì¡°ìë£Œ: {len(supplementary_files)}ê°œ")
            for i, supp in enumerate(supplementary_files, 1):
                print(f"  {i}. {supp}")
        print(f"{'='*120}\n")
        
        with tempfile.TemporaryDirectory() as temp_dir:
            self.converter = DocumentConverterNode(output_dir=temp_dir)
            
            print("ğŸ“„ [1/3] ì£¼ê°•ì˜ìë£Œ ì²˜ë¦¬ ì¤‘...")
            primary_metadata = self._process_primary_source(primary_file)
            
            print("\nğŸ“š [2/3] ë³´ì¡°ìë£Œ ì²˜ë¦¬ ì¤‘...")
            supplementary_metadata = []
            if supplementary_files:
                for i, supp_file in enumerate(supplementary_files[:3], 1):
                    try:
                        supp_meta = self._process_supplementary_source(supp_file, i)
                        supplementary_metadata.append(supp_meta)
                        print(f"   âœ… ë³´ì¡°ìë£Œ {i} ì²˜ë¦¬ ì„±ê³µ")
                    except Exception as e:
                        print(f"   âš ï¸ ë³´ì¡°ìë£Œ {i} ì²˜ë¦¬ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
            else:
                print("   âš ï¸  ë³´ì¡°ìë£Œ ì—†ìŒ (ì„ íƒ ì‚¬í•­)")
            
            print("\nğŸ”§ [3/3] ë©”íƒ€ë°ì´í„° í†µí•© ì¤‘...")
            metadata = {
                "metadata_version": "1.0",
                "created_at": datetime.now().isoformat(),
                "primary_source": primary_metadata,
                "supplementary_sources": supplementary_metadata
            }
            
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            print(f"\n{'='*120}")
            print(f"âœ… ë©”íƒ€ë°ì´í„° ìƒì„± ì™„ë£Œ!")
            print(f"{'='*120}")
            print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_path}")
            print(f"ğŸ“Š ì£¼ê°•ì˜ìë£Œ í˜ì´ì§€: {primary_metadata['total_pages']}ê°œ")
            print(f"ğŸ–¼ï¸  í•„í„°ë§ëœ ì´ë¯¸ì§€: {len(primary_metadata['filtered_images'])}ê°œ")
            if supplementary_metadata:
                total_supp_pages = sum(s['total_pages'] for s in supplementary_metadata)
                print(f"ğŸ“š ë³´ì¡°ìë£Œ í˜ì´ì§€: {total_supp_pages}ê°œ")
            print(f"{'='*120}\n")
            
            return str(output_path)
    
    def _process_primary_source(self, file_path: str) -> Dict[str, Any]:
        """
        ì£¼ê°•ì˜ìë£Œ ì²˜ë¦¬
        âœ… TXT/URL ì§€ì› ì¶”ê°€
        """
        file_path_str = str(file_path)
        
        # ì›ë³¸ íŒŒì¼ íƒ€ì… ê°ì§€
        if file_path_str.startswith(('http://', 'https://')):
            original_file_type = 'url'
            file_path_obj = None
            display_name = file_path_str[:50]
        else:
            file_path_obj = Path(file_path)
            original_file_type = file_path_obj.suffix.lower().replace('.', '')
            display_name = file_path_obj.name
        
        print(f"   ğŸ“„ íŒŒì¼: {display_name} ({original_file_type})")
        
        # 1. íŒŒì¼ ë³€í™˜ (TXT/URLë„ PDFë¡œ ë³€í™˜ë¨)
        print(f"   ğŸ”„ íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
        processed_path = self.converter.convert(file_path_str)
        
        # 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        print(f"   ğŸ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
        text_data = self.text_extractor.extract_with_markers(processed_path, prefix="MAIN")
        print(f"   âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(text_data['full_text'])}ì")
        
        # 3. ì´ë¯¸ì§€ í•„í„°ë§
        print(f"   ğŸ–¼ï¸  ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘...")
        
        filtered_images = []
        keywords = []
        
        # TXT/URLì€ ì´ë¯¸ì§€ ì—†ìŒ
        if original_file_type in ['txt', 'url']:
            print(f"      â†’ TXT/URLì€ ì´ë¯¸ì§€ ì—†ìŒ, ê±´ë„ˆë›°ê¸°")
            all_images = []
        
        elif original_file_type == 'pptx':
            print(f"      â†’ PPTX ì›ë³¸ì—ì„œ ì§ì ‘ ì¶”ì¶œ")
            self.image_filter.extract_keywords_from_document(file_path_str)
            keywords = self.image_filter.document_keywords
            all_images = self._extract_images_from_pptx(file_path_str)
            
        elif original_file_type in ['docx', 'pdf']:
            print(f"      â†’ PDFì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ")
            self.image_filter.extract_keywords_from_document(processed_path)
            keywords = self.image_filter.document_keywords
            extractor = UniversalImageExtractor()
            all_images = extractor.extract(processed_path)
        
        else:
            print(f"   âš ï¸  ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {original_file_type}")
            all_images = []
        
        # 4. í•„í„°ë§ ì‹¤í–‰
        if all_images:
            print(f"   ğŸ” {len(all_images)}ê°œ ì´ë¯¸ì§€ ë°œê²¬, í•„í„°ë§ ì‹œì‘...")

            for img_meta in all_images:
                decision, reason = self.image_filter.step1_rule_check(img_meta)
                
                if decision == "INCLUDE":
                    img_meta.is_core_content = True
                    img_meta.filter_reason = reason
                    filtered_images.append(img_meta)
                    
                elif decision == "PENDING":
                    ai_result = self.image_filter.step2_gemini_check(img_meta)

                    # íŠœí”Œ ë°˜í™˜ ëŒ€ì‘
                    if isinstance(ai_result, tuple):
                        ai_result = ai_result[0]

                    if ai_result.upper().startswith("KEEP"):
                        img_meta.is_core_content = True
                        img_meta.filter_reason = ai_result
                        filtered_images.append(img_meta)
            
            print(f"   âœ… í•„í„°ë§ ì™„ë£Œ: {len(filtered_images)}ê°œ ì„ íƒ")
        
        # 5. ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±
        filtered_image_metadata = []
        
        if filtered_images:
            print(f"   ğŸ“ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì¤‘... (0/{len(filtered_images)})", end='', flush=True)
            
            for i, img_meta in enumerate(filtered_images, 1):
                description = self.image_describer.generate_description(
                    img_meta.image_bytes,
                    img_meta.adjacent_text,
                    keywords
                )
                
                page_title = self._extract_page_title(
                    img_meta.slide_title,
                    img_meta.adjacent_text
                )
                
                filtered_image_metadata.append({
                    "image_id": img_meta.image_id.replace("S", "MAIN_P").replace("P", "MAIN_P"),
                    "page_number": img_meta.slide_number,
                    "page_title": page_title,
                    "description": description,
                    "filter_stage": "1ì°¨ (Rule)" if "Rule" in img_meta.filter_reason else "2ì°¨ (AI)",
                    "area_percentage": img_meta.area_percentage
                })
                
                print(f"\r   ğŸ“ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì¤‘... ({i}/{len(filtered_images)})", end='', flush=True)
            
            print()
            
            print(f"\n   {'='*80}")
            print(f"   ğŸ“Š ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì™„ë£Œ")
            print(f"      - ì²˜ë¦¬ëœ ì´ë¯¸ì§€: {len(filtered_images)}ê°œ")
            print(f"   {'='*80}\n")

        # 6. í†µê³„
        total_images = len(all_images)
        passed_images = len(filtered_images)
        
        return {
            "role": "main",
            "filename": display_name if original_file_type == 'url' else file_path_obj.name,
            "file_type": original_file_type,
            "total_pages": text_data['total_pages'],
            "content": {
                "full_text": text_data['full_text']
            },
            "filtered_images": filtered_image_metadata,
            "statistics": {
                "total_images_found": total_images,
                "images_passed": passed_images,
                "filter_rate": passed_images / total_images if total_images > 0 else 0
            }
        }
    
    def _process_supplementary_source(self, file_path: str, order: int) -> Dict[str, Any]:
        file_path_str = str(file_path)
        
        # URLê³¼ íŒŒì¼ êµ¬ë¶„
        if file_path_str.startswith(('http://', 'https://')):
            file_type = 'url'
            display_name = 'Web Content'
        else:
            file_path_obj = Path(file_path)
            file_type = file_path_obj.suffix.lower().replace('.', '')
            display_name = file_path_obj.name
        
        print(f"   ğŸ“š ë³´ì¡°ìë£Œ {order}: {display_name} ({file_type})")
        
        print(f"      ğŸ”„ PDF ë³€í™˜ ì¤‘...")
        pdf_path = self.converter.convert(file_path_str)
        
        print(f"      ğŸ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
        text_data = self.text_extractor.extract_with_markers(pdf_path, prefix=f"SUPP{order}")
        
        print(f"      âœ… ì™„ë£Œ ({text_data['total_pages']}í˜ì´ì§€)")
        
        return {
            "order": order,
            "filename": display_name,
            "file_type": file_type,
            "total_pages": text_data['total_pages'],
            "content": {
                "full_text": text_data['full_text']
            }
        }
    
    def _extract_images_from_pptx(self, pptx_path: str) -> List[ImageMetadata]:
        """PPTXì—ì„œ ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        extractor = UniversalImageExtractor()
        return extractor.extract(pptx_path)


# CLI ì¸í„°í˜ì´ìŠ¤
if __name__ == "__main__":
    import sys
    
    print("\n" + "="*120)
    print("ğŸ¯ Metadata Generator Node (V2 - pdfplumber)")
    print("="*120)
    
    if len(sys.argv) < 2:
        print("\nì‚¬ìš©ë²•:")
        print("  python metadata_generator_node.py <ì£¼ê°•ì˜ìë£Œ> [ë³´ì¡°1] [ë³´ì¡°2] [ë³´ì¡°3]")
        print("\nì˜ˆì‹œ:")
        print("  python metadata_generator_node.py ì¤‘ë“±êµ­ì–´1.pptx")
        print("  python metadata_generator_node.py notes.txt")
        print("  python metadata_generator_node.py https://example.com/article")
        print("\nâœ… ì§€ì› í˜•ì‹: PPTX, DOCX, PDF, TXT, URL")
        print("="*120 + "\n")
        sys.exit(1)
    
    primary_file = sys.argv[1]
    supplementary_files = sys.argv[2:5] if len(sys.argv) > 2 else None
    
    if not primary_file.startswith('http') and not os.path.exists(primary_file):
        print(f"\nâŒ ì£¼ê°•ì˜ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {primary_file}")
        sys.exit(1)
    
    if supplementary_files:
        for supp in supplementary_files:
            if not supp.startswith('http') and not os.path.exists(supp):
                print(f"\nâŒ ë³´ì¡°ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {supp}")
                sys.exit(1)
    
    try:
        generator = MetadataGenerator()
        output_path = generator.generate(
            primary_file=primary_file,
            supplementary_files=supplementary_files,
            output_path="output/metadata.json"
        )
        
        print(f"âœ… ì„±ê³µ!")
        print(f"ğŸ“ {output_path}")
        
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)