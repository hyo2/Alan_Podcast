import os
import logging
from app.utils.binary_helper import prepare_ffmpeg_binaries
from app.services.langsmith_tracing import _get_root_run_id, _trace_with_parent

logger = logging.getLogger(__name__)

# ========================================
# ê° ë‹¨ê³„ë³„ ì‹¤í–‰ í•¨ìˆ˜
# ========================================

def run_extract_ocr_step(session_id, channel_id, options, storage, session_repo, session_input_repo):
    import tempfile
    from app.utils.session_helpers import session_exists
    
    logger.info(f"[ExtractOCR] Starting for session={session_id}")
    
    if not session_exists(session_repo, session_id):
        logger.info(f"[ExtractOCR] Session {session_id} deleted - skipping")
        return
    
    session = session_repo.get_session(session_id)
    storage_prefix = session.get("storage_prefix", "")
    
    # âœ… Root run ID ì¡°íšŒ
    root_run_id = _get_root_run_id(storage, storage_prefix)
    
    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
    session_repo.update_session_fields(session_id, current_step="extract_texts")
    
    # ì…ë ¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    inputs = session_input_repo.list_inputs(session_id)
    if not inputs:
        raise Exception("ì…ë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    main_sources = []
    aux_sources = []
    temp_files = []
    
    try:
        for inp in inputs:
            if inp.get("is_link"):
                source_path = inp["link_url"]
            else:
                input_key = inp["input_key"]
                file_data = storage.download(input_key)
                
                file_ext = os.path.splitext(input_key)[1] or ".tmp"
                temp_fd, temp_path = tempfile.mkstemp(suffix=file_ext, prefix=f"input_{inp['input_id']}_")
                
                with os.fdopen(temp_fd, 'wb') as f:
                    f.write(file_data)
                
                temp_files.append(temp_path)
                source_path = temp_path
            
            if inp["role"] == "main":
                main_sources.append(source_path)
            else:
                aux_sources.append(source_path)
        
        # ì²´í¬í¬ì¸íŠ¸ ì½œë°±
        def checkpoint_callback(key: str, data: dict):
            try:
                storage.upload_json(key, data)
                logger.info(f"[ExtractOCR] ğŸ’¾ Checkpoint saved: {key}")
            except Exception as e:
                logger.warning(f"[ExtractOCR] âš ï¸ Checkpoint save failed: {e}")

        # âœ… State ì¤€ë¹„
        from app.langgraph_pipeline.podcast.graph import extract_texts_node
        
        state_input = {
            "main_sources": main_sources,
            "aux_sources": aux_sources,
            "source_data": {},
            "main_texts": [],
            "aux_texts": [],
            "combined_text": "",
            "errors": [],
            "usage": {},
            "session_id": session_id,
            "storage_prefix": storage_prefix,
            "checkpoint_callback": checkpoint_callback,
        }
        
        # âœ… Parent runìœ¼ë¡œ ì—°ê²°í•˜ì—¬ node ì‹¤í–‰
        if root_run_id:
            state = _trace_with_parent(
                name="extract_texts",
                parent_run_id=root_run_id,
                func=lambda s: extract_texts_node(s),
                state_input=state_input,
            )
        else:
            state = extract_texts_node(state_input)
        
        # ì„¸ì…˜ ì‚­ì œ ì²´í¬
        if not session_exists(session_repo, session_id):
            logger.info(f"[ExtractOCR] Session {session_id} deleted during execution")
            return
        
        if state.get("errors"):
            raise Exception(f"Extract OCR failed: {state['errors']}")
        
        # OCR ê²°ê³¼ ì €ì¥
        ocr_results_key = f"{storage_prefix}pipeline/ocr_results.json"
        ocr_data = {
            "source_data": state["source_data"],
            "main_texts": state["main_texts"],
            "aux_texts": state["aux_texts"],
            "usage": state.get("usage", {}),
        }
        storage.upload_json(ocr_results_key, ocr_data)
        
        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
        progress_key = f"{storage_prefix}pipeline/progress.json"
        progress_data = storage.download_json(progress_key)
        progress_data["completed_steps"].append("extract_ocr")
        progress_data["current_step"] = "extract_finalize"
        progress_data["intermediate_keys"]["ocr_results"] = ocr_results_key
        storage.upload_json(progress_key, progress_data)
        
        session_repo.update_session_fields(session_id, current_step="extract_ocr_complete")
        logger.info(f"[ExtractOCR] âœ… Completed for session={session_id}")
        
        # ë‹¤ìŒ ë‹¨ê³„ íì‰
        from app.services.queue_service import enqueue_pipeline_step
        enqueue_pipeline_step(
            session_id=session_id,
            channel_id=channel_id,
            step="extract_finalize",
            options=options,
        )
        
    finally:
        for temp_file in temp_files:
            try:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
            except Exception:
                pass


def run_extract_finalize_step(session_id, channel_id, options, storage, session_repo):
    """
    âœ… Extract Phase 2: í…ìŠ¤íŠ¸ ë³‘í•© + ë©”íƒ€ë°ì´í„° ìƒì„±
    - OCR ê²°ê³¼ ë¡œë“œ
    - combine_texts_node() ì‹¤í–‰
    - ìµœì¢… extracted_data.json ì €ì¥
    """
    from app.utils.session_helpers import session_exists
    
    logger.info(f"[ExtractFinalize] Starting for session={session_id}")
    
    if not session_exists(session_repo, session_id):
        logger.info(f"[ExtractFinalize] Session {session_id} deleted - skipping")
        return
    
    session = session_repo.get_session(session_id)
    storage_prefix = session.get("storage_prefix", "")
    
    # âœ… Root run ID ì¡°íšŒ
    root_run_id = _get_root_run_id(storage, storage_prefix)
    
    session_repo.update_session_fields(session_id, current_step="combine_texts")
    
    # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ
    ocr_results_key = f"{storage_prefix}pipeline/ocr_results.json"
    ocr_data = storage.download_json(ocr_results_key)
    
    from app.langgraph_pipeline.podcast.graph import combine_texts_node
    
    state_input = {
        "source_data": ocr_data["source_data"],
        "main_texts": ocr_data["main_texts"],
        "aux_texts": ocr_data["aux_texts"],
        "combined_text": "",
        "errors": [],
        "usage": ocr_data.get("usage", {}),
    }
    
    # âœ… Node ì‹¤í–‰ + trace
    state = _trace_with_parent("combine_texts", root_run_id, combine_texts_node, state_input)
    
    if not session_exists(session_repo, session_id):
        logger.info(f"[ExtractFinalize] Session {session_id} deleted during execution")
        return
    
    # ìµœì¢… ê²°ê³¼ ì €ì¥
    extracted_key = f"{storage_prefix}pipeline/extracted_data.json"
    
    extracted_data = {
        "combined_text": state["combined_text"],
        "source_data": state["source_data"],
        "main_texts": state["main_texts"],
        "aux_texts": state["aux_texts"],
        "usage": state.get("usage", {}),
    }
    
    storage.upload_json(extracted_key, extracted_data)
    
    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
    progress_key = f"{storage_prefix}pipeline/progress.json"
    progress_data = storage.download_json(progress_key)
    progress_data["completed_steps"].append("extract_finalize")
    progress_data["current_step"] = "script"
    progress_data["intermediate_keys"]["extracted_data"] = extracted_key
    storage.upload_json(progress_key, progress_data)
    
    session_repo.update_session_fields(
        session_id,
        current_step="extract_complete",
    )
    
    logger.info(f"[ExtractFinalize] âœ… Completed for session={session_id}")
    
    # ë‹¤ìŒ ë‹¨ê³„ íì‰
    from app.services.queue_service import enqueue_pipeline_step
    enqueue_pipeline_step(
        session_id=session_id,
        channel_id=channel_id,
        step="script",
        options=options,
    )


def run_script_step(session_id, channel_id, options, storage, session_repo):
    """2ë‹¨ê³„: ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
    from app.utils.session_helpers import session_exists
    
    logger.info(f"[Script] Starting for session={session_id}")
    
    if not session_exists(session_repo, session_id):
        logger.info(f"[Script] Session {session_id} deleted - skipping")
        return
    
    session = session_repo.get_session(session_id)
    storage_prefix = session.get("storage_prefix", "")
    
    # âœ… Root run ID ì¡°íšŒ
    root_run_id = _get_root_run_id(storage, storage_prefix)
    
    session_repo.update_session_fields(session_id, current_step="generate_script")
    
    # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ
    extracted_key = f"{storage_prefix}pipeline/extracted_data.json"
    extracted_data = storage.download_json(extracted_key)
    
    from app.langgraph_pipeline.podcast.graph import generate_script_node
    
    # Vertex AI ì„¤ì •
    project_id = os.getenv("VERTEX_AI_PROJECT_ID")
    region = os.getenv("VERTEX_AI_REGION")
    sa_file = os.getenv("VERTEX_AI_SERVICE_ACCOUNT_FILE")
    
    state_input = {
        "combined_text": extracted_data["combined_text"],
        "source_data": extracted_data["source_data"],
        "project_id": project_id,
        "region": region,
        "sa_file": sa_file,
        "host_name": options.get("host1", "Fenrir"),
        "guest_name": options.get("host2", ""),
        "style": options.get("style", "explain"),
        "duration": options.get("duration", 5),
        "difficulty": options.get("difficulty", "intermediate"),
        "user_prompt": options.get("user_prompt", ""),
        "usage": extracted_data.get("usage", {}),
        "errors": [],
    }
    
    # âœ… Node ì‹¤í–‰ + trace
    state = _trace_with_parent("generate_script", root_run_id, generate_script_node, state_input)
    
    if not session_exists(session_repo, session_id):
        logger.info(f"[Script] Session {session_id} deleted during execution")
        return
    
    if state.get("errors"):
        raise Exception(f"Script generation failed: {state['errors']}")
    
    # ìŠ¤í¬ë¦½íŠ¸ ì €ì¥
    script_key = f"{storage_prefix}pipeline/script.json"
    script_data = {
        "title": state["title"],
        "script": state["script"],
        "usage": state.get("usage", {}),
    }
    storage.upload_json(script_key, script_data)
    
    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
    progress_key = f"{storage_prefix}pipeline/progress.json"
    progress_data = storage.download_json(progress_key)
    progress_data["completed_steps"].append("script")
    progress_data["current_step"] = "audio"
    progress_data["intermediate_keys"]["script"] = script_key
    storage.upload_json(progress_key, progress_data)
    
    session_repo.update_session_fields(
        session_id,
        current_step="script_complete",
        title=state["title"],
    )
    
    logger.info(f"[Script] âœ… Completed for session={session_id}")
    
    # ë‹¤ìŒ ë‹¨ê³„ íì‰
    from app.services.queue_service import enqueue_pipeline_step
    enqueue_pipeline_step(
        session_id=session_id,
        channel_id=channel_id,
        step="audio",
        options=options,
    )


def run_audio_step(session_id, channel_id, options, storage, session_repo):
    """3ë‹¨ê³„: ì˜¤ë””ì˜¤ ìƒì„±"""
    from app.utils.session_helpers import session_exists
    
    logger.info(f"[Audio] Starting for session={session_id}")
    
    if not session_exists(session_repo, session_id):
        logger.info(f"[Audio] Session {session_id} deleted - skipping")
        return
    
    session = session_repo.get_session(session_id)
    storage_prefix = session.get("storage_prefix", "")
    
    # âœ… Root run ID ì¡°íšŒ
    root_run_id = _get_root_run_id(storage, storage_prefix)
    
    session_repo.update_session_fields(session_id, current_step="generate_audio")
    
    # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ
    script_key = f"{storage_prefix}pipeline/script.json"
    script_data = storage.download_json(script_key)
    
    from app.langgraph_pipeline.podcast.graph import generate_audio_node
    
    state_input = {
        "script": script_data["script"],
        "host_name": options.get("host1", "Fenrir"),
        "guest_name": options.get("host2", ""),
        "usage": script_data.get("usage", {}),
        "errors": [],
    }
    
    # âœ… Node ì‹¤í–‰ + trace
    state = _trace_with_parent("generate_audio", root_run_id, generate_audio_node, state_input)
    
    if not session_exists(session_repo, session_id):
        logger.info(f"[Audio] Session {session_id} deleted during execution")
        return
    
    if state.get("errors"):
        raise Exception(f"Audio generation failed: {state['errors']}")
    
    # ì˜¤ë””ì˜¤ íŒŒì¼ë“¤ì„ Blobì— ì—…ë¡œë“œ
    audio_parts_dir = f"{storage_prefix}pipeline/audio_parts/"
    wav_files = state["wav_files"]
    
    uploaded_audio_keys = []
    for i, wav_path in enumerate(wav_files):
        with open(wav_path, 'rb') as f:
            audio_data = f.read()
        
        audio_key = f"{audio_parts_dir}part_{i}.wav"
        storage.upload_bytes(audio_key, audio_data, content_type="audio/wav")
        uploaded_audio_keys.append(audio_key)
        
        # ë¡œì»¬ ì„ì‹œ íŒŒì¼ ì‚­ì œ
        try:
            os.remove(wav_path)
        except Exception:
            pass
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    audio_metadata_key = f"{storage_prefix}pipeline/audio_metadata.json"
    audio_data = {
        "audio_metadata": state["audio_metadata"],
        "audio_parts_keys": uploaded_audio_keys,
        "usage": state.get("usage", {}),
    }
    storage.upload_json(audio_metadata_key, audio_data)
    
    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
    progress_key = f"{storage_prefix}pipeline/progress.json"
    progress_data = storage.download_json(progress_key)
    progress_data["completed_steps"].append("audio")
    progress_data["current_step"] = "finalize"
    progress_data["intermediate_keys"]["audio_metadata"] = audio_metadata_key
    storage.upload_json(progress_key, progress_data)
    
    session_repo.update_session_fields(session_id, current_step="audio_complete")
    
    logger.info(f"[Audio] âœ… Completed for session={session_id}")
    
    # ë‹¤ìŒ ë‹¨ê³„ íì‰
    from app.services.queue_service import enqueue_pipeline_step
    enqueue_pipeline_step(
        session_id=session_id,
        channel_id=channel_id,
        step="finalize",
        options=options,
    )


def run_finalize_step(session_id, channel_id, options, storage, session_repo):
    """4ë‹¨ê³„: ìµœì¢… ë³‘í•© + íŠ¸ëœìŠ¤í¬ë¦½íŠ¸"""
    import tempfile
    from app.utils.session_helpers import session_exists
    
    logger.info(f"[Finalize] Starting for session={session_id}")
    
    if not session_exists(session_repo, session_id):
        logger.info(f"[Finalize] Session {session_id} deleted - skipping")
        return
    
    session = session_repo.get_session(session_id)
    storage_prefix = session.get("storage_prefix", "")
    
    # âœ… Root run ID ì¡°íšŒ
    root_run_id = _get_root_run_id(storage, storage_prefix)
    
    session_repo.update_session_fields(session_id, current_step="merge_audio")
    
    # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ
    script_key = f"{storage_prefix}pipeline/script.json"
    audio_metadata_key = f"{storage_prefix}pipeline/audio_metadata.json"
    
    script_data = storage.download_json(script_key)
    audio_data = storage.download_json(audio_metadata_key)
    
    # ì˜¤ë””ì˜¤ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    audio_parts_keys = audio_data["audio_parts_keys"]
    temp_wav_files = []
    
    try:
        for audio_key in audio_parts_keys:
            audio_bytes = storage.download(audio_key)
            
            temp_fd, temp_path = tempfile.mkstemp(suffix=".wav")
            with os.fdopen(temp_fd, 'wb') as f:
                f.write(audio_bytes)
            
            temp_wav_files.append(temp_path)
        
        from app.langgraph_pipeline.podcast.graph import merge_audio_node, generate_transcript_node
        
        # âœ… Merge node
        merge_input = {
            "wav_files": temp_wav_files,
            "audio_metadata": audio_data["audio_metadata"],
            "script": script_data["script"],
            "title": script_data["title"],
            "usage": audio_data.get("usage", {}),
            "errors": [],
        }
        
        state = _trace_with_parent("merge_audio", root_run_id, merge_audio_node, merge_input)
        
        if not session_exists(session_repo, session_id):
            logger.info(f"[Finalize] Session {session_id} deleted during merge")
            return
        
        if state.get("errors"):
            raise Exception(f"Audio merge failed: {state['errors']}")
        
        session_repo.update_session_fields(session_id, current_step="merge_complete")
        
        # âœ… Transcript node
        state = _trace_with_parent("generate_transcript", root_run_id, generate_transcript_node, state)
        
        if not session_exists(session_repo, session_id):
            logger.info(f"[Finalize] Session {session_id} deleted during transcript")
            return
        
        # ìµœì¢… íŒŒì¼ ì—…ë¡œë“œ
        final_audio_path = state["final_podcast_path"]
        transcript_path = state["transcript_path"]
        
        output_dir = f"{storage_prefix}output_files/"
        
        # ì˜¤ë””ì˜¤ ì—…ë¡œë“œ
        with open(final_audio_path, 'rb') as f:
            audio_bytes = f.read()
        audio_key = f"{output_dir}audio/{os.path.basename(final_audio_path)}"
        storage.upload_bytes(audio_key, audio_bytes, content_type="audio/mpeg")
        
        # ìŠ¤í¬ë¦½íŠ¸ ì—…ë¡œë“œ
        with open(transcript_path, 'rb') as f:
            script_bytes = f.read()
        script_out_key = f"{output_dir}script/{os.path.basename(transcript_path)}"
        storage.upload_bytes(script_out_key, script_bytes, content_type="text/plain")
        

        # ì˜¤ë””ì˜¤ ê¸¸ì´ ê³„ì‚°
        from pydub import AudioSegment
        from pathlib import Path

        ffmpeg_path, ffprobe_path = prepare_ffmpeg_binaries()

        # pydubì´ ffmpegëŠ” converterë¡œ ì‚¬ìš©
        AudioSegment.converter = ffmpeg_path

        # ffprobeëŠ” PATHì—ì„œ ì°¾ëŠ” ê²½ìš°ê°€ ë§ì•„ì„œ /tmp/binì„ PATHì— ì¶”ê°€
        tmp_bin = str(Path(ffprobe_path).parent)  # "/tmp/bin"
        os.environ["PATH"] = tmp_bin + ":" + os.environ.get("PATH", "")

        # (ìˆìœ¼ë©´) pydubì— ffprobe ê²½ë¡œë„ ì§ì ‘ ì§€ì •
        try:
            AudioSegment.ffprobe = ffprobe_path
        except Exception:
            pass

        # (ë³´ì¡°) envë„ ê°™ì´ ë„£ì–´ë‘ë©´ ë‹¤ë¥¸ ì½”ë“œ ê²½ë¡œì—ë„ ì•ˆì „
        os.environ["FFMPEG_BINARY"] = ffmpeg_path
        os.environ["FFPROBE_BINARY"] = ffprobe_path

        audio = AudioSegment.from_file(final_audio_path)
        total_duration_sec = int(len(audio) / 1000)
        
        # ìŠ¤í¬ë¦½íŠ¸ í…ìŠ¤íŠ¸ ì½ê¸°
        with open(transcript_path, 'r', encoding='utf-8') as f:
            script_text = f.read()
        
        # ìµœì¢… ì—…ë°ì´íŠ¸
        if not session_exists(session_repo, session_id):
            logger.info(f"[Finalize] Session {session_id} deleted before final update")
            try:
                storage.delete(audio_key)
                storage.delete(script_out_key)
            except:
                pass
            return
        
        session_repo.update_session_fields(
            session_id,
            status="completed",
            current_step="completed",
            audio_key=audio_key,
            script_key=script_out_key,
            script_text=script_text,
            total_duration_sec=total_duration_sec,
        )
        
        logger.info(f"[Finalize] âœ… Completed for session={session_id}")
        
        # âœ… Root run ì¢…ë£Œ
        logger.info(f"ğŸ” Root run ID for closing: {root_run_id}")
        if root_run_id:
            try:
                from langsmith import Client
                from datetime import datetime
                
                logger.info(f"âœ… Attempting to close root run: {root_run_id}")
                ls_client = Client()
                ls_client.update_run(
                    root_run_id,
                    end_time=datetime.now(),
                    outputs={
                        "audio_key": audio_key,
                        "script_key": script_out_key,
                        "total_duration_sec": total_duration_sec,
                        "status": "completed",
                    }
                )
                logger.info(f"âœ… LangSmith root run closed successfully: {root_run_id}")
            except Exception as e:
                logger.error(f"âŒ Root run ì¢…ë£Œ ì‹¤íŒ¨: {e}", exc_info=True)
        else:
            logger.warning("âš ï¸ No root_run_id found - cannot close root run")
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        try:
            os.remove(final_audio_path)
            os.remove(transcript_path)
        except Exception:
            pass
        
    finally:
        # WAV íŒŒì¼ ì •ë¦¬
        for temp_file in temp_wav_files:
            try:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
            except Exception:
                pass