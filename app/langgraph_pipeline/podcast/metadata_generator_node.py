"""
Metadata Generator Node (V2 - pdfplumber ì „í™˜)
===============================================

ë³€ê²½ì‚¬í•­:
- PyMuPDF ì™„ì „ ì œê±°
- pdfplumber + OCR (pdf2image + PaddleOCR)ë¡œ í†µí•©
- improved_hybrid_filter.py V3ì™€ ì™„ì „ í˜¸í™˜

ì…ë ¥:
- primary_file: ì£¼ê°•ì˜ìë£Œ (1ê°œ, í•„ìˆ˜)
- supplementary_files: ë³´ì¡°ìë£Œ (0~3ê°œ, ì„ íƒ)

ì¶œë ¥:
- metadata.json (ì´ë¯¸ì§€ ì„¤ëª… í¬í•¨)

í†µí•©:
- DocumentConverterNode: PDF ë³€í™˜ + TXT/URL ì²˜ë¦¬
- ImprovedHybridFilterPipeline: ì´ë¯¸ì§€ í•„í„°ë§
- TextExtractor: í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
- ImageDescriptionGenerator: ì´ë¯¸ì§€ ìƒì„¸ ì„¤ëª…

"""

import os
import json
import tempfile
from pathlib import Path
from typing import List, Optional, Dict, Any
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

# OCR ë¡œê·¸ ì–µì œ
os.environ['FLAGS_log_level'] = '3'
os.environ['PPOCR_SHOW_LOG'] = 'False'

# pdfplumber (í•„ìˆ˜)
try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    print("âŒ pdfplumberê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("   pip install pdfplumber")
    PDFPLUMBER_AVAILABLE = False

# OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„ íƒ)
try:
    from paddleocr import PaddleOCR
    from pdf2image import convert_from_path
    import numpy as np
    from PIL import Image
    from io import BytesIO
    
    OCR_AVAILABLE = True
    ocr_engine = PaddleOCR(lang='korean', use_textline_orientation=True)
    print("âœ… OCR ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ (PaddleOCR)")
except ImportError:
    OCR_AVAILABLE = False
    ocr_engine = None
    print("âš ï¸  OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜ (ì„ íƒ ì‚¬í•­)")
except Exception as e:
    print(f"âš ï¸  PaddleOCR ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    OCR_AVAILABLE = False
    ocr_engine = None

# ê¸°ì¡´ ë…¸ë“œ ì„í¬íŠ¸
from .document_converter_node import DocumentConverterNode, DocumentType
from .improved_hybrid_filter import (
    ImprovedHybridFilterPipeline,
    UniversalImageExtractor,
    ImageMetadata,
    model
)

from vertexai.generative_models import Part


class TextExtractor:
    """
    PDFì—ì„œ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ + ë§ˆì»¤ ì‚½ì…
    V2: pdfplumber + OCR í†µí•©
    """
    
    def __init__(self):
        """TextExtractor ì´ˆê¸°í™”"""
        if not PDFPLUMBER_AVAILABLE:
            raise ImportError("pdfplumberê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install pdfplumber")
        
        self.ocr_enabled = OCR_AVAILABLE
        self.min_text_length = 100  # OCR íŠ¸ë¦¬ê±° ê¸°ì¤€
    
    def _perform_ocr_on_page(self, pdf_path: str, page_num: int) -> str:
        """
        í˜ì´ì§€ì— OCR ìˆ˜í–‰ (pdf2image + PaddleOCR)
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            page_num: í˜ì´ì§€ ë²ˆí˜¸ (0-indexed)
        
        Returns:
            OCRë¡œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸
        """
        if not self.ocr_enabled or ocr_engine is None:
            return ""
        
        try:
            # PDF í˜ì´ì§€ â†’ ì´ë¯¸ì§€ ë³€í™˜
            images = convert_from_path(
                pdf_path,
                first_page=page_num + 1,  # 1-indexed
                last_page=page_num + 1,
                dpi=150
            )
            
            if not images:
                return ""
            
            img = images[0]
            img_array = np.array(img)
            
            # OCR ì‹¤í–‰
            result = ocr_engine.ocr(img_array, cls=True)
            
            if result and result[0]:
                lines = []
                for line in result[0]:
                    if line and len(line) >= 2:
                        text = line[1][0]
                        lines.append(text)
                return "\n".join(lines)
            
            return ""
        
        except Exception as e:
            print(f"      âš ï¸  OCR ì‹¤íŒ¨: {e}")
            return ""
    
    def extract_with_markers(
        self, 
        pdf_path: str, 
        prefix: str = "MAIN"
    ) -> Dict[str, Any]:
        """
        PDFì—ì„œ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ + ë§ˆì»¤ ì‚½ì…
        pdfplumber ì‚¬ìš©, í…ìŠ¤íŠ¸ ë¶€ì¡± ì‹œ OCR ìë™ ìˆ˜í–‰
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            prefix: í˜ì´ì§€ ë§ˆì»¤ ì ‘ë‘ì‚¬ (MAIN, SUPP1, SUPP2, SUPP3)
        
        Returns:
            {
                "full_text": "[MAIN-PAGE 1: ì œëª©]\në‚´ìš©...",
                "total_pages": 21
            }
        """
        pages_text = []
        total_pages = 0
        ocr_count = 0
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                total_pages = len(pdf.pages)
                
                print(f"   ğŸ“„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘... (OCR {'í™œì„±í™”' if self.ocr_enabled else 'ë¹„í™œì„±í™”'})")
                
                for page_num, page in enumerate(pdf.pages):
                    # pdfplumberë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    text = page.extract_text() or ""
                    text_length = len(text.strip())
                    
                    # í…ìŠ¤íŠ¸ ë¶€ì¡± â†’ OCR ìˆ˜í–‰
                    if text_length < self.min_text_length and self.ocr_enabled:
                        print(f"      â†’ í˜ì´ì§€ {page_num + 1}: í…ìŠ¤íŠ¸ ë¶€ì¡± ({text_length}ì) â†’ OCR ìˆ˜í–‰")
                        ocr_text = self._perform_ocr_on_page(pdf_path, page_num)
                        
                        if ocr_text:
                            text = ocr_text
                            ocr_count += 1
                            print(f"         âœ… OCR ì™„ë£Œ ({len(ocr_text)}ì ì¶”ì¶œ)")
                        else:
                            print(f"         âš ï¸  OCR ì‹¤íŒ¨, ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©")
                    
                    # í˜ì´ì§€ ì œëª© ì¶”ì¶œ
                    lines = text.split('\n')
                    title = lines[0][:50] if lines and lines[0].strip() else f"Page {page_num + 1}"
                    
                    # ë§ˆì»¤ ì‚½ì…
                    pages_text.append(f"[{prefix}-PAGE {page_num + 1}: {title}]")
                    pages_text.append(text)
                    pages_text.append("")
                
                if ocr_count > 0:
                    print(f"   âœ… OCR ì²˜ë¦¬ ì™„ë£Œ: {ocr_count}ê°œ í˜ì´ì§€")
        
        except Exception as e:
            print(f"   âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {"full_text": "", "total_pages": 0}
        
        return {
            "full_text": "\n".join(pages_text),
            "total_pages": total_pages
        }


class ImageDescriptionGenerator:
    """í†µê³¼ëœ ì´ë¯¸ì§€ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª… ìƒì„± (2-4ë¬¸ì¥)"""
    
    
    def __init__(self):
        """ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±ê¸° ì´ˆê¸°í™”"""
        self.total_tokens = 0  # âœ… ëˆ„ì  í† í° ìˆ˜
        self.description_count = 0  # ìƒì„±í•œ ì„¤ëª… ê°œìˆ˜
        
        # âœ… Gemini ëª¨ë¸ ì´ˆê¸°í™”
        from .improved_hybrid_filter import get_global_model
        self.model = get_global_model()
        
        if self.model is None:
            print("      âš ï¸  Warning: Gemini ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨ - ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ë¶ˆê°€")
    def generate_description(
        self, 
        image_bytes: bytes, 
        adjacent_text: str,
        keywords: List[str],
        max_retries=3
    ) -> str:
        """
        Vision APIë¡œ ì´ë¯¸ì§€ ìƒì„¸ ì„¤ëª… ìƒì„±
        ì¬ì‹œë„ ë¡œì§ í¬í•¨ (429 Rate Limit ëŒ€ì‘)
        """
        import time
        
        for attempt in range(max_retries):
            try:
                mime_type = self._get_mime_type(image_bytes)
                image_part = Part.from_data(data=image_bytes, mime_type=mime_type)
                
                keyword_context = ', '.join(keywords[:10]) if keywords else "ì¼ë°˜ í•™ìŠµ ë‚´ìš©"
                
                prompt = f"""
ì´ ì´ë¯¸ì§€ë¥¼ 2-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.

ê°•ì˜ ì£¼ì œ: {keyword_context}
ì£¼ë³€ í…ìŠ¤íŠ¸: "{adjacent_text}"

ì„¤ëª…ì— í¬í•¨í•  ë‚´ìš©:
1. ì´ë¯¸ì§€ê°€ ë‚˜íƒ€ë‚´ëŠ” ì£¼ì œ/ê°œë… (1ë¬¸ì¥)
2. ì£¼ìš” êµ¬ì„± ìš”ì†Œ 2-3ê°œ (1-2ë¬¸ì¥)
3. í•µì‹¬ ì •ë³´ë‚˜ íŒ¨í„´ (1ë¬¸ì¥)

ì œì™¸í•  ë‚´ìš©:
- ì„¸ë¶€ ìš”ì†Œ ì „ì²´ ë‚˜ì—´
- ë¶ˆí•„ìš”í•œ ì¶”ì¸¡ì´ë‚˜ í•´ì„

ì¶œë ¥: ëª…í™•í•˜ê³  ê°„ê²°í•œ 2-4ë¬¸ì¥ë§Œ.
"""
                
                if not self.model:
                    return "ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: Model not initialized"
                
                response = self.model.generate_content([image_part, prompt])
                description = response.text.strip()
                
                # âœ… í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  (usage_metadata ìš°ì„ )
                tokens_added = 0
                try:
                    # Method 1: response.usage_metadata (ê°€ì¥ ì •í™•)
                    if hasattr(response, 'usage_metadata') and response.usage_metadata:
                        tokens_added = getattr(response.usage_metadata, 'total_token_count', 0)
                        if tokens_added > 0:
                            self.total_tokens += tokens_added
                            self.description_count += 1
                except Exception:
                    pass
                
                return description
                
            except Exception as e:
                error_msg = str(e)
                
                if "429" in error_msg or "Resource exhausted" in error_msg:
                    if attempt < max_retries - 1:
                        wait_time = (attempt + 1) * 3
                        print(f"      âš ï¸  Rate Limit, {wait_time}ì´ˆ ëŒ€ê¸° ì¤‘...", end='', flush=True)
                        time.sleep(wait_time)
                        print(" ì¬ì‹œë„")
                        continue
                    else:
                        return "ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: API rate limit exceeded"
                else:
                    return f"ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {error_msg}"
        
        return "ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: Failed after all retries"
    
    def _get_mime_type(self, image_bytes: bytes) -> str:
        """ì´ë¯¸ì§€ ë°”ì´ë„ˆë¦¬ì—ì„œ MIME íƒ€ì… ê°ì§€"""
        if image_bytes.startswith(b'\xff\xd8'):
            return "image/jpeg"
        elif image_bytes.startswith(b'\x89PNG\r\n\x1a\n'):
            return "image/png"
        elif image_bytes.startswith(b'GIF87a') or image_bytes.startswith(b'GIF89a'):
            return "image/gif"
        elif image_bytes.startswith(b'RIFF') and image_bytes[8:12] == b'WEBP':
            return "image/webp"
        return "image/png"


class MetadataGenerator:
    """
    ë©”íƒ€ë°ì´í„° ìƒì„± ë…¸ë“œ
    
    ì£¼ê°•ì˜ìë£Œ + ë³´ì¡°ìë£Œ â†’ metadata.json
    """
    
    def __init__(self):
        self.converter = None
        self.text_extractor = TextExtractor()
        self.image_filter = ImprovedHybridFilterPipeline(auto_extract_keywords=True)
        self.image_describer = ImageDescriptionGenerator()
    
    def _extract_page_title(self, slide_title: str, adjacent_text: str) -> str:
        """ì˜ë¯¸ìˆëŠ” í˜ì´ì§€ ì œëª© ì¶”ì¶œ"""
        if slide_title and slide_title.strip() and slide_title.lower() != "no title":
            return slide_title.strip()[:50]
        
        if adjacent_text:
            lines = adjacent_text.strip().split('\n')
            for line in lines:
                line = line.strip()
                if len(line) > 3 and not line.startswith('â˜'):
                    return line[:50]
        
        return "í˜ì´ì§€ ì œëª© ì—†ìŒ"
    
    def generate(
        self,
        primary_file: str,
        supplementary_files: Optional[List[str]] = None,
        output_path: str = "output/metadata.json"
    ) -> str:
        """ë©”íƒ€ë°ì´í„° ìƒì„±"""
        print(f"\n{'='*120}")
        print(f"ğŸ¯ ë©”íƒ€ë°ì´í„° ìƒì„± ì‹œì‘")
        print(f"{'='*120}")
        print(f"ì£¼ê°•ì˜ìë£Œ: {primary_file}")
        if supplementary_files:
            print(f"ë³´ì¡°ìë£Œ: {len(supplementary_files)}ê°œ")
            for i, supp in enumerate(supplementary_files, 1):
                print(f"  {i}. {supp}")
        print(f"{'='*120}\n")
        
        with tempfile.TemporaryDirectory() as temp_dir:
            self.converter = DocumentConverterNode(output_dir=temp_dir)
            
            print("ğŸ“„ [1/3] ì£¼ê°•ì˜ìë£Œ ì²˜ë¦¬ ì¤‘...")
            primary_metadata = self._process_primary_source(primary_file)
            
            print("\nğŸ“š [2/3] ë³´ì¡°ìë£Œ ì²˜ë¦¬ ì¤‘...")
            supplementary_metadata = []
            if supplementary_files:
                for i, supp_file in enumerate(supplementary_files[:3], 1):
                    try:
                        supp_meta = self._process_supplementary_source(supp_file, i)
                        supplementary_metadata.append(supp_meta)
                        print(f"   âœ… ë³´ì¡°ìë£Œ {i} ì²˜ë¦¬ ì„±ê³µ")
                    except Exception as e:
                        print(f"   âš ï¸ ë³´ì¡°ìë£Œ {i} ì²˜ë¦¬ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
            else:
                print("   âš ï¸  ë³´ì¡°ìë£Œ ì—†ìŒ (ì„ íƒ ì‚¬í•­)")
            
            print("\nğŸ”§ [3/3] ë©”íƒ€ë°ì´í„° í†µí•© ì¤‘...")
            
            # âœ… Vision í† í° í†µê³„ ìˆ˜ì§‘
            vision_tokens = {}
            if hasattr(self.image_filter, 'vision_tokens'):
                vision_tokens = self.image_filter.vision_tokens.copy()
                print(f"   [DEBUG] image_filter.vision_tokens = {vision_tokens}")
            
            # âœ… ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± í† í° ì¶”ê°€
            print(f"   [DEBUG] image_describer.total_tokens = {self.image_describer.total_tokens}")
            print(f"   [DEBUG] image_describer.description_count = {self.image_describer.description_count}")
            
            if self.image_describer.total_tokens > 0:
                vision_tokens['image_description'] = self.image_describer.total_tokens
                vision_tokens['description_count'] = self.image_describer.description_count
                vision_tokens['total'] = vision_tokens.get('total', 0) + self.image_describer.total_tokens
                print(f"   [DEBUG] vision_tokens after adding image_description = {vision_tokens}")
            
            # âœ… ë¹„ìš© ê³„ì‚°
            if vision_tokens.get('total', 0) > 0:
                from .pricing import calculate_vision_cost, format_cost
                vision_cost = calculate_vision_cost(vision_tokens['total'])
                vision_tokens['cost_usd'] = vision_cost
            
            metadata = {
                "metadata_version": "1.0",
                "created_at": datetime.now().isoformat(),
                "primary_source": primary_metadata,
                "supplementary_sources": supplementary_metadata
            }
            
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            print(f"\n{'='*120}")
            print(f"âœ… ë©”íƒ€ë°ì´í„° ìƒì„± ì™„ë£Œ!")
            print(f"{'='*120}")
            print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_path}")
            print(f"ğŸ“Š ì£¼ê°•ì˜ìë£Œ í˜ì´ì§€: {primary_metadata['total_pages']}ê°œ")
            print(f"ğŸ–¼ï¸  í•„í„°ë§ëœ ì´ë¯¸ì§€: {len(primary_metadata['filtered_images'])}ê°œ")
            if supplementary_metadata:
                total_supp_pages = sum(s['total_pages'] for s in supplementary_metadata)
                print(f"ğŸ“š ë³´ì¡°ìë£Œ í˜ì´ì§€: {total_supp_pages}ê°œ")
            
            # âœ… Vision í† í° í†µê³„ ì¶œë ¥
            if vision_tokens:
                print(f"\nğŸ’° Vision API ì‚¬ìš© í†µê³„:")
                if 'keyword_extraction' in vision_tokens:
                    print(f"   ğŸ“ í‚¤ì›Œë“œ ì¶”ì¶œ: {vision_tokens['keyword_extraction']:,} tokens")
                if 'image_filtering' in vision_tokens:
                    print(f"   ğŸ” ì´ë¯¸ì§€ í•„í„°ë§: {vision_tokens['image_filtering']:,} tokens")
                if 'image_description' in vision_tokens:
                    print(f"   ğŸ“¸ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±: {vision_tokens['image_description']:,} tokens ({vision_tokens['description_count']}ê°œ)")
                if 'total' in vision_tokens:
                    print(f"   ğŸ“Š Total: {vision_tokens['total']:,} tokens")
                if 'cost_usd' in vision_tokens:
                    print(f"   ğŸ’µ ë¹„ìš©: {format_cost(vision_tokens['cost_usd'])}")
            
            print(f"{'='*120}\n")
            
            # âœ… vision_tokensì™€ í•¨ê»˜ ë°˜í™˜
            return {
                "metadata_path": str(output_path),
                "vision_tokens": vision_tokens
            }
    
    def _process_primary_source(self, file_path: str) -> Dict[str, Any]:
        """
        ì£¼ê°•ì˜ìë£Œ ì²˜ë¦¬
        âœ… TXT/URL ì§€ì› ì¶”ê°€
        âœ… PPTX ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ (PDF ë³€í™˜ ì—†ì´)
        """
        file_path_str = str(file_path)
        
        # ì›ë³¸ íŒŒì¼ íƒ€ì… ê°ì§€
        if file_path_str.startswith(('http://', 'https://')):
            original_file_type = 'url'
            file_path_obj = None
            display_name = file_path_str[:50]
        else:
            file_path_obj = Path(file_path)
            original_file_type = file_path_obj.suffix.lower().replace('.', '')
            display_name = file_path_obj.name
        
        print(f"   ğŸ“„ íŒŒì¼: {display_name} ({original_file_type})")
        
        # âœ… PPTXëŠ” ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ (PDF ë³€í™˜ ì‹œ í•œê¸€ ê¹¨ì§ ë°©ì§€)
        if original_file_type == 'pptx':
            print(f"   ğŸ“ PPTX ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘... (PDF ë³€í™˜ ê±´ë„ˆëœ€)")
            from pptx import Presentation
            
            prs = Presentation(file_path_str)
            pages_text = []
            total_pages = 0
            
            for slide_num, slide in enumerate(prs.slides, 1):
                total_pages += 1
                
                # ìŠ¬ë¼ì´ë“œ ì œëª© ì¶”ì¶œ
                title = "No Title"
                if slide.shapes.title and slide.shapes.title.text.strip():
                    title = slide.shapes.title.text.strip()[:50]
                
                # í˜ì´ì§€ ë§ˆì»¤
                pages_text.append(f"[MAIN-PAGE {slide_num}: {title}]")
                
                # ìŠ¬ë¼ì´ë“œ ë‚´ìš© ì¶”ì¶œ
                for shape in slide.shapes:
                    if hasattr(shape, "text") and shape.text.strip():
                        pages_text.append(shape.text.strip())
                
                pages_text.append("")  # ìŠ¬ë¼ì´ë“œ êµ¬ë¶„
            
            full_text = "\n".join(pages_text)
            print(f"   âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(full_text)}ì, {total_pages}í˜ì´ì§€")
            
            # ì´ë¯¸ì§€ëŠ” PPTX ì›ë³¸ì—ì„œ ì¶”ì¶œ
            print(f"   ğŸ–¼ï¸  ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘...")
            print(f"      â†’ PPTX ì›ë³¸ì—ì„œ ì§ì ‘ ì¶”ì¶œ")
            self.image_filter.extract_keywords_from_document(file_path_str)
            keywords = self.image_filter.document_keywords
            all_images = self._extract_images_from_pptx(file_path_str)
            
        else:
            # ê¸°ì¡´ ë°©ì‹: PDF ë³€í™˜
            print(f"   ğŸ”„ íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
            processed_path = self.converter.convert(file_path_str)
            
            # 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ
            print(f"   ğŸ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
            text_data = self.text_extractor.extract_with_markers(processed_path, prefix="MAIN")
            full_text = text_data['full_text']
            total_pages = text_data['total_pages']
            print(f"   âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(full_text)}ì")
            
            # 3. ì´ë¯¸ì§€ í•„í„°ë§
            print(f"   ğŸ–¼ï¸  ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘...")
            
            # TXT/URLì€ ì´ë¯¸ì§€ ì—†ìŒ
            if original_file_type in ['txt', 'url']:
                print(f"      â†’ TXT/URLì€ ì´ë¯¸ì§€ ì—†ìŒ, ê±´ë„ˆë›°ê¸°")
                all_images = []
                keywords = []
            
            elif original_file_type in ['docx', 'pdf']:
                print(f"      â†’ PDFì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ")
                self.image_filter.extract_keywords_from_document(processed_path)
                keywords = self.image_filter.document_keywords
                extractor = UniversalImageExtractor()
                all_images = extractor.extract(processed_path)
            
            else:
                print(f"   âš ï¸  ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {original_file_type}")
                all_images = []
                keywords = []
        
        # 4. í•„í„°ë§ ì‹¤í–‰ (ê³µí†µ)
        filtered_images = []
        if all_images:
            print(f"   ğŸ” {len(all_images)}ê°œ ì´ë¯¸ì§€ ë°œê²¬, í•„í„°ë§ ì‹œì‘...")

            for img_meta in all_images:
                decision, reason = self.image_filter.step1_rule_check(img_meta)
                
                if decision == "INCLUDE":
                    img_meta.is_core_content = True
                    img_meta.filter_reason = reason
                    filtered_images.append(img_meta)
                    
                elif decision == "PENDING":
                    ai_result = self.image_filter.step2_gemini_check(img_meta)

                    # íŠœí”Œ ë°˜í™˜ ëŒ€ì‘
                    if isinstance(ai_result, tuple):
                        ai_result = ai_result[0]

                    if ai_result.upper().startswith("KEEP"):
                        img_meta.is_core_content = True
                        img_meta.filter_reason = ai_result
                        filtered_images.append(img_meta)
            
            print(f"   âœ… í•„í„°ë§ ì™„ë£Œ: {len(filtered_images)}ê°œ ì„ íƒ")
        
        # 5. ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±
        filtered_image_metadata = []
        
        if filtered_images:
            print(f"   ğŸ“ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì¤‘... (0/{len(filtered_images)})", end='', flush=True)
            
            # âœ… ì´ì „ í† í° ìˆ˜ ì €ì¥ (ê° ì´ë¯¸ì§€ë‹¹ í† í° ì¶”ì ìš©)
            prev_tokens = self.image_describer.total_tokens
            
            for i, img_meta in enumerate(filtered_images, 1):
                description = self.image_describer.generate_description(
                    img_meta.image_bytes,
                    img_meta.adjacent_text,
                    keywords
                )
                
                # âœ… ì´ë²ˆ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±ì— ì‚¬ìš©ëœ í† í° ê³„ì‚°
                current_tokens = self.image_describer.total_tokens - prev_tokens
                
                page_title = self._extract_page_title(
                    img_meta.slide_title,
                    img_meta.adjacent_text
                )
                
                filtered_image_metadata.append({
                    "image_id": img_meta.image_id.replace("S", "MAIN_P").replace("P", "MAIN_P"),
                    "page_number": img_meta.slide_number,
                    "page_title": page_title,
                    "description": description,
                    "filter_stage": "1ì°¨ (Rule)" if "Rule" in img_meta.filter_reason else "2ì°¨ (AI)",
                    "area_percentage": img_meta.area_percentage
                })
                
                # âœ… ì§„í–‰ ìƒí™©ê³¼ í•¨ê»˜ í† í° ì •ë³´ ì¶œë ¥
                if current_tokens > 0:
                    print(f"\r   ğŸ“ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì¤‘... ({i}/{len(filtered_images)}) - #{i}: {current_tokens:,} tokens", end='', flush=True)
                else:
                    print(f"\r   ğŸ“ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì¤‘... ({i}/{len(filtered_images)})", end='', flush=True)
                
                # ë‹¤ìŒ ì´ë¯¸ì§€ë¥¼ ìœ„í•´ prev_tokens ì—…ë°ì´íŠ¸
                prev_tokens = self.image_describer.total_tokens
            
            print()
            
            print(f"\n   {'='*80}")
            print(f"   ğŸ“Š ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì™„ë£Œ")
            print(f"      - ì²˜ë¦¬ëœ ì´ë¯¸ì§€: {len(filtered_images)}ê°œ")
            # âœ… ì´ í† í° ìˆ˜ ì¶œë ¥            
            if self.image_describer.total_tokens > 0:
                avg_tokens = self.image_describer.total_tokens / len(filtered_images) if len(filtered_images) > 0 else 0
                print(f"      - ì´ í† í°: {self.image_describer.total_tokens:,} tokens")
                print(f"      - í‰ê· : {avg_tokens:.0f} tokens/image")
            else:
                print(f"      âš ï¸  í† í° ì •ë³´ ì—†ìŒ (usage_metadata ë¯¸ì§€ì› ê°€ëŠ¥ì„±)")
            print(f"   {'='*80}\n")

        # 6. í†µê³„
        total_images = len(all_images)
        passed_images = len(filtered_images)
        
        return {
            "role": "main",
            "filename": display_name if original_file_type == 'url' else (file_path_obj.name if file_path_obj else display_name),
            "file_type": original_file_type,
            "total_pages": total_pages,
            "content": {
                "full_text": full_text
            },
            "filtered_images": filtered_image_metadata,
            "statistics": {
                "total_images_found": total_images,
                "images_passed": passed_images,
                "filter_rate": passed_images / total_images if total_images > 0 else 0
            }
        }
    
    def _process_supplementary_source(self, file_path: str, order: int) -> Dict[str, Any]:
        """
        ë³´ì¡°ìë£Œ ì²˜ë¦¬
        âœ… PPTX ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ (PDF ë³€í™˜ ì—†ì´)
        """
        file_path_str = str(file_path)
        
        # URLê³¼ íŒŒì¼ êµ¬ë¶„
        if file_path_str.startswith(('http://', 'https://')):
            file_type = 'url'
            display_name = 'Web Content'
            file_path_obj = None
        else:
            file_path_obj = Path(file_path)
            file_type = file_path_obj.suffix.lower().replace('.', '')
            display_name = file_path_obj.name
        
        print(f"   ğŸ“š ë³´ì¡°ìë£Œ {order}: {display_name} ({file_type})")
        
        # âœ… PPTXëŠ” ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ (PDF ë³€í™˜ ê±´ë„ˆëœ€)
        if file_type == 'pptx':
            print(f"      ğŸ“ PPTX ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘... (PDF ë³€í™˜ ê±´ë„ˆëœ€)")
            from pptx import Presentation
            
            prs = Presentation(file_path_str)
            pages_text = []
            total_pages = 0
            
            for slide_num, slide in enumerate(prs.slides, 1):
                total_pages += 1
                
                # ìŠ¬ë¼ì´ë“œ ì œëª© ì¶”ì¶œ
                title = "No Title"
                if slide.shapes.title and slide.shapes.title.text.strip():
                    title = slide.shapes.title.text.strip()[:50]
                
                # í˜ì´ì§€ ë§ˆì»¤
                pages_text.append(f"[SUPP{order}-PAGE {slide_num}: {title}]")
                
                # ìŠ¬ë¼ì´ë“œ ë‚´ìš© ì¶”ì¶œ
                for shape in slide.shapes:
                    if hasattr(shape, "text") and shape.text.strip():
                        pages_text.append(shape.text.strip())
                
                pages_text.append("")  # ìŠ¬ë¼ì´ë“œ êµ¬ë¶„
            
            full_text = "\n".join(pages_text)
            print(f"      âœ… ì™„ë£Œ ({total_pages}í˜ì´ì§€)")
            
        else:
            # ê¸°ì¡´ ë°©ì‹: PDF ë³€í™˜
            print(f"      ğŸ”„ PDF ë³€í™˜ ì¤‘...")
            pdf_path = self.converter.convert(file_path_str)
            
            print(f"      ğŸ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
            text_data = self.text_extractor.extract_with_markers(pdf_path, prefix=f"SUPP{order}")
            
            full_text = text_data['full_text']
            total_pages = text_data['total_pages']
            
            print(f"      âœ… ì™„ë£Œ ({total_pages}í˜ì´ì§€)")
        
        return {
            "order": order,
            "filename": display_name,
            "file_type": file_type,
            "total_pages": total_pages,
            "content": {
                "full_text": full_text
            }
        }
    
    def _extract_images_from_pptx(self, pptx_path: str) -> List[ImageMetadata]:
        """PPTXì—ì„œ ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        extractor = UniversalImageExtractor()
        return extractor.extract(pptx_path)


# CLI ì¸í„°í˜ì´ìŠ¤
if __name__ == "__main__":
    import sys
    
    print("\n" + "="*120)
    print("ğŸ¯ Metadata Generator Node (V2 - pdfplumber)")
    print("="*120)
    
    if len(sys.argv) < 2:
        print("\nì‚¬ìš©ë²•:")
        print("  python metadata_generator_node.py <ì£¼ê°•ì˜ìë£Œ> [ë³´ì¡°1] [ë³´ì¡°2] [ë³´ì¡°3]")
        print("\nì˜ˆì‹œ:")
        print("  python metadata_generator_node.py ì¤‘ë“±êµ­ì–´1.pptx")
        print("  python metadata_generator_node.py notes.txt")
        print("  python metadata_generator_node.py https://example.com/article")
        print("\nâœ… ì§€ì› í˜•ì‹: PPTX, DOCX, PDF, TXT, URL")
        print("="*120 + "\n")
        sys.exit(1)
    
    primary_file = sys.argv[1]
    supplementary_files = sys.argv[2:5] if len(sys.argv) > 2 else None
    
    if not primary_file.startswith('http') and not os.path.exists(primary_file):
        print(f"\nâŒ ì£¼ê°•ì˜ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {primary_file}")
        sys.exit(1)
    
    if supplementary_files:
        for supp in supplementary_files:
            if not supp.startswith('http') and not os.path.exists(supp):
                print(f"\nâŒ ë³´ì¡°ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {supp}")
                sys.exit(1)
    
    try:
        generator = MetadataGenerator()
        output_path = generator.generate(
            primary_file=primary_file,
            supplementary_files=supplementary_files,
            output_path="output/metadata.json"
        )
        
        print(f"âœ… ì„±ê³µ!")
        print(f"ğŸ“ {output_path}")
        
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)