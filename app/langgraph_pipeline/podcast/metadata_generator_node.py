"""
Metadata Generator Node (V2 - pdfplumber ì „í™˜)
===============================================

ë³€ê²½ì‚¬í•­:
- PyMuPDF ì™„ì „ ì œê±°
- pdfplumber + OCR (pypdfium2 + PaddleOCR)ë¡œ í†µí•©
- improved_hybrid_filter.py V3ì™€ ì™„ì „ í˜¸í™˜

ì…ë ¥:
- primary_file: ì£¼ê°•ì˜ìë£Œ (1ê°œ, í•„ìˆ˜)
- supplementary_files: ë³´ì¡°ìë£Œ (0~3ê°œ, ì„ íƒ)

ì¶œë ¥:
- metadata.json (ì´ë¯¸ì§€ ì„¤ëª… í¬í•¨)

í†µí•©:
- DocumentConverterNode: PDF ë³€í™˜ + TXT/URL ì²˜ë¦¬
- ImprovedHybridFilterPipeline: ì´ë¯¸ì§€ í•„í„°ë§
- TextExtractor: í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
- ImageDescriptionGenerator: ì´ë¯¸ì§€ ìƒì„¸ ì„¤ëª…

"""

import os
import json
import tempfile
from pathlib import Path
from typing import List, Optional, Dict, Any
from datetime import datetime
from pathlib import Path
import traceback
import cv2
import io
import numpy as np
import logging
import sys

logger = logging.getLogger(__name__)

def _log(*args, level: str | None = None, exc_info: bool = False, end: str = '\n', flush: bool = False) -> None:
    """
    logger ê¸°ë°˜ ë¡œê·¸ (í™˜ê²½ë³„ LOG_LEVEL ì ìš©).
    - ê¸°ë³¸ level: DEBUG
    - end/flushë¥¼ ì“°ëŠ” ì§„í–‰í˜• ì¶œë ¥(end != '\\n' ë˜ëŠ” flush=True)ì€ ê¸°ì¡´ì²˜ëŸ¼ print ìœ ì§€
      (ë¡œê¹…ìœ¼ë¡œ ë°”ê¾¸ë©´ ì¤„ë°”ê¿ˆ/ë²„í¼ë§ ë™ì‘ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ)
    """
    msg = " ".join(str(a) for a in args).rstrip() if args else ""

    # ì§„í–‰í˜• ì¶œë ¥ì€ ê·¸ëŒ€ë¡œ stdoutë¡œ (ê¸°ì¡´ UX ìœ ì§€)
    if end != "\n" or flush:
        print(msg, end=end, flush=flush)
        return

    lvl = (level or "DEBUG").upper()
    if lvl == "DEBUG":
        logger.debug(msg, exc_info=exc_info)
    elif lvl == "INFO":
        logger.info(msg, exc_info=exc_info)
    elif lvl in ("WARN", "WARNING"):
        logger.warning(msg, exc_info=exc_info)
    elif lvl == "ERROR":
        logger.error(msg, exc_info=exc_info)
    elif lvl in ("CRITICAL", "FATAL"):
        logger.critical(msg, exc_info=exc_info)
    else:
        logger.debug(msg, exc_info=exc_info)

# OCR ë¡œê·¸ ì–µì œ
os.environ['FLAGS_log_level'] = '3'
os.environ['PPOCR_SHOW_LOG'] = 'False'

# pdfplumber (í•„ìˆ˜)
try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    _log("âŒ pdfplumberê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", level="ERROR")
    _log("   pip install pdfplumber", level="ERROR")
    PDFPLUMBER_AVAILABLE = False

# OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„ íƒ) - pypdfium2 ì‚¬ìš©
OCR_AVAILABLE = False
ocr_engine = None

try:
    from paddleocr import PaddleOCR
    import numpy as np
    from pypdfium2 import PdfDocument
    from PIL import Image

    # ì—¬ê¸°ê¹Œì§€ ì„±ê³µí•˜ë©´ "ì˜ì¡´ì„±"ì€ OK
    try:
        ocr_engine = PaddleOCR(
            lang='korean',
            use_angle_cls=True
        )
        OCR_AVAILABLE = True
        _log("âœ… OCR ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ (PaddleOCR + pypdfium2)", level="INFO")
    except Exception as e:
        # ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨ëŠ” ì˜ì¡´ì„± ë¬¸ì œì™€ ë¶„ë¦¬í•´ì„œ í‘œì‹œ
        _log(f"âš ï¸  OCR ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨(ì—”ì§„): {e}", level="WARNING")

except ImportError as e:
    # ì˜ì¡´ì„± import ì‹¤íŒ¨ë§Œ ì—¬ê¸°ë¡œ ì˜´
    _log(f"âš ï¸  OCR ì˜ì¡´ì„± ë¯¸ì„¤ì¹˜(ì„ íƒ): {e}", level="WARNING")


# ê¸°ì¡´ ë…¸ë“œ ì„í¬íŠ¸
from .document_converter_node import DocumentConverterNode, DocumentType
from .improved_hybrid_filter import (
    ImprovedHybridFilterPipeline,
    UniversalImageExtractor,
    ImageMetadata,
    get_global_model,
    gemini_ocr_image_bytes
)

from vertexai.generative_models import Part

class TextExtractor:
    """
    PDFì—ì„œ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ + ë§ˆì»¤ ì‚½ì…
    V2: pdfplumber + OCR í†µí•©
    """

    def __init__(self):
        if not PDFPLUMBER_AVAILABLE:
            raise ImportError("pdfplumberê°€ í•„ìš”í•©ë‹ˆë‹¤")

        self.ocr_enabled = OCR_AVAILABLE
        self.min_text_length = 100
        self._ocr_engine = None

        # OCR ê²°ê³¼ê°€ ë¹„ì–´ìˆì„ ë•Œ Gemini(Vision)ë¡œ ë³´ì¡° OCR ì‹œë„ (ë¹„ìš©/ì§€ì—° ì£¼ì˜)
        self.gemini_ocr_fallback = os.getenv('GEMINI_OCR_FALLBACK', 'false').lower() in ('1','true','yes','y')
        
        # âœ… V3: ìƒ˜í”Œë§ í˜ì´ì§€ ìˆ˜ ì„¤ì • (ê¸°ë³¸: 15)
        self.gemini_ocr_max_sample_pages = int(os.getenv('GEMINI_OCR_MAX_SAMPLE_PAGES', '15'))
        
        # âœ… V3: ìƒ˜í”Œë§ í†µê³„
        self._gemini_ocr_used_pages = 0
        self._gemini_ocr_skipped_pages = 0

    def _safe_parse_ocr_result(self, result):
        """
        PaddleOCR ê²°ê³¼ë¥¼ ì•ˆì „í•˜ê²Œ íŒŒì‹±
        ë‹¤ì–‘í•œ ê²°ê³¼ í¬ë§· ëŒ€ì‘
        
        Args:
            result: PaddleOCR ê²°ê³¼ (ë‹¤ì–‘í•œ í˜•íƒœ ê°€ëŠ¥)
        
        Returns:
            List[str]: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        texts = []
        
        # None ì²´í¬
        if not result:
            return texts
        
        # resultê°€ listì¸ì§€ í™•ì¸
        if not isinstance(result, list):
            return texts
        
        # result[0] ì¶”ì¶œ (PaddleOCRëŠ” ë³´í†µ [[...]] í˜•íƒœ)
        try:
            items = result[0] if result and isinstance(result[0], list) else result
        except (IndexError, TypeError):
            return texts
        
        # ê° item íŒŒì‹±
        for item in items:
            try:
                # Case 1: [[bbox], ("text", score)] í˜•íƒœ
                if isinstance(item, (list, tuple)) and len(item) >= 2:
                    # item[1]ì´ tuple/listì¸ì§€ í™•ì¸
                    if isinstance(item[1], (list, tuple)) and len(item[1]) > 0:
                        text = str(item[1][0]).strip()
                        if text and len(text) > 1:  # 1ê¸€ì ì¡ìŒ ì œê±°
                            texts.append(text)
                    # item[1]ì´ stringì¸ ê²½ìš°
                    elif isinstance(item[1], str):
                        text = item[1].strip()
                        if text and len(text) > 1:
                            texts.append(text)
                
                # Case 2: {"text": "...", "score": ...} í˜•íƒœ (dict)
                elif isinstance(item, dict):
                    text = item.get("text", "").strip()
                    if text and len(text) > 1:
                        texts.append(text)
                
                # Case 3: ë‹¨ìˆœ string (ë“œë¬¼ì§€ë§Œ ëŒ€ì‘)
                elif isinstance(item, str):
                    text = item.strip()
                    if text and len(text) > 1:
                        texts.append(text)
                        
            except Exception as e:
                # ê°œë³„ item íŒŒì‹± ì‹¤íŒ¨ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì†
                _log(f"âš ï¸ OCR ê²°ê³¼ íŒŒì‹± ìŠ¤í‚µ: {e}", level="DEBUG")
                continue
        
        return texts
    
    def _perform_ocr_on_page(self, pdf_path: str, page_number: int):
        """
        í˜ì´ì§€ì— OCR ìˆ˜í–‰ (pypdfium2 + PaddleOCR)
        """
        try:
            pdf = PdfDocument(pdf_path)
            page = pdf[page_number - 1]

            # 1. PDF â†’ ì´ë¯¸ì§€
            bitmap = page.render(scale=3.0)  # ìŠ¤ì¼€ì¼ ì˜¬ë¦¼
            pil_img = bitmap.to_pil()

            # 2. OCR ì—”ì§„ ì´ˆê¸°í™”
            if self._ocr_engine is None:
                self._ocr_engine = PaddleOCR(
                    lang="korean",
                    use_angle_cls=True,
                    det_db_thresh=0.1,  # ë‚®ì¶¤ (ë” ë§ì€ í…ìŠ¤íŠ¸ ê°ì§€)
                    det_db_box_thresh=0.3  # ë‚®ì¶¤
                )

            # 3. ê°•í™”ëœ ì „ì²˜ë¦¬
            img_np = np.array(pil_img)
            
            # RGB ë³€í™˜
            if img_np.ndim == 2:
                img_np = cv2.cvtColor(img_np, cv2.COLOR_GRAY2RGB)
            elif img_np.ndim == 3 and img_np.shape[2] == 4:
                img_np = cv2.cvtColor(img_np, cv2.COLOR_RGBA2RGB)

            # Grayscale
            gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)
            
            # Gaussian Blur (ë…¸ì´ì¦ˆ ì œê±°)
            blur = cv2.GaussianBlur(gray, (3, 3), 0)
            
            # Otsu Threshold (ìë™ ì´ì§„í™”)
            _, binary = cv2.threshold(
                blur, 0, 255,
                cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU
            )
            
            # Morphology Close (ì¡ìŒ ì œê±°)
            kernel = np.ones((2, 2), np.uint8)
            cleaned = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
            
            # Sharpen (ì„ ëª…ë„ í–¥ìƒ)
            kernel_sharp = np.array([[-1,-1,-1],
                                    [-1, 9,-1],
                                    [-1,-1,-1]])
            sharpened = cv2.filter2D(cleaned, -1, kernel_sharp)
            
            # RGB ë³€í™˜
            final_img = cv2.cvtColor(sharpened, cv2.COLOR_GRAY2RGB)

            # 4. OCR ì‹¤í–‰
            result = self._ocr_engine.ocr(final_img)

            # 5. ê²°ê³¼ íŒŒì‹±
            texts = self._safe_parse_ocr_result(result)

            return "\n".join(texts), pil_img

        except Exception as e:
            _log(f"âŒ OCR ë Œë” ì‹¤íŒ¨ (page {page_number}): {e}", level="ERROR", exc_info=True)
            return "", None
        
    def _calculate_sample_pages(self, total_pages: int, max_samples: int) -> List[int]:
        """
        ì „ì²´ ë‚´ìš©ì„ ì»¤ë²„í•˜ë„ë¡ í˜ì´ì§€ ìƒ˜í”Œë§
        
        ì „ëµ:
        - ì•ë¶€ë¶„ (ë„ì…/ëª©ì°¨): 6í˜ì´ì§€
        - ë’·ë¶€ë¶„ (ìš”ì•½/ê³¼ì œ): 6í˜ì´ì§€
        - ì¤‘ê°„ë¶€ë¶„ (ë³¸ë¡ ): ê· ë“± ê°„ê²© ìƒ˜í”Œë§
        
        Args:
            total_pages: ì „ì²´ í˜ì´ì§€ ìˆ˜
            max_samples: ìµœëŒ€ ìƒ˜í”Œ í˜ì´ì§€ ìˆ˜
            
        Returns:
            ìƒ˜í”Œë§í•  í˜ì´ì§€ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ (1-based)
        """
        if total_pages <= max_samples:
            # ì „ì²´ í˜ì´ì§€ê°€ ìƒ˜í”Œ ìˆ˜ë³´ë‹¤ ì ìœ¼ë©´ ì „ì²´ ì²˜ë¦¬
            return list(range(1, total_pages + 1))
        
        # ì•/ë’¤ ê° 6í˜ì´ì§€
        head_count = min(6, total_pages)
        tail_count = min(6, total_pages)
        
        head_pages = list(range(1, head_count + 1))
        tail_pages = list(range(max(total_pages - tail_count + 1, head_count + 1), total_pages + 1))
        
        # ì¤‘ê°„ ìƒ˜í”Œ í˜ì´ì§€ ìˆ˜ ê³„ì‚°
        mid_count = max_samples - len(head_pages) - len(tail_pages)
        
        if mid_count > 0:
            # ì¤‘ê°„ ì˜ì—­ ë²”ìœ„
            mid_start = head_count + 1
            mid_end = total_pages - tail_count
            
            if mid_end > mid_start:
                # ê· ë“± ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
                step = (mid_end - mid_start + 1) / (mid_count + 1)
                mid_pages = [
                    int(mid_start + step * (i + 1))
                    for i in range(mid_count)
                ]
                # ì¤‘ë³µ ì œê±°
                mid_pages = [p for p in mid_pages if p not in head_pages and p not in tail_pages]
            else:
                mid_pages = []
        else:
            mid_pages = []
        
        # ì •ë ¬ ë° ì¤‘ë³µ ì œê±°
        all_pages = sorted(set(head_pages + mid_pages + tail_pages))
        
        _log(f"   ğŸ“Š ìƒ˜í”Œë§ ì „ëµ: ì „ì²´ {total_pages}í˜ì´ì§€ â†’ {len(all_pages)}í˜ì´ì§€ ì„ íƒ", level="INFO")
        _log(f"      - ì•ë¶€ë¶„: {head_pages}", level="DEBUG")
        if mid_pages:
            _log(f"      - ì¤‘ê°„ë¶€ë¶„: {mid_pages}", level="DEBUG")
        _log(f"      - ë’·ë¶€ë¶„: {tail_pages}", level="DEBUG")
        
        return all_pages

        
    def _save_debug_image(self, image, pdf_path: str, page_number: int):
        if image is None:
            return

        pdf_name = Path(pdf_path).stem
        debug_dir = Path("/tmp/ocr_debug") / pdf_name
        debug_dir.mkdir(parents=True, exist_ok=True)

        out_path = debug_dir / f"page_{page_number:03d}.png"
        image.save(out_path)

        _log(f"ğŸ§ª OCR DEBUG ì´ë¯¸ì§€ ì €ì¥: {out_path}", level="DEBUG")

    def extract_with_markers(self, pdf_path: str, prefix: str = "MAIN"): 
        """
            PDFì—ì„œ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ + ë§ˆì»¤ ì‚½ì…
            pdfplumber ì‚¬ìš©, í…ìŠ¤íŠ¸ ë¶€ì¡± ì‹œ OCR ìë™ ìˆ˜í–‰
            
            V3 ë³€ê²½ì‚¬í•­:
            - Gemini OCR Fallback ì‹œ ìƒ˜í”Œë§ ì ìš©
            - PaddleOCR ì„±ê³µ ì‹œì—ëŠ” ê¸°ì¡´ëŒ€ë¡œ ì „ì²´ í˜ì´ì§€ ì²˜ë¦¬
            ...
        """
        pages_text = []
        total_pages = 0
        ocr_count = 0
        
        # âœ… V3: Gemini ìƒ˜í”Œë§ ê´€ë ¨ ì¹´ìš´í„° ì´ˆê¸°í™”
        self._gemini_ocr_used_pages = 0
        self._gemini_ocr_skipped_pages = 0
        
        # âœ… V3: 1ë‹¨ê³„ - ì „ì²´ í˜ì´ì§€ ìˆ˜ í™•ì¸ í›„ ìƒ˜í”Œ í˜ì´ì§€ ê²°ì •
        sample_pages = None
        with pdfplumber.open(pdf_path) as pdf:
            total_pages = len(pdf.pages)
            
            # âœ… V3: Gemini Fallbackì´ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ ìƒ˜í”Œ í˜ì´ì§€ ë¯¸ë¦¬ ê³„ì‚°
            if self.gemini_ocr_fallback:
                sample_pages = self._calculate_sample_pages(
                    total_pages, 
                    self.gemini_ocr_max_sample_pages
                )
                _log(f"   ğŸ¯ Gemini OCR Fallback ìƒ˜í”Œë§ í™œì„±í™”: {len(sample_pages)}/{total_pages} í˜ì´ì§€", level="INFO")
        
        # âœ… V3: 2ë‹¨ê³„ - í˜ì´ì§€ë³„ ì²˜ë¦¬
        with pdfplumber.open(pdf_path) as pdf:
            for page_idx, page in enumerate(pdf.pages, start=1):
                text = page.extract_text() or ""
                text_length = len(text.strip())

                _log(
                     f"      page={page_idx} text_len={text_length} ocr_enabled={self.ocr_enabled}",
                     level="DEBUG"
                 )

                if text_length < self.min_text_length and self.ocr_enabled:
                    _log(f"      â†’ í˜ì´ì§€ {page_idx}: OCR ìˆ˜í–‰", level="INFO")

                    ocr_text, pil_img = self._perform_ocr_on_page(pdf_path, page_idx)

                    # ğŸ”¥ OCR ê²°ê³¼ì™€ ë¬´ê´€í•˜ê²Œ ì´ë¯¸ì§€ ì €ì¥
                    self._save_debug_image(
                        pil_img, pdf_path, page_idx
                    )

                    if ocr_text:
                        text = ocr_text
                        ocr_count += 1
                        _log(f"         âœ… OCR ì™„ë£Œ ({len(ocr_text)}ì)", level="INFO")
                    # âœ… V3: PaddleOCR ì‹¤íŒ¨ â†’ Gemini Fallback (ìƒ˜í”Œë§ ì ìš©)
                    else:
                        _log(f"         âš ï¸ PaddleOCR ê²°ê³¼ ì—†ìŒ", level="WARNING")

                        # Gemini Fallback í™œì„±í™” + ì´ë¯¸ì§€ ìˆìŒ
                        if self.gemini_ocr_fallback and pil_img is not None:
                            # âœ… V3: ìƒ˜í”Œ í˜ì´ì§€ì¸ì§€ í™•ì¸
                            if sample_pages and page_idx in sample_pages:
                                try:
                                    buf = io.BytesIO()
                                    pil_img.save(buf, format="PNG")
                                    gem_text, usage = gemini_ocr_image_bytes(
                                            buf.getvalue(),
                                            language_hint="ko",
                                        )
                                    self._gemini_ocr_used_pages += 1

                                    if gem_text and gem_text.strip():
                                        text = gem_text
                                        ocr_count += 1
                                        _log(
                                            f"         âœ… Gemini OCR fallback ì„±ê³µ ({len(gem_text)}ì) "
                                            f"tokens={usage.get('total_tokens','?')}",
                                            level="INFO",
                                        )
                                    else:
                                        _log("         âš ï¸ Gemini OCR fallbackë„ ê²°ê³¼ ì—†ìŒ", level="WARNING")

                                except Exception as e:
                                        _log(f"         âš ï¸ Gemini OCR fallback ì‹¤íŒ¨: {e}", level="WARNING")
                            else:
                                # âœ… V3: ìƒ˜í”Œ í˜ì´ì§€ê°€ ì•„ë‹ˆë©´ ìŠ¤í‚µ
                                self._gemini_ocr_skipped_pages += 1
                                _log(
                                    f"         â­ï¸  Gemini OCR ìƒ˜í”Œë§ ë²”ìœ„ ì™¸ (ìŠ¤í‚µ: {self._gemini_ocr_skipped_pages}í˜ì´ì§€)",
                                    level="DEBUG"
                                )

                # í˜ì´ì§€ ë§ˆì»¤ ë° í…ìŠ¤íŠ¸ ì¶”ê°€
                title = (
                    text.split("\n")[0][:50]
                    if text.strip()
                    else f"Page {page_idx}"
                )

                pages_text.append(f"[{prefix}-PAGE {page_idx}: {title}]")
                pages_text.append(text)
                pages_text.append("")

        if ocr_count:
            _log(f"   âœ… OCR ì²˜ë¦¬ ì™„ë£Œ: {ocr_count} í˜ì´ì§€", level="INFO")
        
        # âœ… V3: Gemini ìƒ˜í”Œë§ í†µê³„ ì¶œë ¥
        if self.gemini_ocr_fallback and self._gemini_ocr_used_pages > 0:
            _log(
                f"   ğŸ’° Gemini OCR Fallback ì‚¬ìš©: {self._gemini_ocr_used_pages}í˜ì´ì§€ "
                f"(ìŠ¤í‚µ: {self._gemini_ocr_skipped_pages}í˜ì´ì§€)",
                level="INFO"
            )

        return {
            "full_text": "\n".join(pages_text),
            "total_pages": total_pages,
            "gemini_fallback_used": self._gemini_ocr_used_pages > 0,
        }

class ImageDescriptionGenerator:
    """í†µê³¼ëœ ì´ë¯¸ì§€ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª… ìƒì„± (2-4ë¬¸ì¥)"""
    
    
    def __init__(self):
        """ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±ê¸° ì´ˆê¸°í™”"""
        self.total_tokens = 0  # âœ… ëˆ„ì  í† í° ìˆ˜
        self.description_count = 0  # ìƒì„±í•œ ì„¤ëª… ê°œìˆ˜
        
        # âœ… Gemini ëª¨ë¸ ì´ˆê¸°í™”
        from .improved_hybrid_filter import get_global_model
        self.model = get_global_model()
        
        if self.model is None:
            print("      âš ï¸  Warning: Gemini ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨ - ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ë¶ˆê°€", level="WARNING")
    def generate_description(
        self, 
        image_bytes: bytes, 
        adjacent_text: str,
        keywords: List[str],
        max_retries=3
    ) -> str:
        """
        Vision APIë¡œ ì´ë¯¸ì§€ ìƒì„¸ ì„¤ëª… ìƒì„±
        ì¬ì‹œë„ ë¡œì§ í¬í•¨ (429 Rate Limit ëŒ€ì‘)
        """
        import time
        
        for attempt in range(max_retries):
            try:
                mime_type = self._get_mime_type(image_bytes)
                image_part = Part.from_data(data=image_bytes, mime_type=mime_type)
                
                keyword_context = ', '.join(keywords[:10]) if keywords else "ì¼ë°˜ í•™ìŠµ ë‚´ìš©"
                
                prompt = f"""
ì´ ì´ë¯¸ì§€ë¥¼ 2-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.

ê°•ì˜ ì£¼ì œ: {keyword_context}
ì£¼ë³€ í…ìŠ¤íŠ¸: "{adjacent_text}"

ì„¤ëª…ì— í¬í•¨í•  ë‚´ìš©:
1. ì´ë¯¸ì§€ê°€ ë‚˜íƒ€ë‚´ëŠ” ì£¼ì œ/ê°œë… (1ë¬¸ì¥)
2. ì£¼ìš” êµ¬ì„± ìš”ì†Œ 2-3ê°œ (1-2ë¬¸ì¥)
3. í•µì‹¬ ì •ë³´ë‚˜ íŒ¨í„´ (1ë¬¸ì¥)

ì œì™¸í•  ë‚´ìš©:
- ì„¸ë¶€ ìš”ì†Œ ì „ì²´ ë‚˜ì—´
- ë¶ˆí•„ìš”í•œ ì¶”ì¸¡ì´ë‚˜ í•´ì„

ì¶œë ¥: ëª…í™•í•˜ê³  ê°„ê²°í•œ 2-4ë¬¸ì¥ë§Œ.
"""
                # âœ… Vertex AI model ê°€ì ¸ì˜¤ê¸°
                model = get_global_model()
                if model is None:
                    return "ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: Vertex AI model not initialized"

                response = model.generate_content([image_part, prompt])
                description = response.text.strip()
                
                # âœ… í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  (usage_metadata ìš°ì„ )
                tokens_added = 0
                try:
                    # Method 1: response.usage_metadata (ê°€ì¥ ì •í™•)
                    if hasattr(response, 'usage_metadata') and response.usage_metadata:
                        tokens_added = getattr(response.usage_metadata, 'total_token_count', 0)
                        if tokens_added > 0:
                            self.total_tokens += tokens_added
                            self.description_count += 1
                except Exception:
                    pass
                
                return description
                
            except Exception as e:
                error_msg = str(e)
                
                if "429" in error_msg or "Resource exhausted" in error_msg:
                    if attempt < max_retries - 1:
                        wait_time = (attempt + 1) * 3
                        _log(f"      âš ï¸  Rate Limit, {wait_time}ì´ˆ ëŒ€ê¸° ì¤‘...", level="WARNING", end='', flush=True)
                        time.sleep(wait_time)
                        _log(" ì¬ì‹œë„", level="WARNING")
                        continue
                    else:
                        return "ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: API rate limit exceeded"
                else:
                    return f"ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {error_msg}"
        
        return "ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: Failed after all retries"
    
    def _get_mime_type(self, image_bytes: bytes) -> str:
        """ì´ë¯¸ì§€ ë°”ì´ë„ˆë¦¬ì—ì„œ MIME íƒ€ì… ê°ì§€"""
        if image_bytes.startswith(b'\xff\xd8'):
            return "image/jpeg"
        elif image_bytes.startswith(b'\x89PNG\r\n\x1a\n'):
            return "image/png"
        elif image_bytes.startswith(b'GIF87a') or image_bytes.startswith(b'GIF89a'):
            return "image/gif"
        elif image_bytes.startswith(b'RIFF') and image_bytes[8:12] == b'WEBP':
            return "image/webp"
        return "image/png"


class MetadataGenerator:
    """
    ë©”íƒ€ë°ì´í„° ìƒì„± ë…¸ë“œ
    
    ì£¼ê°•ì˜ìë£Œ + ë³´ì¡°ìë£Œ â†’ metadata.json
    """
    
    def __init__(self):
        self.converter = None
        self.text_extractor = TextExtractor()
        self.image_filter = ImprovedHybridFilterPipeline(auto_extract_keywords=True)
        self.image_describer = ImageDescriptionGenerator()
        self.debug = True  # ğŸ”§ DEBUG í•­ìƒ ì¼œê¸° (ì›ì¸ íŒŒì•…ìš©)
            
    def _extract_page_title(self, slide_title: str, adjacent_text: str) -> str:
        """ì˜ë¯¸ìˆëŠ” í˜ì´ì§€ ì œëª© ì¶”ì¶œ"""
        if slide_title and slide_title.strip() and slide_title.lower() != "no title":
            return slide_title.strip()[:50]
        
        if adjacent_text:
            lines = adjacent_text.strip().split('\n')
            for line in lines:
                line = line.strip()
                if len(line) > 3 and not line.startswith('â˜'):
                    return line[:50]
        
        return "í˜ì´ì§€ ì œëª© ì—†ìŒ"
    
    def generate(
        self,
        primary_file: str,
        supplementary_files: Optional[List[str]] = None,
        output_path: str = "output/metadata.json"
    ) -> str:
        """ë©”íƒ€ë°ì´í„° ìƒì„±"""
        _log(f"\n{'='*120}")
        _log(f"ğŸ¯ ë©”íƒ€ë°ì´í„° ìƒì„± ì‹œì‘", level="INFO")
        _log(f"{'='*120}")
        _log(f"ì£¼ê°•ì˜ìë£Œ: {primary_file}", level="INFO")
        if supplementary_files:
            _log(f"ë³´ì¡°ìë£Œ: {len(supplementary_files)}ê°œ")
            for i, supp in enumerate(supplementary_files, 1):
                _log(f"  {i}. {supp}")
        _log(f"{'='*120}\n")
        
        with tempfile.TemporaryDirectory() as temp_dir:
            self.converter = DocumentConverterNode(output_dir=temp_dir)
            
            _log("ğŸ“„ [1/3] ì£¼ê°•ì˜ìë£Œ ì²˜ë¦¬ ì¤‘...", level="INFO")
            primary_metadata = self._process_primary_source(primary_file)
            
            _log("\nğŸ“š [2/3] ë³´ì¡°ìë£Œ ì²˜ë¦¬ ì¤‘...", level="INFO")
            supplementary_metadata = []
            if supplementary_files:
                for i, supp_file in enumerate(supplementary_files[:3], 1):
                    try:
                        supp_meta = self._process_supplementary_source(supp_file, i)
                        supplementary_metadata.append(supp_meta)
                        _log(f"   âœ… ë³´ì¡°ìë£Œ {i} ì²˜ë¦¬ ì„±ê³µ", level="INFO")
                    except Exception as e:
                        _log(f"   âš ï¸ ë³´ì¡°ìë£Œ {i} ì²˜ë¦¬ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}", level="WARNING", exc_info=True)
            else:
                _log("   âš ï¸  ë³´ì¡°ìë£Œ ì—†ìŒ (ì„ íƒ ì‚¬í•­)", level="INFO")
            
            _log("\nğŸ”§ [3/3] ë©”íƒ€ë°ì´í„° í†µí•© ì¤‘...", level="INFO")
            
            # âœ… Vision í† í° í†µê³„ ìˆ˜ì§‘
            vision_tokens = {}
            if hasattr(self.image_filter, 'vision_tokens'):
                vision_tokens = self.image_filter.vision_tokens.copy()
                _log(f"   image_filter.vision_tokens = {vision_tokens}", level="DEBUG")
            
            # âœ… ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± í† í° ì¶”ê°€
            _log(f"   image_describer.total_tokens = {self.image_describer.total_tokens}", level="DEBUG")
            _log(f"   image_describer.description_count = {self.image_describer.description_count}", level="DEBUG")
            
            if self.image_describer.total_tokens > 0:
                vision_tokens['image_description'] = self.image_describer.total_tokens
                vision_tokens['description_count'] = self.image_describer.description_count
                vision_tokens['total'] = vision_tokens.get('total', 0) + self.image_describer.total_tokens
                _log(f"   vision_tokens after adding image_description = {vision_tokens}", level="DEBUG")
            
            # âœ… ë¹„ìš© ê³„ì‚°
            if vision_tokens.get('total', 0) > 0:
                from .pricing import calculate_vision_cost, format_cost
                vision_cost = calculate_vision_cost(vision_tokens['total'])
                vision_tokens['cost_usd'] = vision_cost
            
            metadata = {
                "metadata_version": "1.0",
                "created_at": datetime.now().isoformat(),
                "primary_source": primary_metadata,
                "supplementary_sources": supplementary_metadata
            }
            
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            _log(f"\n{'='*120}")
            _log(f"âœ… ë©”íƒ€ë°ì´í„° ìƒì„± ì™„ë£Œ!", level="INFO")
            _log(f"{'='*120}")
            _log(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_path}")
            _log(f"ğŸ“Š ì£¼ê°•ì˜ìë£Œ í˜ì´ì§€: {primary_metadata['total_pages']}ê°œ")
            _log(f"ğŸ–¼ï¸  í•„í„°ë§ëœ ì´ë¯¸ì§€: {len(primary_metadata['filtered_images'])}ê°œ")
            if supplementary_metadata:
                total_supp_pages = sum(s['total_pages'] for s in supplementary_metadata)
                _log(f"ğŸ“š ë³´ì¡°ìë£Œ í˜ì´ì§€: {total_supp_pages}ê°œ", level="INFO")
            
            # âœ… Vision í† í° í†µê³„ ì¶œë ¥
            if vision_tokens:
                _log(f"\nğŸ’° Vision API ì‚¬ìš© í†µê³„:", level="INFO")
                if 'keyword_extraction' in vision_tokens:
                    _log(f"   ğŸ“ í‚¤ì›Œë“œ ì¶”ì¶œ: {vision_tokens['keyword_extraction']:,} tokens", level="INFO")
                if 'image_filtering' in vision_tokens:
                    _log(f"   ğŸ” ì´ë¯¸ì§€ í•„í„°ë§: {vision_tokens['image_filtering']:,} tokens", level="INFO")
                if 'image_description' in vision_tokens:
                    _log(f"   ğŸ“¸ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±: {vision_tokens['image_description']:,} tokens ({vision_tokens['description_count']}ê°œ)", level="INFO")
                if 'total' in vision_tokens:
                    _log(f"   ğŸ“Š Total: {vision_tokens['total']:,} tokens", level="INFO")
                if 'cost_usd' in vision_tokens:
                    _log(f"   ğŸ’µ ë¹„ìš©: {format_cost(vision_tokens['cost_usd'])}", level="INFO")
            
            print(f"{'='*120}\n")
            
            # âœ… vision_tokensì™€ í•¨ê»˜ ë°˜í™˜
            return {
                "metadata_path": str(output_path),
                "vision_tokens": vision_tokens
            }
    
    def _process_primary_source(self, file_path: str) -> Dict[str, Any]:
        """
        ì£¼ê°•ì˜ìë£Œ ì²˜ë¦¬
        âœ… TXT/URL ì§€ì› ì¶”ê°€
        âœ… PPTX ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ (PDF ë³€í™˜ ì—†ì´)
        """
        file_path_str = str(file_path)
        
        # ì›ë³¸ íŒŒì¼ íƒ€ì… ê°ì§€
        if file_path_str.startswith(('http://', 'https://')):
            original_file_type = 'url'
            file_path_obj = None
            display_name = file_path_str[:50]
        else:
            file_path_obj = Path(file_path)
            original_file_type = file_path_obj.suffix.lower().replace('.', '')
            display_name = file_path_obj.name
        
        _log(f"   ğŸ“„ íŒŒì¼: {display_name} ({original_file_type})", level="INFO")
        
        # âœ… PPTXëŠ” ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ (PDF ë³€í™˜ ì‹œ í•œê¸€ ê¹¨ì§ ë°©ì§€)
        if original_file_type == 'pptx':
            _log(f"   ğŸ“ PPTX ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘... (PDF ë³€í™˜ ê±´ë„ˆëœ€)", level="INFO")
            from pptx import Presentation
            
            prs = Presentation(file_path_str)
            pages_text = []
            total_pages = 0
            
            for slide_num, slide in enumerate(prs.slides, 1):
                total_pages += 1
                
                # ìŠ¬ë¼ì´ë“œ ì œëª© ì¶”ì¶œ
                title = "No Title"
                if slide.shapes.title and slide.shapes.title.text.strip():
                    title = slide.shapes.title.text.strip()[:50]
                
                # í˜ì´ì§€ ë§ˆì»¤
                pages_text.append(f"[MAIN-PAGE {slide_num}: {title}]")
                
                # ìŠ¬ë¼ì´ë“œ ë‚´ìš© ì¶”ì¶œ
                for shape in slide.shapes:
                    if hasattr(shape, "text") and shape.text.strip():
                        pages_text.append(shape.text.strip())
                
                pages_text.append("")  # ìŠ¬ë¼ì´ë“œ êµ¬ë¶„
            
            full_text = "\n".join(pages_text)
            _log(f"   âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(full_text)}ì, {total_pages}í˜ì´ì§€", level="INFO")
            
            # ì´ë¯¸ì§€ëŠ” PPTX ì›ë³¸ì—ì„œ ì¶”ì¶œ
            _log(f"   ğŸ–¼ï¸  ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘...", level="INFO")
            _log(f"      â†’ PPTX ì›ë³¸ì—ì„œ ì§ì ‘ ì¶”ì¶œ", level="INFO")
            self.image_filter.extract_keywords_from_document(file_path_str)
            keywords = self.image_filter.document_keywords
            all_images = self._extract_images_from_pptx(file_path_str)
            
        else:
            # ê¸°ì¡´ ë°©ì‹: PDF ë³€í™˜
            _log(f"   ğŸ”„ íŒŒì¼ ì²˜ë¦¬ ì¤‘...", level="INFO")
            processed_path = self.converter.convert(file_path_str)
            
            # 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ
            _log(f"   ğŸ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...", level="INFO")
            text_data = self.text_extractor.extract_with_markers(processed_path, prefix="MAIN")
            full_text = text_data['full_text']
            total_pages = text_data['total_pages']
            _log(f"   âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(full_text)}ì", level="INFO")
            
            # 3. ì´ë¯¸ì§€ í•„í„°ë§
            # 3. ì´ë¯¸ì§€ í•„í„°ë§
            _log(f"   ğŸ–¼ï¸  ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘...", level="INFO")
            
            # TXT/URLì€ ì´ë¯¸ì§€ ì—†ìŒ
            if original_file_type in ['txt', 'url']:
                _log(f"      â†’ TXT/URLì€ ì´ë¯¸ì§€ ì—†ìŒ, ê±´ë„ˆë›°ê¸°", level="INFO")
                all_images = []
                keywords = []
            
            elif original_file_type in ['docx', 'pdf']:
                _log(f"      â†’ PDFì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ", level="INFO")
                self.image_filter.extract_keywords_from_document(processed_path)
                keywords = self.image_filter.document_keywords
                extractor = UniversalImageExtractor()
                
                # âœ… Gemini Fallback ì‚¬ìš© ì—¬ë¶€ ì „ë‹¬
                gemini_used = text_data.get('gemini_fallback_used', False)
                all_images = extractor.extract(processed_path, skip_ocr=gemini_used)
            
            else:
                _log(f"   âš ï¸  ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {original_file_type}", level="WARNING")
                all_images = []
                keywords = []
        
        # 4. í•„í„°ë§ ì‹¤í–‰ (ê³µí†µ)
        filtered_images = []
        if all_images:
            _log(f"   ğŸ” {len(all_images)}ê°œ ì´ë¯¸ì§€ ë°œê²¬, í•„í„°ë§ ì‹œì‘...")

            for img_meta in all_images:
                decision, reason = self.image_filter.step1_rule_check(img_meta)
                
                if decision == "INCLUDE":
                    img_meta.is_core_content = True
                    img_meta.filter_reason = reason
                    filtered_images.append(img_meta)
                    
                elif decision == "PENDING":
                    ai_result = self.image_filter.step2_gemini_check(img_meta)

                    # íŠœí”Œ ë°˜í™˜ ëŒ€ì‘
                    if isinstance(ai_result, tuple):
                        ai_result = ai_result[0]

                    if ai_result.upper().startswith("KEEP"):
                        img_meta.is_core_content = True
                        img_meta.filter_reason = ai_result
                        filtered_images.append(img_meta)
            
            _log(f"   âœ… í•„í„°ë§ ì™„ë£Œ: {len(filtered_images)}ê°œ ì„ íƒ")
        
        # 5. ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±
        filtered_image_metadata = []
        
        if filtered_images:
            _log(f"   ğŸ“ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì¤‘... (0/{len(filtered_images)})", end='', flush=True)
            
            # âœ… ì´ì „ í† í° ìˆ˜ ì €ì¥ (ê° ì´ë¯¸ì§€ë‹¹ í† í° ì¶”ì ìš©)
            prev_tokens = self.image_describer.total_tokens
            
            for i, img_meta in enumerate(filtered_images, 1):
                description = self.image_describer.generate_description(
                    img_meta.image_bytes,
                    img_meta.adjacent_text,
                    keywords
                )
                
                # âœ… ì´ë²ˆ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±ì— ì‚¬ìš©ëœ í† í° ê³„ì‚°
                current_tokens = self.image_describer.total_tokens - prev_tokens
                
                page_title = self._extract_page_title(
                    img_meta.slide_title,
                    img_meta.adjacent_text
                )
                
                filtered_image_metadata.append({
                    "image_id": img_meta.image_id.replace("S", "MAIN_P").replace("P", "MAIN_P"),
                    "page_number": img_meta.slide_number,
                    "page_title": page_title,
                    "description": description,
                    "filter_stage": "1ì°¨ (Rule)" if "Rule" in img_meta.filter_reason else "2ì°¨ (AI)",
                    "area_percentage": img_meta.area_percentage
                })
                
                # âœ… ì§„í–‰ ìƒí™©ê³¼ í•¨ê»˜ í† í° ì •ë³´ ì¶œë ¥
                if current_tokens > 0:
                    _log(f"\r   ğŸ“ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì¤‘... ({i}/{len(filtered_images)}) - #{i}: {current_tokens:,} tokens", level="INFO", end='', flush=True)
                else:
                    _log(f"\r   ğŸ“ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì¤‘... ({i}/{len(filtered_images)})", level="INFO", end='', flush=True)
                
                # ë‹¤ìŒ ì´ë¯¸ì§€ë¥¼ ìœ„í•´ prev_tokens ì—…ë°ì´íŠ¸
                prev_tokens = self.image_describer.total_tokens
            
            _log(f"\n   {'='*80}", level="INFO")
            _log(f"   ğŸ“Š ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì™„ë£Œ", level="INFO")
            _log(f"      - ì²˜ë¦¬ëœ ì´ë¯¸ì§€: {len(filtered_images)}ê°œ", level="INFO")
            # âœ… ì´ í† í° ìˆ˜ ì¶œë ¥            
            if self.image_describer.total_tokens > 0:
                avg_tokens = self.image_describer.total_tokens / len(filtered_images) if len(filtered_images) > 0 else 0
                _log(f"      - ì´ í† í°: {self.image_describer.total_tokens:,} tokens", level="INFO")
                _log(f"      - í‰ê· : {avg_tokens:.0f} tokens/image", level="INFO")
            else:
                _log(f"      âš ï¸  í† í° ì •ë³´ ì—†ìŒ (usage_metadata ë¯¸ì§€ì› ê°€ëŠ¥ì„±)", level="WARNING")
            _log(f"   {'='*80}\n", level="INFO")

        # 6. í†µê³„
        total_images = len(all_images)
        passed_images = len(filtered_images)
        
        return {
            "role": "main",
            "filename": display_name if original_file_type == 'url' else (file_path_obj.name if file_path_obj else display_name),
            "file_type": original_file_type,
            "total_pages": total_pages,
            "content": {
                "full_text": full_text
            },
            "filtered_images": filtered_image_metadata,
            "statistics": {
                "total_images_found": total_images,
                "images_passed": passed_images,
                "filter_rate": passed_images / total_images if total_images > 0 else 0
            }
        }
    
    def _process_supplementary_source(self, file_path: str, order: int) -> Dict[str, Any]:
        """
        ë³´ì¡°ìë£Œ ì²˜ë¦¬
        âœ… PPTX ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ (PDF ë³€í™˜ ì—†ì´)
        """
        file_path_str = str(file_path)
        
        # URLê³¼ íŒŒì¼ êµ¬ë¶„
        if file_path_str.startswith(('http://', 'https://')):
            file_type = 'url'
            display_name = 'Web Content'
            file_path_obj = None
        else:
            file_path_obj = Path(file_path)
            file_type = file_path_obj.suffix.lower().replace('.', '')
            display_name = file_path_obj.name
        
        _log(f"   ğŸ“š ë³´ì¡°ìë£Œ {order}: {display_name} ({file_type})")
        
        # âœ… PPTXëŠ” ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ (PDF ë³€í™˜ ê±´ë„ˆëœ€)
        if file_type == 'pptx':
            print(f"      ğŸ“ PPTX ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘... (PDF ë³€í™˜ ê±´ë„ˆëœ€)")
            from pptx import Presentation
            
            prs = Presentation(file_path_str)
            pages_text = []
            total_pages = 0
            
            for slide_num, slide in enumerate(prs.slides, 1):
                total_pages += 1
                
                # ìŠ¬ë¼ì´ë“œ ì œëª© ì¶”ì¶œ
                title = "No Title"
                if slide.shapes.title and slide.shapes.title.text.strip():
                    title = slide.shapes.title.text.strip()[:50]
                
                # í˜ì´ì§€ ë§ˆì»¤
                pages_text.append(f"[SUPP{order}-PAGE {slide_num}: {title}]")
                
                # ìŠ¬ë¼ì´ë“œ ë‚´ìš© ì¶”ì¶œ
                for shape in slide.shapes:
                    if hasattr(shape, "text") and shape.text.strip():
                        pages_text.append(shape.text.strip())
                
                pages_text.append("")  # ìŠ¬ë¼ì´ë“œ êµ¬ë¶„
            
            full_text = "\n".join(pages_text)
            print(f"      âœ… ì™„ë£Œ ({total_pages}í˜ì´ì§€)")
            
        else:
            # ê¸°ì¡´ ë°©ì‹: PDF ë³€í™˜
            print(f"      ğŸ”„ PDF ë³€í™˜ ì¤‘...")
            pdf_path = self.converter.convert(file_path_str)
            
            print(f"      ğŸ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
            text_data = self.text_extractor.extract_with_markers(pdf_path, prefix=f"SUPP{order}")
            
            full_text = text_data['full_text']
            total_pages = text_data['total_pages']
            
            print(f"      âœ… ì™„ë£Œ ({total_pages}í˜ì´ì§€)")
        
        return {
            "order": order,
            "filename": display_name,
            "file_type": file_type,
            "total_pages": total_pages,
            "content": {
                "full_text": full_text
            }
        }
    
    def _extract_images_from_pptx(self, pptx_path: str) -> List[ImageMetadata]:
        """PPTXì—ì„œ ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        extractor = UniversalImageExtractor()
        return extractor.extract(pptx_path)


# CLI ì¸í„°í˜ì´ìŠ¤
if __name__ == "__main__":
    import sys
    
    _log("\n" + "="*120)
    _log("ğŸ¯ Metadata Generator Node (V2 - pdfplumber)")
    _log("="*120)
    
    if len(sys.argv) < 2:
        _log("\nì‚¬ìš©ë²•:")
        _log("  python metadata_generator_node.py <ì£¼ê°•ì˜ìë£Œ> [ë³´ì¡°1] [ë³´ì¡°2] [ë³´ì¡°3]")
        _log("\nì˜ˆì‹œ:")
        _log("  python metadata_generator_node.py ì¤‘ë“±êµ­ì–´1.pptx")
        _log("  python metadata_generator_node.py notes.txt")
        _log("  python metadata_generator_node.py https://example.com/article")
        _log("\nâœ… ì§€ì› í˜•ì‹: PPTX, DOCX, PDF, TXT, URL")
        _log("="*120 + "\n")
        sys.exit(1)
    
    primary_file = sys.argv[1]
    supplementary_files = sys.argv[2:5] if len(sys.argv) > 2 else None
    
    if not primary_file.startswith('http') and not os.path.exists(primary_file):
        _log(f"\nâŒ ì£¼ê°•ì˜ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {primary_file}")
        sys.exit(1)
    
    if supplementary_files:
        for supp in supplementary_files:
            if not supp.startswith('http') and not os.path.exists(supp):
                _log(f"\nâŒ ë³´ì¡°ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {supp}")
                sys.exit(1)
    
    try:
        generator = MetadataGenerator()
        output_path = generator.generate(
            primary_file=primary_file,
            supplementary_files=supplementary_files,
            output_path="output/metadata.json"
        )
        
        _log(f"âœ… ì„±ê³µ!")
        _log(f"ğŸ“ {output_path}")
        
    except Exception as e:
        _log(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)