"""
Metadata Generator Node (V2 - pdfplumber ì „í™˜)
===============================================

ë³€ê²½ì‚¬í•­:
- PyMuPDF ì™„ì „ ì œê±°
- pdfplumber + OCR (pypdfium2 + PaddleOCR)ë¡œ í†µí•©
- improved_hybrid_filter.py V3ì™€ ì™„ì „ í˜¸í™˜

ì…ë ¥:
- primary_file: ì£¼ê°•ì˜ìë£Œ (1ê°œ, í•„ìˆ˜)
- supplementary_files: ë³´ì¡°ìë£Œ (0~3ê°œ, ì„ íƒ)

ì¶œë ¥:
- metadata.json (ì´ë¯¸ì§€ ì„¤ëª… í¬í•¨)

í†µí•©:
- DocumentConverterNode: PDF ë³€í™˜ + TXT/URL ì²˜ë¦¬
- ImprovedHybridFilterPipeline: ì´ë¯¸ì§€ í•„í„°ë§
- TextExtractor: í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
- ImageDescriptionGenerator: ì´ë¯¸ì§€ ìƒì„¸ ì„¤ëª…

"""

import os
import json
import tempfile
from pathlib import Path
from typing import List, Optional, Dict, Any
from datetime import datetime
from pathlib import Path
import traceback
import cv2
import numpy as np
import logging
import sys

logger = logging.getLogger(__name__)

def _log(*args, level: str | None = None, exc_info: bool = False, end: str = '\n', flush: bool = False) -> None:
    """
    logger ê¸°ë°˜ ë¡œê·¸ (í™˜ê²½ë³„ LOG_LEVEL ì ìš©).
    - ê¸°ë³¸ level: DEBUG
    - end/flushë¥¼ ì“°ëŠ” ì§„í–‰í˜• ì¶œë ¥(end != '\\n' ë˜ëŠ” flush=True)ì€ ê¸°ì¡´ì²˜ëŸ¼ print ìœ ì§€
      (ë¡œê¹…ìœ¼ë¡œ ë°”ê¾¸ë©´ ì¤„ë°”ê¿ˆ/ë²„í¼ë§ ë™ì‘ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ)
    """
    msg = " ".join(str(a) for a in args).rstrip() if args else ""

    # ì§„í–‰í˜• ì¶œë ¥ì€ ê·¸ëŒ€ë¡œ stdoutë¡œ (ê¸°ì¡´ UX ìœ ì§€)
    if end != "\n" or flush:
        print(msg, end=end, flush=flush)
        return

    lvl = (level or "DEBUG").upper()
    if lvl == "DEBUG":
        logger.debug(msg, exc_info=exc_info)
    elif lvl == "INFO":
        logger.info(msg, exc_info=exc_info)
    elif lvl in ("WARN", "WARNING"):
        logger.warning(msg, exc_info=exc_info)
    elif lvl == "ERROR":
        logger.error(msg, exc_info=exc_info)
    elif lvl in ("CRITICAL", "FATAL"):
        logger.critical(msg, exc_info=exc_info)
    else:
        logger.debug(msg, exc_info=exc_info)

# OCR ë¡œê·¸ ì–µì œ
os.environ['FLAGS_log_level'] = '3'
os.environ['PPOCR_SHOW_LOG'] = 'False'

# pdfplumber (í•„ìˆ˜)
try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    _log("âŒ pdfplumberê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    _log("   pip install pdfplumber")
    PDFPLUMBER_AVAILABLE = False

# OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„ íƒ) - pypdfium2 ì‚¬ìš©
OCR_AVAILABLE = False
ocr_engine = None

try:
    from paddleocr import PaddleOCR
    import numpy as np
    from pypdfium2 import PdfDocument
    from PIL import Image

    # ì—¬ê¸°ê¹Œì§€ ì„±ê³µí•˜ë©´ "ì˜ì¡´ì„±"ì€ OK
    try:
        ocr_engine = PaddleOCR(
            lang='korean',
            use_angle_cls=True
        )
        OCR_AVAILABLE = True
        _log("âœ… OCR ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ (PaddleOCR + pypdfium2)")
    except Exception as e:
        # ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨ëŠ” ì˜ì¡´ì„± ë¬¸ì œì™€ ë¶„ë¦¬í•´ì„œ í‘œì‹œ
        _log(f"âš ï¸  OCR ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨(ì—”ì§„): {e}")

except ImportError as e:
    # ì˜ì¡´ì„± import ì‹¤íŒ¨ë§Œ ì—¬ê¸°ë¡œ ì˜´
    _log(f"âš ï¸  OCR ì˜ì¡´ì„± ë¯¸ì„¤ì¹˜(ì„ íƒ): {e}")


# ê¸°ì¡´ ë…¸ë“œ ì„í¬íŠ¸
from .document_converter_node import DocumentConverterNode, DocumentType
from .improved_hybrid_filter import (
    ImprovedHybridFilterPipeline,
    UniversalImageExtractor,
    ImageMetadata,
    get_global_model
)

from vertexai.generative_models import Part

class TextExtractor:
    """
    PDFì—ì„œ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ + ë§ˆì»¤ ì‚½ì…
    V2: pdfplumber + OCR í†µí•©
    """

    def __init__(self):
        if not PDFPLUMBER_AVAILABLE:
            raise ImportError("pdfplumberê°€ í•„ìš”í•©ë‹ˆë‹¤")

        self.ocr_enabled = OCR_AVAILABLE
        self.min_text_length = 100
        self._ocr_engine = None

    def _safe_parse_ocr_result(self, result):
        """
        PaddleOCR ê²°ê³¼ë¥¼ ì•ˆì „í•˜ê²Œ íŒŒì‹±
        ë‹¤ì–‘í•œ ê²°ê³¼ í¬ë§· ëŒ€ì‘
        
        Args:
            result: PaddleOCR ê²°ê³¼ (ë‹¤ì–‘í•œ í˜•íƒœ ê°€ëŠ¥)
        
        Returns:
            List[str]: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        texts = []
        
        # None ì²´í¬
        if not result:
            return texts
        
        # resultê°€ listì¸ì§€ í™•ì¸
        if not isinstance(result, list):
            return texts
        
        # result[0] ì¶”ì¶œ (PaddleOCRëŠ” ë³´í†µ [[...]] í˜•íƒœ)
        try:
            items = result[0] if result and isinstance(result[0], list) else result
        except (IndexError, TypeError):
            return texts
        
        # ê° item íŒŒì‹±
        for item in items:
            try:
                # Case 1: [[bbox], ("text", score)] í˜•íƒœ
                if isinstance(item, (list, tuple)) and len(item) >= 2:
                    # item[1]ì´ tuple/listì¸ì§€ í™•ì¸
                    if isinstance(item[1], (list, tuple)) and len(item[1]) > 0:
                        text = str(item[1][0]).strip()
                        if text and len(text) > 1:  # 1ê¸€ì ì¡ìŒ ì œê±°
                            texts.append(text)
                    # item[1]ì´ stringì¸ ê²½ìš°
                    elif isinstance(item[1], str):
                        text = item[1].strip()
                        if text and len(text) > 1:
                            texts.append(text)
                
                # Case 2: {"text": "...", "score": ...} í˜•íƒœ (dict)
                elif isinstance(item, dict):
                    text = item.get("text", "").strip()
                    if text and len(text) > 1:
                        texts.append(text)
                
                # Case 3: ë‹¨ìˆœ string (ë“œë¬¼ì§€ë§Œ ëŒ€ì‘)
                elif isinstance(item, str):
                    text = item.strip()
                    if text and len(text) > 1:
                        texts.append(text)
                        
            except Exception as e:
                # ê°œë³„ item íŒŒì‹± ì‹¤íŒ¨ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì†
                _log(f"âš ï¸ OCR ê²°ê³¼ íŒŒì‹± ìŠ¤í‚µ: {e}")
                continue
        
        return texts
    
    def _perform_ocr_on_page(self, pdf_path: str, page_number: int):
        """
        í˜ì´ì§€ì— OCR ìˆ˜í–‰ (pypdfium2 + PaddleOCR)
        """
        try:
            pdf = PdfDocument(pdf_path)
            page = pdf[page_number - 1]

            # 1. PDF â†’ ì´ë¯¸ì§€
            bitmap = page.render(scale=3.0)  # ìŠ¤ì¼€ì¼ ì˜¬ë¦¼
            pil_img = bitmap.to_pil()

            # 2. OCR ì—”ì§„ ì´ˆê¸°í™”
            if self._ocr_engine is None:
                self._ocr_engine = PaddleOCR(
                    lang="korean",
                    use_angle_cls=True,
                    det_db_thresh=0.1,  # ë‚®ì¶¤ (ë” ë§ì€ í…ìŠ¤íŠ¸ ê°ì§€)
                    det_db_box_thresh=0.3  # ë‚®ì¶¤
                )

            # 3. ê°•í™”ëœ ì „ì²˜ë¦¬
            img_np = np.array(pil_img)
            
            # RGB ë³€í™˜
            if img_np.ndim == 2:
                img_np = cv2.cvtColor(img_np, cv2.COLOR_GRAY2RGB)
            elif img_np.ndim == 3 and img_np.shape[2] == 4:
                img_np = cv2.cvtColor(img_np, cv2.COLOR_RGBA2RGB)

            # Grayscale
            gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)
            
            # Gaussian Blur (ë…¸ì´ì¦ˆ ì œê±°)
            blur = cv2.GaussianBlur(gray, (3, 3), 0)
            
            # Otsu Threshold (ìë™ ì´ì§„í™”)
            _, binary = cv2.threshold(
                blur, 0, 255,
                cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU
            )
            
            # Morphology Close (ì¡ìŒ ì œê±°)
            kernel = np.ones((2, 2), np.uint8)
            cleaned = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
            
            # Sharpen (ì„ ëª…ë„ í–¥ìƒ)
            kernel_sharp = np.array([[-1,-1,-1],
                                    [-1, 9,-1],
                                    [-1,-1,-1]])
            sharpened = cv2.filter2D(cleaned, -1, kernel_sharp)
            
            # RGB ë³€í™˜
            final_img = cv2.cvtColor(sharpened, cv2.COLOR_GRAY2RGB)

            # 4. OCR ì‹¤í–‰
            result = self._ocr_engine.ocr(final_img)

            # 5. ê²°ê³¼ íŒŒì‹±
            texts = self._safe_parse_ocr_result(result)

            return "\n".join(texts), pil_img

        except Exception as e:
            _log(f"âŒ OCR ë Œë” ì‹¤íŒ¨ (page {page_number}): {e}")
            return "", None

        
    def _save_debug_image(self, image, pdf_path: str, page_number: int):
        if image is None:
            return

        pdf_name = Path(pdf_path).stem
        debug_dir = Path("/tmp/ocr_debug") / pdf_name
        debug_dir.mkdir(parents=True, exist_ok=True)

        out_path = debug_dir / f"page_{page_number:03d}.png"
        image.save(out_path)

        _log(f"ğŸ§ª OCR DEBUG ì´ë¯¸ì§€ ì €ì¥: {out_path}")

    def extract_with_markers(self, pdf_path: str, prefix: str = "MAIN"): 
        """
            PDFì—ì„œ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ + ë§ˆì»¤ ì‚½ì…
            pdfplumber ì‚¬ìš©, í…ìŠ¤íŠ¸ ë¶€ì¡± ì‹œ OCR ìë™ ìˆ˜í–‰
            
            Args:
                pdf_path: PDF íŒŒì¼ ê²½ë¡œ
                prefix: í˜ì´ì§€ ë§ˆì»¤ ì ‘ë‘ì‚¬ (MAIN, SUPP1, SUPP2, SUPP3)
            
            Returns:
                {
                    "full_text": "[MAIN-PAGE 1: ì œëª©]\në‚´ìš©...",
                    "total_pages": 21
                }
        """
        pages_text = []
        total_pages = 0
        ocr_count = 0

        with pdfplumber.open(pdf_path) as pdf:
            total_pages = len(pdf.pages)

            for page_idx, page in enumerate(pdf.pages, start=1):
                text = page.extract_text() or ""
                text_length = len(text.strip())

                _log(
                    f"      [DEBUG] page={page_idx} text_len={text_length} "
                    f"ocr_enabled={self.ocr_enabled}"
                )

                if text_length < self.min_text_length and self.ocr_enabled:
                    _log(f"      â†’ í˜ì´ì§€ {page_idx}: OCR ìˆ˜í–‰")

                    ocr_text, pil_img = self._perform_ocr_on_page(pdf_path, page_idx)

                    # ğŸ”¥ OCR ê²°ê³¼ì™€ ë¬´ê´€í•˜ê²Œ ì´ë¯¸ì§€ ì €ì¥
                    self._save_debug_image(
                        pil_img, pdf_path, page_idx
                    )

                    if ocr_text:
                        text = ocr_text
                        ocr_count += 1
                        _log(f"         âœ… OCR ì™„ë£Œ ({len(ocr_text)}ì)")
                    else:
                        _log(f"         âš ï¸ OCR ê²°ê³¼ ì—†ìŒ")


                title = (
                    text.split("\n")[0][:50]
                    if text.strip()
                    else f"Page {page_idx}"
                )

                pages_text.append(f"[{prefix}-PAGE {page_idx}: {title}]")
                pages_text.append(text)
                pages_text.append("")

        if ocr_count:
            _log(f"   âœ… OCR ì²˜ë¦¬ ì™„ë£Œ: {ocr_count} í˜ì´ì§€")

        return {
            "full_text": "\n".join(pages_text),
            "total_pages": total_pages,
        }

class ImageDescriptionGenerator:
    """í†µê³¼ëœ ì´ë¯¸ì§€ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª… ìƒì„± (2-4ë¬¸ì¥)"""
    
    def generate_description(
        self, 
        image_bytes: bytes, 
        adjacent_text: str,
        keywords: List[str],
        max_retries=3
    ) -> str:
        """
        Vision APIë¡œ ì´ë¯¸ì§€ ìƒì„¸ ì„¤ëª… ìƒì„±
        ì¬ì‹œë„ ë¡œì§ í¬í•¨ (429 Rate Limit ëŒ€ì‘)
        """
        import time
        
        for attempt in range(max_retries):
            try:
                mime_type = self._get_mime_type(image_bytes)
                image_part = Part.from_data(data=image_bytes, mime_type=mime_type)
                
                keyword_context = ', '.join(keywords[:10]) if keywords else "ì¼ë°˜ í•™ìŠµ ë‚´ìš©"
                
                prompt = f"""
ì´ ì´ë¯¸ì§€ë¥¼ 2-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.

ê°•ì˜ ì£¼ì œ: {keyword_context}
ì£¼ë³€ í…ìŠ¤íŠ¸: "{adjacent_text}"

ì„¤ëª…ì— í¬í•¨í•  ë‚´ìš©:
1. ì´ë¯¸ì§€ê°€ ë‚˜íƒ€ë‚´ëŠ” ì£¼ì œ/ê°œë… (1ë¬¸ì¥)
2. ì£¼ìš” êµ¬ì„± ìš”ì†Œ 2-3ê°œ (1-2ë¬¸ì¥)
3. í•µì‹¬ ì •ë³´ë‚˜ íŒ¨í„´ (1ë¬¸ì¥)

ì œì™¸í•  ë‚´ìš©:
- ì„¸ë¶€ ìš”ì†Œ ì „ì²´ ë‚˜ì—´
- ë¶ˆí•„ìš”í•œ ì¶”ì¸¡ì´ë‚˜ í•´ì„

ì¶œë ¥: ëª…í™•í•˜ê³  ê°„ê²°í•œ 2-4ë¬¸ì¥ë§Œ.
"""
                # âœ… Vertex AI model ê°€ì ¸ì˜¤ê¸°
                model = get_global_model()
                if model is None:
                    return "ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: Vertex AI model not initialized"

                response = model.generate_content([image_part, prompt])
                description = response.text.strip()
                return description
                
            except Exception as e:
                error_msg = str(e)
                
                if "429" in error_msg or "Resource exhausted" in error_msg:
                    if attempt < max_retries - 1:
                        wait_time = (attempt + 1) * 3
                        _log(f"      âš ï¸  Rate Limit, {wait_time}ì´ˆ ëŒ€ê¸° ì¤‘...", end='', flush=True)
                        time.sleep(wait_time)
                        _log(" ì¬ì‹œë„")
                        continue
                    else:
                        return "ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: API rate limit exceeded"
                else:
                    return f"ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {error_msg}"
        
        return "ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: Failed after all retries"
    
    def _get_mime_type(self, image_bytes: bytes) -> str:
        """ì´ë¯¸ì§€ ë°”ì´ë„ˆë¦¬ì—ì„œ MIME íƒ€ì… ê°ì§€"""
        if image_bytes.startswith(b'\xff\xd8'):
            return "image/jpeg"
        elif image_bytes.startswith(b'\x89PNG\r\n\x1a\n'):
            return "image/png"
        elif image_bytes.startswith(b'GIF87a') or image_bytes.startswith(b'GIF89a'):
            return "image/gif"
        elif image_bytes.startswith(b'RIFF') and image_bytes[8:12] == b'WEBP':
            return "image/webp"
        return "image/png"


class MetadataGenerator:
    """
    ë©”íƒ€ë°ì´í„° ìƒì„± ë…¸ë“œ
    
    ì£¼ê°•ì˜ìë£Œ + ë³´ì¡°ìë£Œ â†’ metadata.json
    """
    
    def __init__(self):
        self.converter = None
        self.text_extractor = TextExtractor()
        self.image_filter = ImprovedHybridFilterPipeline(auto_extract_keywords=True)
        self.image_describer = ImageDescriptionGenerator()
        self.debug = True  # ğŸ”§ DEBUG í•­ìƒ ì¼œê¸° (ì›ì¸ íŒŒì•…ìš©)
            
    def _extract_page_title(self, slide_title: str, adjacent_text: str) -> str:
        """ì˜ë¯¸ìˆëŠ” í˜ì´ì§€ ì œëª© ì¶”ì¶œ"""
        if slide_title and slide_title.strip() and slide_title.lower() != "no title":
            return slide_title.strip()[:50]
        
        if adjacent_text:
            lines = adjacent_text.strip().split('\n')
            for line in lines:
                line = line.strip()
                if len(line) > 3 and not line.startswith('â˜'):
                    return line[:50]
        
        return "í˜ì´ì§€ ì œëª© ì—†ìŒ"
    
    def generate(
        self,
        primary_file: str,
        supplementary_files: Optional[List[str]] = None,
        output_path: str = "output/metadata.json"
    ) -> str:
        """ë©”íƒ€ë°ì´í„° ìƒì„±"""
        _log(f"\n{'='*120}")
        _log(f"ğŸ¯ ë©”íƒ€ë°ì´í„° ìƒì„± ì‹œì‘")
        _log(f"{'='*120}")
        _log(f"ì£¼ê°•ì˜ìë£Œ: {primary_file}")
        if supplementary_files:
            _log(f"ë³´ì¡°ìë£Œ: {len(supplementary_files)}ê°œ")
            for i, supp in enumerate(supplementary_files, 1):
                _log(f"  {i}. {supp}")
        _log(f"{'='*120}\n")
        
        with tempfile.TemporaryDirectory() as temp_dir:
            self.converter = DocumentConverterNode(output_dir=temp_dir)
            
            _log("ğŸ“„ [1/3] ì£¼ê°•ì˜ìë£Œ ì²˜ë¦¬ ì¤‘...")
            primary_metadata = self._process_primary_source(primary_file)
            
            _log("\nğŸ“š [2/3] ë³´ì¡°ìë£Œ ì²˜ë¦¬ ì¤‘...")
            supplementary_metadata = []
            if supplementary_files:
                for i, supp_file in enumerate(supplementary_files[:3], 1):
                    try:
                        supp_meta = self._process_supplementary_source(supp_file, i)
                        supplementary_metadata.append(supp_meta)
                        _log(f"   âœ… ë³´ì¡°ìë£Œ {i} ì²˜ë¦¬ ì„±ê³µ")
                    except Exception as e:
                        _log(f"   âš ï¸ ë³´ì¡°ìë£Œ {i} ì²˜ë¦¬ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
            else:
                _log("   âš ï¸  ë³´ì¡°ìë£Œ ì—†ìŒ (ì„ íƒ ì‚¬í•­)")
            
            _log("\nğŸ”§ [3/3] ë©”íƒ€ë°ì´í„° í†µí•© ì¤‘...")
            metadata = {
                "metadata_version": "1.0",
                "created_at": datetime.now().isoformat(),
                "primary_source": primary_metadata,
                "supplementary_sources": supplementary_metadata
            }
            
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            _log(f"\n{'='*120}")
            _log(f"âœ… ë©”íƒ€ë°ì´í„° ìƒì„± ì™„ë£Œ!")
            _log(f"{'='*120}")
            _log(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_path}")
            _log(f"ğŸ“Š ì£¼ê°•ì˜ìë£Œ í˜ì´ì§€: {primary_metadata['total_pages']}ê°œ")
            _log(f"ğŸ–¼ï¸  í•„í„°ë§ëœ ì´ë¯¸ì§€: {len(primary_metadata['filtered_images'])}ê°œ")
            if supplementary_metadata:
                total_supp_pages = sum(s['total_pages'] for s in supplementary_metadata)
                _log(f"ğŸ“š ë³´ì¡°ìë£Œ í˜ì´ì§€: {total_supp_pages}ê°œ")
            _log(f"{'='*120}\n")
            
            return str(output_path)
    
    def _process_primary_source(self, file_path: str) -> Dict[str, Any]:
        """
        ì£¼ê°•ì˜ìë£Œ ì²˜ë¦¬
        âœ… TXT/URL ì§€ì› ì¶”ê°€
        """
        file_path_str = str(file_path)
        
        # ì›ë³¸ íŒŒì¼ íƒ€ì… ê°ì§€
        if file_path_str.startswith(('http://', 'https://')):
            original_file_type = 'url'
            file_path_obj = None
            display_name = file_path_str[:50]
        else:
            file_path_obj = Path(file_path)
            original_file_type = file_path_obj.suffix.lower().replace('.', '')
            display_name = file_path_obj.name
        
        _log(f"   ğŸ“„ íŒŒì¼: {display_name} ({original_file_type})")
        
        # 1. íŒŒì¼ ë³€í™˜ (TXT/URLë„ PDFë¡œ ë³€í™˜ë¨)
        _log(f"   ğŸ”„ íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
        processed_path = self.converter.convert(file_path_str)
        
        # 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        _log(f"   ğŸ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
        text_data = self.text_extractor.extract_with_markers(processed_path, prefix="MAIN")
        _log(f"   âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(text_data['full_text'])}ì")
        
        # 3. ì´ë¯¸ì§€ í•„í„°ë§
        _log(f"   ğŸ–¼ï¸  ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘...")
        
        filtered_images = []
        keywords = []
        
        # TXT/URLì€ ì´ë¯¸ì§€ ì—†ìŒ
        if original_file_type in ['txt', 'url']:
            _log(f"      â†’ TXT/URLì€ ì´ë¯¸ì§€ ì—†ìŒ, ê±´ë„ˆë›°ê¸°")
            all_images = []
        
        elif original_file_type == 'pptx':
            _log(f"      â†’ PPTX ì›ë³¸ì—ì„œ ì§ì ‘ ì¶”ì¶œ")
            self.image_filter.extract_keywords_from_document(file_path_str)
            keywords = self.image_filter.document_keywords
            all_images = self._extract_images_from_pptx(file_path_str)
            
        elif original_file_type in ['docx', 'pdf']:
            _log(f"      â†’ PDFì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ")
            self.image_filter.extract_keywords_from_document(processed_path)
            keywords = self.image_filter.document_keywords
            extractor = UniversalImageExtractor()
            all_images = extractor.extract(processed_path)
        
        else:
            _log(f"   âš ï¸  ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {original_file_type}")
            all_images = []
        
        # 4. í•„í„°ë§ ì‹¤í–‰
        if all_images:
            _log(f"   ğŸ” {len(all_images)}ê°œ ì´ë¯¸ì§€ ë°œê²¬, í•„í„°ë§ ì‹œì‘...")

            for img_meta in all_images:
                decision, reason = self.image_filter.step1_rule_check(img_meta)
                
                if decision == "INCLUDE":
                    img_meta.is_core_content = True
                    img_meta.filter_reason = reason
                    filtered_images.append(img_meta)
                    
                elif decision == "PENDING":
                    ai_result = self.image_filter.step2_gemini_check(img_meta)

                    # íŠœí”Œ ë°˜í™˜ ëŒ€ì‘
                    if isinstance(ai_result, tuple):
                        ai_result = ai_result[0]

                    if ai_result.upper().startswith("KEEP"):
                        img_meta.is_core_content = True
                        img_meta.filter_reason = ai_result
                        filtered_images.append(img_meta)
            
            _log(f"   âœ… í•„í„°ë§ ì™„ë£Œ: {len(filtered_images)}ê°œ ì„ íƒ")
        
        # 5. ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±
        filtered_image_metadata = []
        
        if filtered_images:
            _log(f"   ğŸ“ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì¤‘... (0/{len(filtered_images)})", end='', flush=True)
            
            for i, img_meta in enumerate(filtered_images, 1):
                description = self.image_describer.generate_description(
                    img_meta.image_bytes,
                    img_meta.adjacent_text,
                    keywords
                )
                
                page_title = self._extract_page_title(
                    img_meta.slide_title,
                    img_meta.adjacent_text
                )
                
                filtered_image_metadata.append({
                    "image_id": img_meta.image_id.replace("S", "MAIN_P").replace("P", "MAIN_P"),
                    "page_number": img_meta.slide_number,
                    "page_title": page_title,
                    "description": description,
                    "filter_stage": "1ì°¨ (Rule)" if "Rule" in img_meta.filter_reason else "2ì°¨ (AI)",
                    "area_percentage": img_meta.area_percentage
                })
                
                _log(f"\r   ğŸ“ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì¤‘... ({i}/{len(filtered_images)})", end='', flush=True)
            
            _log()
            
            _log(f"\n   {'='*80}")
            _log(f"   ğŸ“Š ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì™„ë£Œ")
            _log(f"      - ì²˜ë¦¬ëœ ì´ë¯¸ì§€: {len(filtered_images)}ê°œ")
            _log(f"   {'='*80}\n")

        # 6. í†µê³„
        total_images = len(all_images)
        passed_images = len(filtered_images)
        
        return {
            "role": "main",
            "filename": display_name if original_file_type == 'url' else file_path_obj.name,
            "file_type": original_file_type,
            "total_pages": text_data['total_pages'],
            "content": {
                "full_text": text_data['full_text']
            },
            "filtered_images": filtered_image_metadata,
            "statistics": {
                "total_images_found": total_images,
                "images_passed": passed_images,
                "filter_rate": passed_images / total_images if total_images > 0 else 0
            }
        }
    
    def _process_supplementary_source(self, file_path: str, order: int) -> Dict[str, Any]:
        file_path_str = str(file_path)
        
        # URLê³¼ íŒŒì¼ êµ¬ë¶„
        if file_path_str.startswith(('http://', 'https://')):
            file_type = 'url'
            display_name = 'Web Content'
        else:
            file_path_obj = Path(file_path)
            file_type = file_path_obj.suffix.lower().replace('.', '')
            display_name = file_path_obj.name
        
        _log(f"   ğŸ“š ë³´ì¡°ìë£Œ {order}: {display_name} ({file_type})")
        
        _log(f"      ğŸ”„ PDF ë³€í™˜ ì¤‘...")
        pdf_path = self.converter.convert(file_path_str)
        
        _log(f"      ğŸ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
        text_data = self.text_extractor.extract_with_markers(pdf_path, prefix=f"SUPP{order}")
        
        _log(f"      âœ… ì™„ë£Œ ({text_data['total_pages']}í˜ì´ì§€)")
        
        return {
            "order": order,
            "filename": display_name,
            "file_type": file_type,
            "total_pages": text_data['total_pages'],
            "content": {
                "full_text": text_data['full_text']
            }
        }
    
    def _extract_images_from_pptx(self, pptx_path: str) -> List[ImageMetadata]:
        """PPTXì—ì„œ ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        extractor = UniversalImageExtractor()
        return extractor.extract(pptx_path)


# CLI ì¸í„°í˜ì´ìŠ¤
if __name__ == "__main__":
    import sys
    
    _log("\n" + "="*120)
    _log("ğŸ¯ Metadata Generator Node (V2 - pdfplumber)")
    _log("="*120)
    
    if len(sys.argv) < 2:
        _log("\nì‚¬ìš©ë²•:")
        _log("  python metadata_generator_node.py <ì£¼ê°•ì˜ìë£Œ> [ë³´ì¡°1] [ë³´ì¡°2] [ë³´ì¡°3]")
        _log("\nì˜ˆì‹œ:")
        _log("  python metadata_generator_node.py ì¤‘ë“±êµ­ì–´1.pptx")
        _log("  python metadata_generator_node.py notes.txt")
        _log("  python metadata_generator_node.py https://example.com/article")
        _log("\nâœ… ì§€ì› í˜•ì‹: PPTX, DOCX, PDF, TXT, URL")
        _log("="*120 + "\n")
        sys.exit(1)
    
    primary_file = sys.argv[1]
    supplementary_files = sys.argv[2:5] if len(sys.argv) > 2 else None
    
    if not primary_file.startswith('http') and not os.path.exists(primary_file):
        _log(f"\nâŒ ì£¼ê°•ì˜ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {primary_file}")
        sys.exit(1)
    
    if supplementary_files:
        for supp in supplementary_files:
            if not supp.startswith('http') and not os.path.exists(supp):
                _log(f"\nâŒ ë³´ì¡°ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {supp}")
                sys.exit(1)
    
    try:
        generator = MetadataGenerator()
        output_path = generator.generate(
            primary_file=primary_file,
            supplementary_files=supplementary_files,
            output_path="output/metadata.json"
        )
        
        _log(f"âœ… ì„±ê³µ!")
        _log(f"ğŸ“ {output_path}")
        
    except Exception as e:
        _log(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)